{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "started\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from importlib import reload\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy\n",
    "\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from random import shuffle, choice, randint\n",
    "\n",
    "import math\n",
    "import keras\n",
    "import tensorflow\n",
    "\n",
    "from NN_utils import BigTable, WeightClip\n",
    "\n",
    "import pickle\n",
    "import zlib\n",
    "\n",
    "def logistic(x, b,off):\n",
    "    z = b*(x-off)\n",
    "    return numpy.exp(z)/(1+numpy.exp(z))\n",
    "\n",
    "def pr_to_spread(p, comps=1, as_A_and_D=True):\n",
    "    per_comp_p = p**(1/(comps))\n",
    "#     print(\"p         \", p)\n",
    "#     print(\"per comp p\", per_comp_p)\n",
    "#     spread = -numpy.log((1.0/per_comp_p)-1.0)\n",
    "    inv_sigmoid = lambda pr : ( -numpy.log((1/pr) -1) )\n",
    "    spread = inv_sigmoid(per_comp_p)\n",
    "#     print(\"spread    \", spread)\n",
    "    if as_A_and_D:\n",
    "        a = spread/2.0\n",
    "        d = -spread/2.0\n",
    "        return a,d\n",
    "    else:\n",
    "        return spread\n",
    "\n",
    "print(\"started\")\n",
    "\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "def calc_probs(s,q):\n",
    "    zmask = numpy.isclose(q,-10).astype(int)\n",
    "    diff = s-q\n",
    "    prs = 1.0/(1.0+ numpy.exp(-diff))\n",
    "    prs = numpy.maximum(zmask,prs)\n",
    "    pr = numpy.prod(prs, axis=-1).reshape(len(q))\n",
    "    return pr\n",
    "\n",
    "def calc_probs_from_embs(students,questions):\n",
    "#     students2 = numpy.repeat(students, len(questions), axis=0)\n",
    "#     questions2 = numpy.tile(questions, (len(students),1))\n",
    "#     zmask = numpy.isclose(questions2,-10).astype(int)\n",
    "#     diffs = students2-questions2\n",
    "#     prs = 1.0/(1.0+ numpy.exp(-diffs))\n",
    "#     prs = numpy.maximum(zmask,prs)\n",
    "#     probs2 = numpy.prod(prs, axis=1).reshape(len(students), len(questions))\n",
    "    probs2 = []\n",
    "    for s in students:\n",
    "        pr = calc_probs(s,questions)\n",
    "        probs2.append(pr)\n",
    "    return numpy.array(probs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_arr_arr_err(split, real_wgts, pred_wgts, max_iter=10):\n",
    "    from scipy.spatial.distance import cosine\n",
    "# pred_wgts = numpy.round(pred_wgts,1)\n",
    "\n",
    "    print(split, real_wgts.shape, pred_wgts.shape, max_iter)\n",
    "\n",
    "    out_cols = [None] * len(real_wgts.T)\n",
    "    curr_sel = None\n",
    "    curr_ix = None\n",
    "    n_iters = 10\n",
    "    chosen = None\n",
    "    curr_real_ix = None\n",
    "    \n",
    "    indices = range(len(real_wgts.T))\n",
    "\n",
    "    min_total_err = math.inf\n",
    "    best_dis = math.inf\n",
    "    for i in range(max_iter): #len(indices)**2):\n",
    "        real_used = set()\n",
    "        pred_used = set()\n",
    "        while len(pred_used) < len(indices):\n",
    "            curr_err = math.inf\n",
    "            curr_cos = math.inf\n",
    "            for rix in numpy.random.permutation(indices):\n",
    "                if rix in real_used:\n",
    "                    continue\n",
    "                real_col = real_wgts.T[rix]\n",
    "                for cix in numpy.random.permutation(indices):\n",
    "                    if cix in pred_used:\n",
    "                        continue\n",
    "                    pred_col = pred_wgts.T[cix]\n",
    "                    pred_col = pred_col #* pred_q_col\n",
    "                    err = numpy.mean(numpy.abs( pred_col - real_col))\n",
    "                    \n",
    "                    if err < curr_err:\n",
    "                        curr_sel = pred_col\n",
    "                        curr_err = err\n",
    "                        curr_cos = 0#cosine(pred_col, real_col)\n",
    "                        curr_ix = cix\n",
    "                        curr_real_ix = rix\n",
    "            real_used.add(curr_real_ix)\n",
    "            pred_used.add(curr_ix)\n",
    "            out_cols[curr_real_ix] = curr_sel\n",
    "        out_col_arr = numpy.array(out_cols).T\n",
    "        total_err = numpy.mean(numpy.abs( out_col_arr - real_wgts ))\n",
    "        \n",
    "        dis = 0\n",
    "        mean_ll = numpy.mean( out_col_arr - real_wgts )\n",
    "        if total_err < min_total_err:\n",
    "            min_total_err = total_err\n",
    "            total_q_err = numpy.mean(numpy.abs( out_col_arr[0:split] - real_wgts[0:split] ))\n",
    "            total_s_err = numpy.mean(numpy.abs( out_col_arr[split:] - real_wgts[split:] ))\n",
    "            best_ll = mean_ll\n",
    "            chosen = out_col_arr\n",
    "            best_dis = dis\n",
    "    return chosen, min_total_err, total_q_err, total_s_err, mean_ll, best_dis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "def plot_items(pred_list, real_items, s_offset):\n",
    "    if real_items.shape[1]<2:\n",
    "        print(\"real_items is only 1 component wide .. needs to be >1 to plot on a PCA graph\")\n",
    "        return None\n",
    "    elif real_items.shape[1]==2:\n",
    "        print(\"2 comps, so no dim reduc\")\n",
    "        tx=None\n",
    "    else:\n",
    "        tx = PCA(n_components=2)\n",
    "#         tx = TSNE(n_components=2)\n",
    "    fyrst = True\n",
    "    Cs = []\n",
    "    C_labs = []\n",
    "    pred_list = numpy.array(pred_list)\n",
    "    print(\"pred list shape\", pred_list.shape)\n",
    "    print(\"real items shape\", real_items.shape)\n",
    "\n",
    "\n",
    "    fitted_pred_list = []\n",
    "#     offset = numpy.median(pred_list[(pred_list>0.1)], axis=0) - numpy.median(real_items[(real_items>0.1)], axis=0)\n",
    "#     real_mean = numpy.min(real_items[real_items > 0.1])\n",
    "#     offset = numpy.min(pred_list[pred_list > 0.1]) - real_mean\n",
    "#     print(\"real mean\", real_mean)\n",
    "#     print(\"offset\", offset)\n",
    "    \n",
    "    m = len(real_items)\n",
    "    cols = list(range(m))\n",
    "    shuffle(cols)\n",
    "    \n",
    "    xmeans = numpy.zeros(m)\n",
    "    ymeans = numpy.zeros(m)\n",
    "    pairs = defaultdict(list)\n",
    "    iter = 0\n",
    "    \n",
    "    cp_real = copy.copy(real_items)\n",
    "    cp_real[cp_real < 1] = numpy.nan\n",
    "    r_offset=numpy.nanmedian(cp_real, axis=0)\n",
    "    \n",
    "    itemz_2 = real_items\n",
    "    n = len(real_items)\n",
    "    \n",
    "    for opreds in pred_list:\n",
    "        preds = copy.copy(opreds) #- s_offset[iter] + r_offset\n",
    "        split = 0\n",
    "        \n",
    "        items_chosen, min_total_err, total_q_err, total_s_err, mean_ll, best_cos_dis = calc_arr_arr_err(0, real_items, preds, max_iter=10)\n",
    "\n",
    "        itemz_pred = items_chosen\n",
    "        print(itemz_pred)\n",
    "#         itemz_pred = numpy.maximum(itemz_pred,0)\n",
    "        fitted_pred_list.append(itemz_pred)\n",
    "        \n",
    "#         itemz = real_items #- offset\n",
    "#         print(numpy.min(itemz), numpy.mean(itemz), numpy.max(itemz))\n",
    "#         itemz = numpy.maximum(itemz,0)\n",
    "#         print(numpy.min(itemz), numpy.mean(itemz), numpy.max(itemz))\n",
    "#         if itemz_2 is None:\n",
    "#             itemz_2 = numpy.concatenate([real_itemz, itemz_pred], axis=0)\n",
    "#         else:\n",
    "        itemz_2 = numpy.concatenate([itemz_2, itemz_pred], axis=0)\n",
    "\n",
    "    if tx:\n",
    "        itemz_2 = tx.fit_transform(itemz_2)\n",
    "#         itemz_2 = numpy.concatenate([itemz, itemz_pred], axis=0)\n",
    "#         if fyrst:\n",
    "#         itemz_2 = tx.fit_transform(itemz_2)\n",
    "#             fyrst = False\n",
    "#         else:\n",
    "#             itemz_2 = tx.transform(itemz_2)\n",
    "\n",
    "    from sklearn.cluster import KMeans\n",
    "    iter=0\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(10, 10)\n",
    "    \n",
    "    for opreds, itemz_pred in zip(pred_list, fitted_pred_list):\n",
    "        n = len(itemz_pred)\n",
    "        km = KMeans()\n",
    "        km.fit(itemz_pred)\n",
    "        cluster_labels = km.predict(itemz_pred)\n",
    "        print(cluster_labels)\n",
    "        \n",
    "        C = []\n",
    "        for l in set(cluster_labels):\n",
    "            cluster = list(numpy.where(cluster_labels==l)[0])\n",
    "            print(\"X\", cluster)\n",
    "            C.append(cluster)\n",
    "        Cs.append(C)\n",
    "        C_labs.append(cluster_labels)\n",
    "                \n",
    "#         NUM_COLORS = 100\n",
    "#         cm = plt.get_cmap('gist_rainbow')\n",
    "#         fig.gca().set_color_cycle([cm(1.*i/NUM_COLORS) for i in range(NUM_COLORS)])\n",
    "        print(type(itemz_2))\n",
    "        minix=n*(iter+1)\n",
    "        maxix=n*(iter+1)+n\n",
    "        \n",
    "        #i=0 -> 100,199\n",
    "        #i=2 -> 200,299\n",
    "        \n",
    "        print(\"no pts=\",n,\" indices=\", minix, maxix)\n",
    "        print(\"shape of itemz_2\", itemz_2.shape)\n",
    "        fig.gca().scatter(itemz_2[minix:maxix,0], itemz_2[minix:maxix,1], alpha=0.7, c=numpy.array(cols), cmap=plt.get_cmap('nipy_spectral'))\n",
    "        j=0\n",
    "        for j in range(n):\n",
    "            x,xh,y,yh = itemz_2[j+(n*iter),0], itemz_2[j+(n*iter+n) ,0], itemz_2[j+(n*iter),1], itemz_2[j+(n*iter+n),1]\n",
    "#             fig.gca().plot([x,xh],[y,yh],  color=\"#aaaaaa80\")\n",
    "            xmeans[j] += xh\n",
    "            ymeans[j] += yh\n",
    "            pairs[iter].append((xh, yh))\n",
    "        iter+=1\n",
    "        \n",
    "    for j in range(n):\n",
    "        fig.gca().annotate(j, (itemz_2[j,0], itemz_2[j,1]))\n",
    "        \n",
    "    fig.gca().scatter(itemz_2[0:n,0], itemz_2[0:n,1], c=\"b\", zorder=10, alpha=0.5)\n",
    "#     fig.gca().axhline(y=1e-6, linestyle=\"--\")\n",
    "#     fig.gca().axvline(x=1e-6, linestyle=\"--\")\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    \n",
    "    xmeans = xmeans / len(pred_list)\n",
    "    ymeans = ymeans / len(pred_list)\n",
    "    \n",
    "\n",
    "    if len(pred_list)>=1:\n",
    "        for it in range(len(pred_list)):\n",
    "            xhyh_pairs = pairs[it]\n",
    "            for j,hat_pair in enumerate(xhyh_pairs):\n",
    "                xh,yh = hat_pair\n",
    "                x,y = itemz_2[j,0], itemz_2[j,1]\n",
    "                mux = xmeans[j]\n",
    "                muy = ymeans[j]\n",
    "#                 fig.gca().scatter(xh, yh, alpha=0.7, c=plt.get_cmap('nipy_spectral')(cols[j]))\n",
    "#                 fig.gca().scatter(mux,muy, c=\"#888888ff\", marker=\"*\", zorder=10)\n",
    "#                 fig.gca().plot([mux,xh],[muy,yh],color=\"#aaaaaa80\", linestyle=\"--\")\n",
    "#                 fig.gca().plot([mux,x],[muy,y],color=\"#888888dd\", linestyle=\"-\")\n",
    "                fig.gca().plot([x,xh],[y,yh],color=\"#aaaaaa80\", linestyle=\"--\")\n",
    "        \n",
    "    plt.show()\n",
    "    print(\"len Cs\", len(Cs))\n",
    "    from sklearn.metrics.cluster import adjusted_rand_score\n",
    "    rands = []\n",
    "    for ix in range(len(Cs)):\n",
    "#         print(ix)\n",
    "        for jx in range(len(Cs)):\n",
    "#             print(jx)\n",
    "            if ix!=jx:\n",
    "#             print(Cs[ix], Cs[jx])\n",
    "#                 print(\"VI:\", ix,jx, varinfo(Cs[ix],Cs[jx]))\n",
    "                a_rand = adjusted_rand_score(C_labs[ix], C_labs[jx])\n",
    "                print(\"Rand:\", a_rand)\n",
    "                rands.append(a_rand)\n",
    "    print(\"Mean rand score =\", numpy.mean(rands), numpy.std(rands))\n",
    "\n",
    "# qws = qn_table.get_weights()[0]\n",
    "# qws2 = qn_table2.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, accuracy_score\n",
    "def generate_and_train(n_students, n_questions, qz,sz,pfz, vqz,vsz,vpfz, w, n_factors, min_active, max_active, nn_mode=None, loss_mode=None, class_weights=None):\n",
    "    btm = 0\n",
    "    top = math.sqrt(.1/w)\n",
    "#     init= (btm,top)\n",
    "#     init = math.sqrt(.5/w)\n",
    "    init_s = (0,1)\n",
    "    init_q = (0,1)\n",
    "    \n",
    "#     1-p + pq = s\n",
    "#     q=0.3 : 1-p + p/3 = s\n",
    "#           : 3-3p + p = s\n",
    "#           : p = (3-s)/2\n",
    "        \n",
    "    if nn_mode==\"COND\":\n",
    "        percompp = .5**(1/w)\n",
    "        print(\"percompp\", percompp)\n",
    "\n",
    "        s_table =  BigTable((n_students, w), 0,1, init_hilo= percompp )#, regulariser=regularizers.l2(10e-6))\n",
    "        qn_table = BigTable((n_questions, w), 0,1, init_hilo= percompp )#, regulariser=regularizers.l1(10e-6))\n",
    "    elif nn_mode==\"MXFN\":\n",
    "        init = math.sqrt(.5/w)\n",
    "        print(\"MXFN init'n\")\n",
    "        print(init)\n",
    "        print(init*init*w)\n",
    "        s_table =  BigTable((n_students, w), -math.inf, math.inf, init_hilo= init) #, regulariser=regularizers.l2(10e-6))\n",
    "        qn_table = BigTable((n_questions, w), -math.inf, math.inf, init_hilo= init) #, regulariser=regularizers.l1(10e-6))\n",
    "    elif nn_mode==\"MLTM\":\n",
    "        sp = pr_to_spread(.5, w, as_A_and_D=False)\n",
    "        print(\"sp is \",sp)\n",
    "        s_table =  BigTable((n_students, w), -math.inf, math.inf, init_hilo= 0) #, regulariser=regularizers.l2(10e-6))\n",
    "        qn_table = BigTable((n_questions, w), -math.inf, math.inf, init_hilo= -sp) #, regulariser=regularizers.l1(10e-6))        \n",
    "    else:\n",
    "        s_table =  BigTable((n_students, w), -math.inf, math.inf, init_hilo= 0) #, regulariser=regularizers.l2(10e-6))\n",
    "        qn_table = BigTable((n_questions, w), -math.inf, math.inf, init_hilo= 0) #, regulariser=regularizers.l1(10e-6))        \n",
    "                \n",
    "    from keras.layers import Embedding\n",
    "    from keras.constraints import NonNeg, MinMaxNorm\n",
    "    from keras.initializers import RandomNormal, RandomUniform\n",
    "    \n",
    "#     wc=WeightClip(0,1)\n",
    "    \n",
    "#     q_gates = None #Embedding(n_questions,w, input_length=1, embeddings_initializer=RandomUniform(minval=0, maxval=1, seed=None), embeddings_constraint=wc)\n",
    "#     qn_table = Embedding(n_questions,w, input_length=1, embeddings_initializer=RandomNormal(mean=6, stddev=0.3))\n",
    "#     s_table = Embedding(n_students,w, input_length=1, embeddings_constraint=WeightClip(0,math.inf), embeddings_initializer=RandomNormal(mean=6, stddev=0.3))\n",
    "    \n",
    "    from keras.optimizers import Adam\n",
    "    from keras.callbacks import EarlyStopping, LambdaCallback\n",
    "    \n",
    "#     vqz=[]\n",
    "    if len(vqz)>0:\n",
    "        lozz=\"val_mean_absolute_error\"\n",
    "        val_dat= [[vqz,vsz], vpfz]\n",
    "    else:\n",
    "#     if True:\n",
    "        lozz=\"mean_absolute_error\"\n",
    "        val_dat=None\n",
    "    \n",
    "    fiftiez = numpy.zeros_like(pfz) + .50\n",
    "    for _ in range(1):\n",
    "#         es = EarlyStopping(monitor=\"loss\", restore_best_weights=True, patience=10)\n",
    "#         m = generate_qs_model(qn_table, s_table, Adam(lr=0.001))\n",
    "#         h = m.fit(x=[qz,sz], y=numpy.array(fiftiez).reshape(-1,1), batch_size=len(pfz), shuffle=True, epochs=10000, verbose=1, callbacks=[es])\n",
    "#         wz = m.get_weights()\n",
    "        m = generate_qs_model(qn_table, s_table, Adam(), _mode=nn_mode, loss=loss_mode)\n",
    "#         m.set_weights(wz)\n",
    "\n",
    "        tr_predz = m.predict([qz,sz])\n",
    "        print(tr_predz)\n",
    "        tr_predz = (m.predict([qz,sz])[:,0] > 0.5)\n",
    "#         for vs,vq,tp,pp in zip(vsz,vqz,predz, vpfz):\n",
    "#             print(vs,vq,\"-\",tp,pp)\n",
    "        print(\"PRE-TR AVG  = \", numpy.mean(tr_predz))\n",
    "    \n",
    "        if len(vqz)>0:\n",
    "            v_predz  = (m.predict([vqz,vsz])[:,0] > 0.5)\n",
    "            print(\"PRE-TR VAVG = \", numpy.mean(v_predz))\n",
    "\n",
    "        es = EarlyStopping(monitor=lozz, restore_best_weights=True, patience=10)\n",
    "        \n",
    "\n",
    "        prs_model = Model(inputs=m.input,\n",
    "                                outputs=m.get_layer(\"Pr_sigmoid1\").output)\n",
    "#         prs_output = prs_model.predict([qz,sz])\n",
    "        \n",
    "\n",
    "        \n",
    "        hazard_model = Model(inputs=m.input,\n",
    "                                outputs=m.get_layer(\"hazard\").output)\n",
    "#         intermediate_output = intermediate_layer_model.predict([qz,sz])\n",
    "        \n",
    "        print_prs = LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "#                                        print(numpy.min(intermediate_layer_model.predict([qz,sz])),\n",
    "#                                              numpy.max(intermediate_layer_model.predict([qz,sz]))))\n",
    "                                       print(prs_model.predict([qz[0:10],sz[0:10]])))\n",
    "\n",
    "        print_hazard = LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "                                       print(hazard_model.predict([qz[0:10],sz[0:10]])))\n",
    "\n",
    "        \n",
    "        \n",
    "        _bs = len(pfz)\n",
    "#         _bs = 1000\n",
    "        for _ in range(1):\n",
    "            h = m.fit(x=[qz,sz], y=pfz, batch_size=_bs, class_weight=class_weights, shuffle=True, epochs=10000, verbose=1, callbacks=[es], validation_data=val_dat)\n",
    "        tr_predz = m.predict([qz,sz])\n",
    "        if len(vqz)>0:\n",
    "            v_predz  = m.predict([vqz,vsz])\n",
    "\n",
    "#         print(\"TR AVG = \", numpy.mean(tr_predz))\n",
    "#         print(\"TR R2  = \", r2_score(pfz, tr_predz))\n",
    "#         print(\"TR MAE = \", mean_absolute_error(pfz, tr_predz))\n",
    "#         print(\"TR ACC = \", accuracy_score((pfz>0.5), (tr_predz>0.5)))\n",
    "#         print(\"TR AGT = \", accuracy_score([random.random() < p for p in pfz], [random.random() < p for p in tr_predz]))\n",
    "        \n",
    "#         if val_dat:\n",
    "#             print(\"VA AVG = \", numpy.mean(v_predz))\n",
    "#             print(\"VA R2  = \", r2_score(vpfz, v_predz))\n",
    "#             print(\"VA MAE = \", mean_absolute_error(vpfz, v_predz))\n",
    "#             print(\"VA ACC = \", accuracy_score((vpfz>0.5), (v_predz>0.5)))\n",
    "#             print(\"VA AGT = \", accuracy_score([random.random() < p for p in vpfz], [random.random() < p for p in v_predz]))\n",
    "\n",
    "#             for ent,hat in zip(vpfz, v_predz):\n",
    "#                 print(ent)\n",
    "#                 print(hat)\n",
    "#                 print(numpy.sum(hat))\n",
    "#                 print(\"~~~~\")\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(classification_report((pfz>0.5), (tr_predz>0.5)))\n",
    "    if len(vqz)>0:\n",
    "        print(classification_report((vpfz>0.5), (v_predz>0.5)))\n",
    "            \n",
    "#     h = m.fit(x=[qz,sz], y=pfz.flatten(), batch_size=32, shuffle=True, epochs=1000, verbose=1, callbacks=[es], validation_data=[[vqz,vsz], vpfz])\n",
    "    return s_table, qn_table, m, h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_n_split(_pairs, sts, qns, realise=True, rpt=False):\n",
    "    out_w = 1\n",
    "    max_fails = out_w -1\n",
    "    if realise:\n",
    "        if rpt:\n",
    "            _counts = defaultdict(int)\n",
    "            _matches=[]\n",
    "            _pfz=[]\n",
    "            _sz, _qz = [],[]\n",
    "            cache = defaultdict(list)\n",
    "            for (vi,mi) in _pairs:\n",
    "                cache[vi].append(mi)\n",
    "            \n",
    "            for vi in cache:\n",
    "                s = sts[vi]\n",
    "                prs = calc_probs(s, [qns[k] for k in cache[vi]])  \n",
    "#                 print(\"shape of prs\", prs.shape)\n",
    "                for mix,mi in enumerate(cache[vi]):\n",
    "#                 pr = _probs[vi,mi]\n",
    "                    pr = prs[mix]\n",
    "                    rnd = random.random()\n",
    "                    i = 0\n",
    "                    zs = numpy.zeros(out_w)\n",
    "                \n",
    "                    while rnd > pr and i<out_w:\n",
    "                        i += 1\n",
    "                        rnd = random.random()\n",
    "                    \n",
    "                    if max_fails > 0:\n",
    "                        zs[min(i, max_fails)] = 1\n",
    "                    else:\n",
    "                        zs[0] = int(i==0)\n",
    "            \n",
    "                    _counts[i] += 1\n",
    "\n",
    "                    _pfz.append(zs)\n",
    "    #                 _pfz.append(i)\n",
    "                    _sz.append(vi)\n",
    "                    _qz.append(mi)\n",
    "            print(\"probs calced\")\n",
    "        else:\n",
    "            _prob_list =  numpy.array([calc_prob(sts[vi],qns[mi]) for (vi,mi) in _pairs])\n",
    "            _pfz = (numpy.random.random(len(_prob_list)) < _prob_list).astype(int)\n",
    "    #     _pfz = (0.5 < _prob_list).astype(int)\n",
    "            _matches = ( numpy.round(_prob_list) == _pfz).astype(int)\n",
    "        print(\"realisation complete\")\n",
    "    else:\n",
    "        cache = defaultdict(list)\n",
    "        for (vi,mi) in _pairs:\n",
    "            cache[vi].append(mi)\n",
    "\n",
    "        _prob_list=[]\n",
    "        for vi in cache:\n",
    "            q_keys = [k for k in cache[vi]]\n",
    "            s = sts[vi]\n",
    "            prs = calc_probs(s, [qns[k] for k in q_keys])  \n",
    "            for mix,mi in enumerate(q_keys):\n",
    "                pr = prs[mix]\n",
    "                _prob_list.append(pr)\n",
    "                \n",
    "        _prob_list =  numpy.array(_prob_list)\n",
    "        _pfz = _prob_list\n",
    "        _sz = [p[0] for p in _pairs]\n",
    "        _qz = [p[1] for p in _pairs]\n",
    "\n",
    "#     print(_pfz)\n",
    "#     _pfz = numpy.array([probs[vi,mi] for (vi,mi) in _pairs])\n",
    "\n",
    "#     print(_matches)\n",
    "#     print(numpy.sum(_matches), \"correctly labelled out of\", len(_matches), \"%=\", numpy.sum(_matches)/len(_matches))\n",
    "    if not rpt:\n",
    "        _sz = [p[0] for p in _pairs]\n",
    "        _qz = [p[1] for p in _pairs]\n",
    "    print(\"sns complete\")\n",
    "    \n",
    "#     one_c = _counts[1]\n",
    "#     for k in _counts:\n",
    "#         c = _counts[k]\n",
    "#         _counts[k] = one_c / c\n",
    "    \n",
    "    return numpy.array(_pfz), _sz, _qz, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_m_cache = pickle.load(open(\"generators.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_m_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "def report(n_factors, min_active, max_active, emb_w, nn_mode, loss_mode, sws_list, qws_list, model_list, real_stu_list, real_que_list, test_datasets, params_list, spars_list, compare=False):\n",
    "    \n",
    "    tot_sqerr = 0\n",
    "    mean_err_list = []\n",
    "    mean_std_list = []\n",
    "    mean_hit_list = []\n",
    "    \n",
    "    print(\"*****\")\n",
    "    print(nn_mode, loss_mode)\n",
    "#     print(\"*****\")\n",
    "    print(len(sws_list), len(qws_list), len(model_list), len(real_stu_list), len(real_que_list), len(test_datasets), len(params_list))\n",
    "    \n",
    "    for sw,qw,m,stz,qnz,tt_pairs, params, spars in zip(sws_list, qws_list, model_list, real_stu_list, real_que_list, test_datasets, params_list, spars_list):\n",
    "        tw,a1,a0,trbal,vbal,agt = params\n",
    "        \n",
    "        print(\"params:\", n_factors, min_active, max_active, emb_w, \"/\", tw,a1,a0, \"(\", trbal,vbal,agt,\") [\",spars,\"]\")\n",
    "        \n",
    "        err_list = []\n",
    "        true_err_list = []\n",
    "        hit_list = []\n",
    "    #     for six,qix in numpy.sort(tt_pairs, axis=0):\n",
    "    \n",
    "        true_pz = []\n",
    "        pred_pz = []\n",
    "        for six, qix in tt_pairs:\n",
    "    #         print(six, qix)\n",
    "    #     print(\"\\n------\\n\")\n",
    "    #     continue\n",
    "    #     if False:\n",
    "            tq = qnz[qix,:]\n",
    "            ts = stz[six,:]\n",
    "            qrow = qw[qix, :]\n",
    "            srow = sw[six, :]\n",
    "#             print(qrow)\n",
    "    #         print(\"raw\",tq,ts)\n",
    "    #         print(\"dif\",ts-tq)\n",
    "    #         print(numpy.prod(logistic(ts-tq,1,0)))\n",
    "#             if rasch:\n",
    "            true_p = float(calc_probs_from_embs(ts.reshape(1,-1),tq.reshape(1,-1)))\n",
    "#                 dif = ts-tq\n",
    "#                 true_ps = 1.0 / (1.0 + numpy.exp(-dif))\n",
    "#                 true_p = numpy.prod(true_ps)\n",
    "#             else:\n",
    "#                 true_p = numpy.prod((1-tq)+(ts*tq))\n",
    "            pred_p = m.predict([[qix],[six]]).flatten()[0]\n",
    "    \n",
    "            if compare:\n",
    "                print(six,qix, \":\", true_p, pred_p)\n",
    "    \n",
    "            true_pz.append(float(true_p))\n",
    "            pred_pz.append(float(pred_p))\n",
    "    #         pred_p = random.random()\n",
    "    \n",
    "            mae = numpy.abs(true_p - pred_p)\n",
    "#             print(true_p, float(pred_p), \"err:\", float(mae))\n",
    "\n",
    "            err = true_p - pred_p\n",
    "\n",
    "            true_err_list.append(err)\n",
    "            err_list.append(mae)\n",
    "            good_guess = int(numpy.round(true_p))==int(numpy.round(pred_p))\n",
    "            hit_list.append(int(good_guess))\n",
    "    #         sqerr = numpy.power(true_p - pred_p, 2)\n",
    "\n",
    "#             print(six, qix, \":\", srow, qrow)\n",
    "#             print(\"-->\", pred_p, true_p, \" ... \", good_guess)\n",
    "\n",
    "        print(\"R2 = \", r2_score(true_pz, pred_pz))\n",
    "        print(\"MAE = \", mean_absolute_error(true_pz, pred_pz))\n",
    "        numpy.set_printoptions(precision=3)\n",
    "    #     print(\"Mean sq err {}:\".format(qrow.shape), numpy.sqrt(numpy.mean(err_list)))\n",
    "    \n",
    "        plt.hist(true_pz, alpha=0.5)\n",
    "        plt.hist(pred_pz, alpha=0.5)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.hist(numpy.array(true_err_list).flatten(), alpha=0.5)\n",
    "        plt.show()\n",
    "        \n",
    "        mean_err_list.append(numpy.mean(err_list))\n",
    "        mean_std_list.append(numpy.std(err_list))\n",
    "        mean_hit_list.append(numpy.mean(hit_list))\n",
    "    #     print(sum(hit_list), len(hit_list), sum(hit_list)/len(hit_list))\n",
    "\n",
    "    # print(mean_err_list)\n",
    "    # print(mean_std_list)\n",
    "    # print(mean_hit_list)\n",
    "    # print(params_list)\n",
    "    print(len(stz),\"x\",len(qnz))\n",
    "#     for e,s,acc,params in zip(mean_err_list, mean_std_list, mean_hit_list, params_list):\n",
    "#         print(\"acc=\",acc)\n",
    "#         print(\"mae=\",e,\"sig=\",s)\n",
    "#         print(params)\n",
    "#     print(\"aggregated:\")\n",
    "    print(numpy.median(mean_hit_list), numpy.std(mean_hit_list), \"/\", numpy.median(mean_err_list), numpy.median(mean_std_list))\n",
    "    print(numpy.median(mean_err_list), numpy.mean(mean_err_list))\n",
    "# report(n_factors, min_active, max_active, emb_w, nn_mode, loss_mode, sws_list, qws_list, model_list, real_stu_list, real_que_list, test_datasets, params_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cache = {}\n",
    "pfz = None\n",
    "amx1 = None\n",
    "qws1 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def render_deets(ss,qs, n=10):\n",
    "#     for six,s in enumerate(ss[0:n]):\n",
    "#         for qix, q in enumerate(qs[0:n]):\n",
    "#             zmask = numpy.isclose(q,-10).astype(int)\n",
    "#             logits = s - q\n",
    "#             prs = 1.0 / (1.0+numpy.exp(-logits))\n",
    "#             prs = numpy.maximum(zmask,prs)\n",
    "#             p = numpy.prod(prs)\n",
    "#             print(six,qix,\":\")\n",
    "#             print(\"SSSS\")\n",
    "#             print(s)\n",
    "#             print(\"QQQQ\")\n",
    "#             print(q)\n",
    "#             print(logits, \"->\", prs, \"=>\", p)\n",
    "# render_deets(students2, questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 students, 1000 questions\n",
      "loaded dataset 5 : [[1.8927248]] 3.7367792700804077 1.1974066857300105\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = './models/MLTM_10000_1000_(100_1_5)_sp025_5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-ec240c742014>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                         \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./models/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'WeightClip'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mWeightClip\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                         \u001b[0mn_students\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/isaac/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/isaac/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'write'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/isaac/lib/python3.6/site-packages/keras/utils/io_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_is_path_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/isaac/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/isaac/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = './models/MLTM_10000_1000_(100_1_5)_sp025_5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "##### tw should be ~U[0.5, 3.5]\n",
    "#sw should be ~N[0, sd] with sd ~U[1, 3.5]\n",
    "#a0 should be ~U[-0.5, 1]\n",
    "#missing proportion should be ~U[0, 0.3]\n",
    "\n",
    "# from tensorflow.random import set_seed\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "explore_mode = False\n",
    "\n",
    "reportz=[]\n",
    "\n",
    "# factors_master = [(10,1,5)]\n",
    "factors_master = [(100,1,5)]\n",
    "w_list = [100]\n",
    "factors_list = [ m+(w,) for m in factors_master for w in w_list ]\n",
    "\n",
    "# nn_modes = [\"MLTM\",\"COND\",\"MXFN\"]\n",
    "nn_modes = [\"DEEP\"]\n",
    "loss_modes = [\"MSE\"]\n",
    "# sq_nums = [(int(1000*(1.4)**3), int(150*(1.4)**3))]\n",
    "# sq_nums = [(int(200*(1.41)**4), int(200*(1.41)**4))]\n",
    "sq_nums = [(10000, 1000)]\n",
    "\n",
    "# student_staminas = [0.01, 0.1, 0.5, 0.75, 1.0]\n",
    "\n",
    "spars_list = [.25] # [0.01, 0.05, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "data_to_run = [5, ]\n",
    "bal = .5\n",
    "\n",
    "\n",
    "for (n_students, n_questions) in sq_nums:\n",
    "    print(\"{} students, {} questions\".format(n_students, n_questions))\n",
    "    for nn_mode in nn_modes:\n",
    "        for loss_mode in loss_modes:\n",
    "            for (n_factors, min_active, max_active, emb_w) in factors_list:\n",
    "                for spars in spars_list:\n",
    "                    \n",
    "                    tup = ((n_factors, min_active, max_active), (10000, 70))\n",
    "                    if tup in gen_m_cache:\n",
    "                        (gen_m, history, best_dims, best_mse) = gen_m_cache[tup]\n",
    "                    else:\n",
    "                        print(gen_m_cache.keys())\n",
    "                        raise Exception(\"Genny not found for\",tup)\n",
    "\n",
    "#                     set_seed(666)\n",
    "                    numpy.random.seed(666)\n",
    "\n",
    "                    pred_list  = []\n",
    "                    model_list = []\n",
    "                    sparss     = []\n",
    "                    sws_list   = []\n",
    "                    qws_list   = []\n",
    "                    real_stu_list = []\n",
    "                    real_que_list = []\n",
    "                    params_list   = []\n",
    "                    test_datasets = []\n",
    "                    for a in data_to_run:\n",
    "                        \n",
    "                        fname = \"MLTM_10000_1000_(100_1_5)_sp{:03d}_{}\".format(int(spars*100), a)\n",
    "\n",
    "                        (tw,a0,a1, students_temp, qz_temp) = pickle.load(open(\"./synth_data/MLTM_10000_1000_(100_1_5)_{}.p\".format(a), \"rb\"))\n",
    "                        print(\"loaded dataset\",a,\":\", a0,a1,tw)\n",
    "                              \n",
    "                        from keras.models import load_model\n",
    "                        m = load_model(\"./models/\" + fname, custom_objects={'WeightClip': WeightClip})\n",
    "\n",
    "                        n_students = 10000\n",
    "                        n_questions = 1000\n",
    "                        students2 = students_temp[0: n_students]\n",
    "                        questions = qz_temp[0: n_questions]\n",
    "                        print(n_students, n_questions)\n",
    "\n",
    "                        slist = list(range(len(students2)))\n",
    "                        qlist= list(range(len(questions)))\n",
    "                        tr_pairs = []\n",
    "                        for vi in slist:\n",
    "                            for mi in qlist:\n",
    "                                tr_pairs.append((vi,mi))\n",
    " \n",
    "                        osz,oqz,opfz, vsz, vqz, vpfz, tt_pairs = pickle.load(open(\"./train_test_pairs/\"+fname+\".p\".format(a), \"rb\"))\n",
    "                        print(\"loaded o/v/t pairs from file\")\n",
    "                        Apfz, Asz, Aqz, _ = stitch_n_split(tr_pairs, students2, questions, realise=False, rpt=True)\n",
    "\n",
    "\n",
    "#                         if explore_mode:\n",
    "#                             plt.hist(numpy.array(pfz).flatten(), alpha=0.5)\n",
    "#                             plt.title(\"pfz\")\n",
    "#                             plt.show()\n",
    "\n",
    "                        print(n_students, n_questions)\n",
    "                        probs = numpy.zeros((n_students, n_questions))\n",
    "                        phats = numpy.zeros((n_students, n_questions))\n",
    "                        for s,q,pf in zip(Asz,Aqz,Apfz):\n",
    "                            probs[s,q] = pf\n",
    "                            \n",
    "                        maske = numpy.zeros((n_students, n_questions), dtype=int)\n",
    "                        for s,q,pf in zip(osz, oqz, opfz):\n",
    "                            maske[s,q] = 1\n",
    "                        \n",
    "                        for s,q,phat in zip(Asz,Aqz, m.predict([Aqz,Asz])):\n",
    "                            phats[s,q] = phat\n",
    "                    \n",
    "#                         sortargs = numpy.argsort(numpy.median(probs, axis=1), axis=0)\n",
    "#                         sortargs = numpy.argsort(probs, axis=0)\n",
    "                        sortargs = range(probs.shape[0])\n",
    "                        tx = PCA(n_components=2)\n",
    "                        mean_probs = []\n",
    "                        for prob_row in probs:\n",
    "                            mean_probs.append(numpy.mean(prob_row))\n",
    "\n",
    "#                             print(numpy.round(prob_row,2), numpy.mean(prob_row))\n",
    "                        mean_proficiencies = []\n",
    "                        for s_row in students2:\n",
    "                            mean_proficiencies.append(numpy.mean(s_row))\n",
    "\n",
    "                \n",
    "                        for prob_row, s_row in zip(probs, students2):\n",
    "                            numpy.mean(prob_row)\n",
    "                \n",
    "                        import scipy\n",
    "                        from sklearn.preprocessing import minmax_scale, scale\n",
    "\n",
    "#                         alphaz = numpy.mean(students2, axis=1)\n",
    "                        \n",
    "                        sts_scaled = minmax_scale(students2)\n",
    "                        #alphaz = numpy.sqrt(numpy.sum(students2**2, axis=1))\n",
    "#                         sts_txd = tx.fit_transform(sts_scaled)\n",
    "#                         plt.scatter(x=sts_txd[:,0], y=sts_txd[:,1], c=alphaz)\n",
    "#                         plt.show()\n",
    "\n",
    "                        p_alphaz = numpy.mean(probs, axis=1)\n",
    "                        ph_alphaz = numpy.mean(phats, axis=1)\n",
    "                        s_alphaz = numpy.mean(students2, axis=1)\n",
    "    \n",
    "#                         probs_txd = tx.fit_transform(probs[sortargs,:])\n",
    "                        probs_txd = tx.fit_transform(probs)\n",
    "                        plt.scatter(x=probs_txd[:,0], y=probs_txd[:,1], c=s_alphaz)\n",
    "#                         plt.scatter(x=students2[:,0], y=students2[:,1], c=s_alphaz)\n",
    "                        plt.show()\n",
    "                        \n",
    "                        plt.scatter(x=probs_txd[:,0], y=probs_txd[:,1])\n",
    "#                         plt.scatter(x=students2[:,0], y=students2[:,1], c=p_alphaz)\n",
    "                        plt.show()\n",
    "    \n",
    "    \n",
    "                        phats = numpy.transpose(phats)\n",
    "                        maske = numpy.transpose(maske)\n",
    "                        probs = numpy.transpose(probs)\n",
    "                \n",
    "#                         probs = minmax_scale(probs)\n",
    "#                         phats = minmax_scale(phats)\n",
    "\n",
    "#                         import matplotlib.pyplot as plt\n",
    "#                         import numpy as np\n",
    "\n",
    "#                         meanz = numpy.median(probs, axis=0)\n",
    "#                         print(\"meanz shape\", meanz.shape)\n",
    "#                         sorth = numpy.argsort(meanz)\n",
    "                        sorth = range(probs.shape[1])\n",
    "#                         a = np.random.random((16, 16))\n",
    "                        plt.figure(figsize=(8,8))\n",
    "                        plt.imshow((probs * maske) + (probs/5), cmap='hot', interpolation='nearest')\n",
    "#                         plt.imshow(maske, cmap='hot', interpolation='nearest')\n",
    "                        plt.show()\n",
    "\n",
    "                        plt.figure(figsize=(8,8))\n",
    "                        plt.imshow(phats, cmap='hot', interpolation='nearest')\n",
    "                        plt.show()                 \n",
    "                        \n",
    "                        \n",
    "                        nn = 500 #len(probs)\n",
    "                        concd = numpy.concatenate((probs[0:nn,:], phats[0:nn,:]))\n",
    "                        plt.figure(figsize=(10,10))\n",
    "#                         tx = TSNE(n_components=2)\n",
    "                        pts_txd = tx.fit_transform(concd)\n",
    "                        x,y = pts_txd[:nn,0], pts_txd[:nn,1]\n",
    "                        xh,yh = pts_txd[nn:,0], pts_txd[nn:,1]\n",
    "                        plt.plot([x,xh],[y,yh],color=\"#aaaaaa\", alpha=0.3, zorder=-5)\n",
    "                        plt.scatter(x, y, c=p_alphaz[0:nn])\n",
    "                        plt.scatter(xh, yh, marker=\"^\", c=ph_alphaz[0:nn])\n",
    "#                         plt.scatter(x=students2[:,0], y=students2[:,1], c=s_alphaz)\n",
    "                        plt.show()\n",
    "                        \n",
    "                    \n",
    "                        pearson_r = scipy.stats.pearsonr(mean_proficiencies, mean_probs)\n",
    "                        print(\"correlation between s and pr=\", pearson_r)\n",
    "#                         class_weights = None\n",
    "#     #                     data_cache[tup] = (students_temp, qz_temp, (pfz, sz, qz),(vpfz, vsz, vqz), class_weights)\n",
    "\n",
    "\n",
    "#         #                 print(\"mean pers is\", numpy.mean(perseverance))\n",
    "#         #                 perseverance_list.append(perseverance)\n",
    "#                         real_stu_list.append(students2)\n",
    "#                         real_que_list.append(questions)\n",
    "#                         test_datasets.append(tt_pairs)\n",
    "#                         agt = None\n",
    "#                         params_list.append((tw,a1,a0,numpy.mean(pfz), numpy.mean(vpfz), agt))\n",
    "\n",
    "                        \n",
    "\n",
    "#                         pred_probs = m2.predict([qz, sz])\n",
    "#                         print(pred_probs)\n",
    "#                         pred_list.append(pred_probs)\n",
    "#                         model_list.append(m2)\n",
    "\n",
    "#                 #     qg = q_gates.get_weights()[0]\n",
    "#                 #     qg_list.append(qg)\n",
    "#                 #     if qn_av is None:\n",
    "#                 #         qn_av = numpy.mean(qws2)\n",
    "\n",
    "#                         sparss.append(spars)\n",
    "#                         sws_list.append(sws2)\n",
    "#                         qws_list.append(qws2)\n",
    "#                         tup = (n_factors, min_active, max_active, emb_w, nn_mode, loss_mode, sws_list, qws_list, model_list, real_stu_list, real_que_list, test_datasets, params_list, sparss)\n",
    "#         #                 reportz.append(zlib.compress(pickle.dumps(tup)))\n",
    "#         #                 print(perseverance_list)\n",
    "\n",
    "#                     reportz.append(tup)\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.set_printoptions(precision=2, suppress=True, threshold=100)\n",
    "def create_adj_mx(nodes):\n",
    "    L = numpy.zeros((len(nodes), len(nodes)))\n",
    "    min_dist, max_dist = math.inf, -math.inf\n",
    "    for i in range(len(nodes)):\n",
    "        sum_dist = 0\n",
    "        for j in range(len(nodes)):\n",
    "            dist = numpy.linalg.norm(nodes[j]-nodes[i], ord=2)\n",
    "            if dist > max_dist:\n",
    "                max_dist = dist\n",
    "            elif dist < min_dist:\n",
    "                min_dist = dist\n",
    "            L[i,j] = dist\n",
    "    return L, min_dist, max_dist\n",
    "\n",
    "def create_laplacian(nodes):\n",
    "    amx = create_adj_mx(nodes)\n",
    "    L = -amx #negate the adj mx\n",
    "    for ix,row in enumerate(L):\n",
    "        deg = -numpy.sum(row)\n",
    "        L[ix,ix] = deg\n",
    "    return L\n",
    "\n",
    "def graph_knn(M, n=3, fn=\"knn\", thresh=None):\n",
    "    from graphviz import Graph\n",
    "    fn = \"\".join(map(str,(fn,\"_\",n)))\n",
    "    print(fn)\n",
    "    gg = Graph(strict=True, filename=fn)\n",
    "    for ix in range(len(M)):\n",
    "        row = M[ix,:]\n",
    "        js = numpy.argsort(row)[1:n+1]\n",
    "        for j in js:\n",
    "            v = row[j]\n",
    "            if thresh and v > thresh:\n",
    "                continue\n",
    "            tup = str(ix),str(j),str(round(v,2))\n",
    "#             print(\"cadd edge\", tup)\n",
    "            sta,end,lab = tup\n",
    "            gg.edge(sta,end, label=str(lab))\n",
    "    gg.view()\n",
    "\n",
    "    \n",
    "def graph_adj_mx(L, n=3, fn=\"adj_mx\", thresh=None):\n",
    "    from graphviz import Graph\n",
    "    gg = Graph(strict=True, filename=fn)\n",
    "    if L[0,1]<0:\n",
    "        L = -L\n",
    "    for ix in range(len(L)):\n",
    "        row = L[ix,:]\n",
    "        js = numpy.argsort(row)[1:n+1]\n",
    "        for j in js:\n",
    "            v = row[j]\n",
    "            if (thresh is not None) and v > thresh:\n",
    "                continue\n",
    "            tup = str(ix),str(j),str(round(v,2))\n",
    "#             print(\"cadd edge\", tup)\n",
    "            sta,end,lab = tup\n",
    "            gg.edge(sta,end, label=str(lab))\n",
    "    gg.view()\n",
    "    \n",
    "    \n",
    "def sketchout_amx(amx):\n",
    "    liste_fuer_vergleich = []\n",
    "    for i, reihe in enumerate(amx):\n",
    "        unterer = 0\n",
    "        print(i,\":\",reihe)\n",
    "        for oberer in [0.05, 0.1, 0.15, 0.2]:\n",
    "            zwischen = numpy.where((reihe >= unterer) & (reihe < oberer))\n",
    "#             print(unterer, \"\\t\", oberer, \"\\t\", len(zwischen[0]))\n",
    "#             print(unterer, oberer, \"\\t\", zwischen[0].flatten(), len(zwischen[0]))\n",
    "            liste_fuer_vergleich.append((i, unterer,oberer, list(zwischen[0]))) # mache hier eine Liste der Listen, dagegen wir vergleichen koennen\n",
    "            unterer = oberer\n",
    "    return liste_fuer_vergleich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for td in test_datasets:\n",
    "    print(td[0:10])\n",
    "    \n",
    "l_ls = []\n",
    "for qws in qws_list:\n",
    "    print(qws[0:10])\n",
    "    amx,minn,maxx = create_adj_mx(qws)\n",
    "    print(amx[0:10][0:10])\n",
    "    ls = sketchout_amx(amx)\n",
    "    l_ls.append(ls)\n",
    "\n",
    "l1,l2 = l_ls[0:2]\n",
    "    \n",
    "# for rqws in real_que_list:\n",
    "#     amx = create_adj_mx(rqws)\n",
    "#     print(amx[0:10][0:10])\n",
    "\n",
    "for ll1, ll2 in zip(l1,l2):\n",
    "    print(ll1[0], (ll1[1], ll1[2]), len(ll1[3]), len(ll2[3]))\n",
    "    if len(ll1[3])<100:\n",
    "        for e1,e2 in zip(ll1[3], ll2[3]):\n",
    "            print(\"\\t\", e1,e2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(reportz))\n",
    "for tup in reportz:\n",
    "#     tup = pickle.loads(zlib.decompress(tup_cmp))/\n",
    "    report(*tup, compare = True)\n",
    "    \n",
    "# qws = reportz[0][7][0]\n",
    "# real_qws = reportz[0][10][0]\n",
    "# print(qws)\n",
    "# numpy.set_printoptions(threshold=10000)\n",
    "# print(real_qws)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sws2= s_table2.get_weights()[0]\n",
    "# qws2= qn_table2.get_weights()[0]\n",
    "\n",
    "# offset = numpy.mean(students2) - numpy.mean(sws2) \n",
    "# qws2 = qws2# + offset\n",
    "# sws2 = sws2# + offset\n",
    "\n",
    "bin_spread = lambda x: max(1,int(abs(2*(numpy.max(x)-numpy.min(x)))))\n",
    "\n",
    "for ss in real_stu_list:\n",
    "    plt.hist(ss.flatten(), alpha=0.2, bins=bin_spread(ss), label=\"s true\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "print(\"There are {} els in sws_list and {} els in qws_list\".format(len(sws_list), len(qws_list)))\n",
    "\n",
    "sw_av_list = []\n",
    "for sw in sws_list:\n",
    "#     (sz,qz,pfz), (vsz,vqz,vpfz), (tsz,tqz,tpfz) = tups\n",
    "    plt.hist(sw.flatten(), alpha=0.2, bins=bin_spread(sw), label=\"s pred\")\n",
    "    sw_av_list.append(numpy.median(sw.flatten()))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for qnz in real_que_list:\n",
    "    plt.hist(qnz.flatten(), alpha=0.2, label=\"q true\", bins=bin_spread(questions))\n",
    "\n",
    "# qws_list_2 = []\n",
    "real_max_q = numpy.max(questions.flatten())\n",
    "print(\"len qws_list\", len(qws_list))\n",
    "\n",
    "for qw in qws_list:#, sw_av_list):\n",
    "    qw = copy.copy(qw)\n",
    "    print(\"med\",numpy.median(qw), \"for shape\", qw.shape)\n",
    "#     (sz,qz,pfz), (vsz,vqz,vpfz), (tsz,tqz,tpfz) = tups\n",
    "#     qg2 = (qg>0.5).astype(int)\n",
    "#     masqd = qw*qg2\n",
    "#     qws_list_2.append(masqd)\n",
    "#     print(qw)\n",
    "#     print(qg)\n",
    "#     plt.hist(qw[list(set(qz))].flatten(), alpha=0.4, bins=bin_spread(qw)) \n",
    "\n",
    "#     max_q = numpy.max(qw)\n",
    "#     os = real_max_q - max_q\n",
    "#     qw += os\n",
    "\n",
    "#     thresh = 0\n",
    "#     qw[qw < thresh] = 0\n",
    "    plt.hist(qw.flatten(), alpha=0.2, label=\"q pred\", bins=bin_spread(qw))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# qws_list = qws_list2\n",
    "\n",
    "print(len(pred_list))\n",
    "plt.hist(pfz, alpha=0.2, label=\"true obs\")\n",
    "plt.hist(probs.flatten(), alpha=0.3, label=\"true probs\")\n",
    "for ix,predz in enumerate(pred_list):\n",
    "    plt.hist(predz, alpha=0.1, label=str(ix))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_items(qws_list[1:], qws_list[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qw in qws_list:#, sw_av_list):\n",
    "    print(qw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_ensemble(ref, pred_list):\n",
    "    summage = numpy.zeros_like(ref)\n",
    "    for items in pred_list:\n",
    "        items_aligned, min_total_err, total_q_err, total_s_err, mean_ll, best_cos_dis = calc_arr_arr_err(0, ref, items, max_iter=10)        \n",
    "        print(\"err\", numpy.mean(numpy.abs(items_aligned - ref)))\n",
    "        summage += items_aligned\n",
    "    summage /= len(pred_list)\n",
    "    print(\"ensem err\", numpy.mean(numpy.abs(summage - ref)))\n",
    "    return summage\n",
    "#         for item, real_item in zip(items, real_items):\n",
    "#             print(numpy.sort(real_item))\n",
    "#             print(numpy.sort(item))\n",
    "#             print()\n",
    "            \n",
    "\n",
    "mean_en = mean_ensemble(qws_list[0], qws_list[1:])\n",
    "plot_items([], questions, None)\n",
    "plot_items(qws_list, mean_en, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(m_list)\n",
    "for m,tr in zip(m_list, tr_list):\n",
    "    (sz2,qz2,pfz2), (vsz2,vqz2,vpfz2), (tsz2,tqz2,tpfz2) = tr\n",
    "    print(sz2,qz2,pfz2)\n",
    "    print(vsz2,vqz2,vpfz2)\n",
    "    print(tsz2,tqz2,tpfz2)\n",
    "    preds = m.predict(x=[qz2,sz2])\n",
    "    for sc_true, sc_hat in zip(pfz2,preds):\n",
    "        print(sc_true, sc_hat)\n",
    "\n",
    "    # print(m.evaluate(x=[mz,vz], y=scz))\n",
    "    from sklearn.metrics import mean_absolute_error, mean_absolute_error\n",
    "    print(mean_absolute_error(numpy.around(pfz2), numpy.around(preds)  ))\n",
    "    \n",
    "plt.hist(pfz2)\n",
    "plt.show()\n",
    "plt.hist(preds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "min_loss = math.inf\n",
    "min_v_loss = math.inf\n",
    "init_patience = 5\n",
    "for i in range(100):\n",
    "    print(\"shiteration i=\",i)\n",
    "    h = m.fit(x=[mz,vz], y=scz, batch_size=1000, epochs=2, shuffle=True, validation_split=1000/len(scz), verbose=1)\n",
    "    val_loss = h.history[\"val_loss\"][-1]\n",
    "    loss = h.history[\"loss\"][-1]\n",
    "    if loss < min_loss and val_loss <= min_v_loss:\n",
    "        min_v_loss = val_loss\n",
    "        min_loss = loss\n",
    "        print(\"patience reset\")\n",
    "        patience = init_patience\n",
    "#         sw = s_table.get_weights()\n",
    "#         qw = qn_table.get_weights()\n",
    "    else:\n",
    "        patience -= 1\n",
    "    if patience==0:\n",
    "        print(\"DONE\")\n",
    "        break\n",
    "#     m.fit(x=[mz,vz], y=numpy.array([(0.5+random.uniform(-0.5,0.5)) for _ in scz]).reshape(-1,1), batch_size=1000, shuffle=False, epochs=10, verbose=0)\n",
    "    m.fit(x=[mz,vz], y=numpy.array([0.5 for _ in scz]).reshape(-1,1), batch_size=1000, shuffle=False, epochs=2, verbose=0)\n",
    "    \n",
    "# s_table.set_weights(sw)\n",
    "# qn_table.set_weights(qw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor=\"val_loss\", restore_best_weights=True, patience=10)\n",
    "m.fit(x=[mz,vz], y=scz, batch_size=1000, epochs=100, validation_split=1000/len(scz))#, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = m.predict(x=[tmz,tvz])\n",
    "for p, sc_obsv, sc_hat in zip(t_probz, tscz,preds):\n",
    "    print(p, sc_obsv, sc_hat, (numpy.around(sc_obsv)==numpy.around(sc_hat)))\n",
    "\n",
    "# print(m.evaluate(x=[tmz,tvz], y=tscz))\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error\n",
    "\n",
    "print(\"obvsd acc\", accuracy_score(numpy.around(tscz), numpy.around(preds)))\n",
    "print(\"non-stoch acc\", accuracy_score(numpy.around(t_probz), numpy.around(preds)))\n",
    "print(mean_absolute_error(t_probz, preds))\n",
    "#0.000302638761699 MSE MxMul\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    plt.hist(obs.flatten())\n",
    "    plt.show()\n",
    "    print(numpy.sum(numpy.around(obs)))\n",
    "    print(len(obs.flatten()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_wgts = qn_table.get_weights()[0]\n",
    "real_wgts = movies\n",
    "\n",
    "split = 0\n",
    "\n",
    "items_chosen, min_total_err, total_q_err, total_s_err, mean_ll, best_cos_dis = calc_arr_arr_err(0, real_wgts, pred_wgts, max_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min_total_err, total_q_err, total_s_err)\n",
    "\n",
    "print(items_chosen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fig = plt.gcf()\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "# pca = TSNE(n_components=2)\n",
    "\n",
    "itemz_pred = items_chosen\n",
    "n = len(items_chosen)\n",
    "itemz = movies\n",
    "\n",
    "# s_pred_mean = numpy.mean(s_table.get_weights()[0])\n",
    "base = min( numpy.min(itemz_pred), numpy.min(itemz))\n",
    "# ss1 = StandardScaler()\n",
    "# itemz_pred = ss1.fit_transform(itemz_pred)\n",
    "# itemz = ss1.transform(movies)\n",
    "\n",
    "itemz = itemz - base\n",
    "itemz_pred = itemz_pred - base\n",
    "\n",
    "print(itemz)\n",
    "print(itemz_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itemz_pred = pca.transform(itemz_pred)\n",
    "\n",
    "itemz_2 = numpy.concatenate([itemz, itemz_pred], axis=0)\n",
    "# itemz_2 = itemz\n",
    "itemz_2 = pca.fit_transform(itemz_2)\n",
    "\n",
    "# itemz_2 = MinMaxScaler().fit_transform(itemz_2)\n",
    "\n",
    "# ixes = itemz_pred < baseline\n",
    "# itemz_pred[ixes] = (baseline-1)\n",
    "# itemz_pred = itemz_pred - (baseline-1)\n",
    "# itemz_pred = MinMaxScaler().fit_transform(itemz_pred)\n",
    "# print(itemz_2)\n",
    "\n",
    "# fig,axs = plt.subplots(1,2)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 10)\n",
    "\n",
    "fig.gca().scatter(itemz_2[0:n,0], itemz_2[0:n,1], alpha=0.7)\n",
    "fig.gca().scatter(itemz_2[n:,0], itemz_2[n:,1], alpha=0.7)\n",
    "j=0\n",
    "for j in range(n):\n",
    "    x,xh,y,yh = itemz_2[j,0], itemz_2[j+n,0], itemz_2[j,1], itemz_2[j+n,1]\n",
    "    fig.gca().plot([x,xh],[y,yh],color=\"#aaaaaa\")\n",
    "    fig.gca().annotate(j, (itemz_2[j+n,0], itemz_2[j+n,1]))\n",
    "\n",
    "# fig.gca().scatter(itemz_pred[:,0], itemz_pred[:,1], alpha=0.5)\n",
    "\n",
    "# for i, txt in enumerate(itemz_2):\n",
    "#     fig.gca().annotate(i, (itemz_2[i,0], itemz_2[i,1]))\n",
    "\n",
    "# fig.gca().axvline(x=baseline, linestyle=\"--\")\n",
    "# fig.gca().axhline(y=baseline, linestyle=\"--\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if qws1 is None:\n",
    "    qws1 = qws2\n",
    "    amx1, min_dist, max_dist = create_adj_mx(qws1)\n",
    "    print(\"min and max distances are\", min_dist, max_dist)\n",
    "\n",
    "amx2, min_dist, max_dist = create_adj_mx(qws2)\n",
    "print(\"min and max distances are\", min_dist, max_dist)\n",
    "# print(amx)\n",
    "# graph_adj_mx(amx)\n",
    "# first = True\n",
    "# for r in range(len(amx)):\n",
    "#     neighbours = numpy.argsort(amx[r,:])[0:11]\n",
    "#     print(r,\":\", neighbours)\n",
    "#     if first:\n",
    "\n",
    "# graph_knn(amx[0:5,:], n=100, thresh=max_dist/20)\n",
    "graph_knn(amx2[0:5,:], n=200, thresh=None)\n",
    "\n",
    "#         first = False\n",
    "    \n",
    "# m=3\n",
    "# L1 = create_laplacian(questions)\n",
    "# print(L1)\n",
    "# graph_adj_mx(L1, fn=\"original\", n=m)\n",
    "\n",
    "# for j,q in enumerate(qws_list):\n",
    "#     L1 = create_laplacian(q)\n",
    "#     graph_adj_mx(L1, fn=\"facsimile_{}\".format(j), n=m)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####\n",
    "# lams = numpy.linalg.eigvalsh(L0)\n",
    "# sum_eig = sum(lams)\n",
    "# sum_upto = 0\n",
    "# ct=0\n",
    "# for lam in lams:\n",
    "#     sum_upto += lam\n",
    "#     if sum_upto > 0.9*sum_eig:\n",
    "#         print(\"broke loop at \\lambda_{}\".format(ct))\n",
    "#         break\n",
    "#     ct+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sketchout_amx(amx):\n",
    "    liste_fuer_vergleich = []\n",
    "    for i, reihe in enumerate(amx):\n",
    "        unterer = 0\n",
    "        print(i,\":\")\n",
    "        for oberer in [0.01, 0.05]:#, 0.1, 0.15, 0.2]:\n",
    "            zwischen = numpy.where((reihe > unterer) & (reihe < oberer))\n",
    "#             print(unterer, oberer, \"\\t\", zwischen[0].flatten(), len(zwischen[0]))\n",
    "            print(unterer, oberer, \"\\t\", len(zwischen[0]))\n",
    "            unterer = oberer\n",
    "            liste_fuer_vergleich.append(list(zwischen[0])) # mache hier eine Liste der Listen, dagegen wir vergleichen koennen\n",
    "    return liste_fuer_vergleich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amx1, min_dist, max_dist = create_adj_mx(qws1)\n",
    "print(\"AMX1 min and max distances are\", min_dist, max_dist)\n",
    "l1 = sketchout_amx(amx1)\n",
    "print(\"**\\n\\n\\n\")\n",
    "# amx2, min_dist, max_dist = create_adj_mx(qws2)\n",
    "print(\"\\nAMX2 min and max distances are\", min_dist, max_dist)\n",
    "l2 = sketchout_amx(amx2)\n",
    "\n",
    "for ll1, ll2 in zip(l1,l2):\n",
    "    print(len(ll1), len(ll2))\n",
    "    if len(ll1)<100:\n",
    "        print(ll1)\n",
    "        print(ll2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
