{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "colab_type": "code",
    "id": "Wf0XGW7uQbqT",
    "outputId": "7c4ebc30-d696-46d7-edb4-9ecca5f2b728"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "started\n",
      "                                     qn_id     activity_name  \\\n",
      "qn_id                                                          \n",
      "Dialogue 1~1.0              Dialogue 1~1.0        Dialogue 1   \n",
      "Dialogue 1~2.0              Dialogue 1~2.0        Dialogue 1   \n",
      "Dialogue 1~3.0              Dialogue 1~3.0        Dialogue 1   \n",
      "Dialogue 1~4.0              Dialogue 1~4.0        Dialogue 1   \n",
      "Dialogue 1~5.0              Dialogue 1~5.0        Dialogue 1   \n",
      "Minimal pairs 1~1.0    Minimal pairs 1~1.0   Minimal pairs 1   \n",
      "Minimal pairs 1~2.0    Minimal pairs 1~2.0   Minimal pairs 1   \n",
      "Minimal pairs 1~3.0    Minimal pairs 1~3.0   Minimal pairs 1   \n",
      "Minimal pairs 1~4.0    Minimal pairs 1~4.0   Minimal pairs 1   \n",
      "Minimal pairs 1~5.0    Minimal pairs 1~5.0   Minimal pairs 1   \n",
      "Minimal pairs 1~6.0    Minimal pairs 1~6.0   Minimal pairs 1   \n",
      "Minimal pairs 1~7.0    Minimal pairs 1~7.0   Minimal pairs 1   \n",
      "Minimal pairs 1~8.0    Minimal pairs 1~8.0   Minimal pairs 1   \n",
      "Minimal pairs 1~9.0    Minimal pairs 1~9.0   Minimal pairs 1   \n",
      "Minimal pairs 1~10.0  Minimal pairs 1~10.0   Minimal pairs 1   \n",
      "Minimal pairs 1~11.0  Minimal pairs 1~11.0   Minimal pairs 1   \n",
      "Minimal pairs 1~12.0  Minimal pairs 1~12.0   Minimal pairs 1   \n",
      "Minimal pairs 1~13.0  Minimal pairs 1~13.0   Minimal pairs 1   \n",
      "Minimal pairs 1~14.0  Minimal pairs 1~14.0   Minimal pairs 1   \n",
      "Minimal pairs 1~15.0  Minimal pairs 1~15.0   Minimal pairs 1   \n",
      "Speed reading 14~1.0  Speed reading 14~1.0  Speed reading 14   \n",
      "Speed reading 14~2.0  Speed reading 14~2.0  Speed reading 14   \n",
      "Speed reading 14~3.0  Speed reading 14~3.0  Speed reading 14   \n",
      "Speed reading 14~4.0  Speed reading 14~4.0  Speed reading 14   \n",
      "Speed reading 14~5.0  Speed reading 14~5.0  Speed reading 14   \n",
      "Error finding 1~1.0    Error finding 1~1.0   Error finding 1   \n",
      "Error finding 1~2.0    Error finding 1~2.0   Error finding 1   \n",
      "Error finding 1~3.0    Error finding 1~3.0   Error finding 1   \n",
      "Error finding 1~4.0    Error finding 1~4.0   Error finding 1   \n",
      "Error finding 1~5.0    Error finding 1~5.0   Error finding 1   \n",
      "...                                    ...               ...   \n",
      "Spelling 3~6.0              Spelling 3~6.0        Spelling 3   \n",
      "Spelling 3~7.0              Spelling 3~7.0        Spelling 3   \n",
      "Spelling 3~8.0              Spelling 3~8.0        Spelling 3   \n",
      "Spelling 3~9.0              Spelling 3~9.0        Spelling 3   \n",
      "Spelling 3~10.0            Spelling 3~10.0        Spelling 3   \n",
      "Spelling 3~11.0            Spelling 3~11.0        Spelling 3   \n",
      "Spelling 3~12.0            Spelling 3~12.0        Spelling 3   \n",
      "Spelling 3~13.0            Spelling 3~13.0        Spelling 3   \n",
      "Spelling 3~14.0            Spelling 3~14.0        Spelling 3   \n",
      "Spelling 3~15.0            Spelling 3~15.0        Spelling 3   \n",
      "Spelling 3~16.0            Spelling 3~16.0        Spelling 3   \n",
      "Spelling 3~17.0            Spelling 3~17.0        Spelling 3   \n",
      "Spelling 3~18.0            Spelling 3~18.0        Spelling 3   \n",
      "Speed reading 13~1.0  Speed reading 13~1.0  Speed reading 13   \n",
      "Speed reading 13~2.0  Speed reading 13~2.0  Speed reading 13   \n",
      "Speed reading 13~3.0  Speed reading 13~3.0  Speed reading 13   \n",
      "Speed reading 13~4.0  Speed reading 13~4.0  Speed reading 13   \n",
      "Speed reading 13~5.0  Speed reading 13~5.0  Speed reading 13   \n",
      "Error finding 6~1.0    Error finding 6~1.0   Error finding 6   \n",
      "Error finding 6~2.0    Error finding 6~2.0   Error finding 6   \n",
      "Error finding 6~3.0    Error finding 6~3.0   Error finding 6   \n",
      "Error finding 6~4.0    Error finding 6~4.0   Error finding 6   \n",
      "Error finding 6~5.0    Error finding 6~5.0   Error finding 6   \n",
      "Error finding 6~6.0    Error finding 6~6.0   Error finding 6   \n",
      "Error finding 6~7.0    Error finding 6~7.0   Error finding 6   \n",
      "Phrasal verbs 3~1.0    Phrasal verbs 3~1.0   Phrasal verbs 3   \n",
      "Phrasal verbs 3~2.0    Phrasal verbs 3~2.0   Phrasal verbs 3   \n",
      "Phrasal verbs 3~3.0    Phrasal verbs 3~3.0   Phrasal verbs 3   \n",
      "Phrasal verbs 3~4.0    Phrasal verbs 3~4.0   Phrasal verbs 3   \n",
      "Phrasal verbs 3~5.0    Phrasal verbs 3~5.0   Phrasal verbs 3   \n",
      "\n",
      "                                activity_skill  \n",
      "qn_id                                           \n",
      "Dialogue 1~1.0               speaking~dialogue  \n",
      "Dialogue 1~2.0               speaking~dialogue  \n",
      "Dialogue 1~3.0               speaking~dialogue  \n",
      "Dialogue 1~4.0               speaking~dialogue  \n",
      "Dialogue 1~5.0               speaking~dialogue  \n",
      "Minimal pairs 1~1.0    listening~minimal_pairs  \n",
      "Minimal pairs 1~2.0    listening~minimal_pairs  \n",
      "Minimal pairs 1~3.0    listening~minimal_pairs  \n",
      "Minimal pairs 1~4.0    listening~minimal_pairs  \n",
      "Minimal pairs 1~5.0    listening~minimal_pairs  \n",
      "Minimal pairs 1~6.0    listening~minimal_pairs  \n",
      "Minimal pairs 1~7.0    listening~minimal_pairs  \n",
      "Minimal pairs 1~8.0    listening~minimal_pairs  \n",
      "Minimal pairs 1~9.0    listening~minimal_pairs  \n",
      "Minimal pairs 1~10.0   listening~minimal_pairs  \n",
      "Minimal pairs 1~11.0   listening~minimal_pairs  \n",
      "Minimal pairs 1~12.0   listening~minimal_pairs  \n",
      "Minimal pairs 1~13.0   listening~minimal_pairs  \n",
      "Minimal pairs 1~14.0   listening~minimal_pairs  \n",
      "Minimal pairs 1~15.0   listening~minimal_pairs  \n",
      "Speed reading 14~1.0     reading~speed_reading  \n",
      "Speed reading 14~2.0     reading~speed_reading  \n",
      "Speed reading 14~3.0     reading~speed_reading  \n",
      "Speed reading 14~4.0     reading~speed_reading  \n",
      "Speed reading 14~5.0     reading~speed_reading  \n",
      "Error finding 1~1.0   writing~error_correction  \n",
      "Error finding 1~2.0   writing~error_correction  \n",
      "Error finding 1~3.0   writing~error_correction  \n",
      "Error finding 1~4.0   writing~error_correction  \n",
      "Error finding 1~5.0   writing~error_correction  \n",
      "...                                        ...  \n",
      "Spelling 3~6.0                writing~spelling  \n",
      "Spelling 3~7.0                writing~spelling  \n",
      "Spelling 3~8.0                writing~spelling  \n",
      "Spelling 3~9.0                writing~spelling  \n",
      "Spelling 3~10.0               writing~spelling  \n",
      "Spelling 3~11.0               writing~spelling  \n",
      "Spelling 3~12.0               writing~spelling  \n",
      "Spelling 3~13.0               writing~spelling  \n",
      "Spelling 3~14.0               writing~spelling  \n",
      "Spelling 3~15.0               writing~spelling  \n",
      "Spelling 3~16.0               writing~spelling  \n",
      "Spelling 3~17.0               writing~spelling  \n",
      "Spelling 3~18.0               writing~spelling  \n",
      "Speed reading 13~1.0     reading~speed_reading  \n",
      "Speed reading 13~2.0     reading~speed_reading  \n",
      "Speed reading 13~3.0     reading~speed_reading  \n",
      "Speed reading 13~4.0     reading~speed_reading  \n",
      "Speed reading 13~5.0     reading~speed_reading  \n",
      "Error finding 6~1.0   writing~error_correction  \n",
      "Error finding 6~2.0   writing~error_correction  \n",
      "Error finding 6~3.0   writing~error_correction  \n",
      "Error finding 6~4.0   writing~error_correction  \n",
      "Error finding 6~5.0   writing~error_correction  \n",
      "Error finding 6~6.0   writing~error_correction  \n",
      "Error finding 6~7.0   writing~error_correction  \n",
      "Phrasal verbs 3~1.0      reading~phrasal_verbs  \n",
      "Phrasal verbs 3~2.0      reading~phrasal_verbs  \n",
      "Phrasal verbs 3~3.0      reading~phrasal_verbs  \n",
      "Phrasal verbs 3~4.0      reading~phrasal_verbs  \n",
      "Phrasal verbs 3~5.0      reading~phrasal_verbs  \n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from importlib import reload\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from random import shuffle, choice, randint\n",
    "\n",
    "import math\n",
    "import keras\n",
    "import tensorflow\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "import sys\n",
    "# sys.path.append('/content/gdrive/My Drive/Colab Notebooks')\n",
    "# from NN_utils import BigTable, WeightClip\n",
    "from NN_utils import WeightClip\n",
    "# def calc_probs_from_embs(students,questions):\n",
    "#     students2 = numpy.repeat(students, len(questions), axis=0)\n",
    "#     questions2 = numpy.tile(questions, (len(students),1))\n",
    "#     zmask = numpy.isclose(questions2,-10).astype(int)\n",
    "#     diffs = students2-questions2\n",
    "#     prs = numpy.exp(diffs)/(1.0+ numpy.exp(diffs))\n",
    "#     prs = numpy.maximum(zmask,prs)\n",
    "#     probs2 = numpy.prod(prs, axis=1).reshape(len(students), len(questions))\n",
    "#     return probs2\n",
    "\n",
    "# def calc_probs(s,q):\n",
    "#     zmask = numpy.isclose(q,-10).astype(int)\n",
    "#     diff = s-q\n",
    "#     prs = 1.0/(1.0+ numpy.exp(-diff))\n",
    "#     prs = numpy.maximum(zmask,prs)\n",
    "#     # print(prs)\n",
    "#     if len(q.shape)>1 and len(q.shape[0]) > 1:\n",
    "#       raise Exception(\"tensor is wrong shape, duh\")\n",
    "#       # pr = pr.reshape(len(q))\n",
    "#     pr = numpy.prod(prs)\n",
    "#     return pr, zmask\n",
    "\n",
    "# home = \"/content/gdrive/My Drive/Colab Notebooks\"\n",
    "home=\".\"\n",
    "\n",
    "mapping = pandas.read_csv(home+\"/real_data/qn_act_map.csv\")\n",
    "mapping.index = mapping.qn_id\n",
    "# mapping.drop(\"qn_id\", axis=1, inplace=True)\n",
    "print(mapping[0:100])\n",
    "print(\"started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_databundle():\n",
    "# if True:\n",
    "#     raw_df = pandas.read_csv(home+\"/real_data/Worksheet_1041.csv\")\n",
    "#     print(raw_df.columns)\n",
    "#     print(len(raw_df))\n",
    "#     raw_df = raw_df[raw_df.event_type==\"answer_submitted\"]\n",
    "#     print(len(raw_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QjdJJEs-Qs1u"
   },
   "outputs": [],
   "source": [
    "def progress_one_step(q,s, s_gammas):\n",
    "  pr, zmask = calc_probs(s,q)\n",
    "  active_in_q = 1-zmask\n",
    "  # print(active_in_q)\n",
    "  # print(\"pr is\", pr)\n",
    "  passed = 0\n",
    "  if (numpy.random.random() <= pr):\n",
    "    passed = 1\n",
    "  s= s + s_gammas*active_in_q # learning rates from a successful attempt\n",
    "  # else:\n",
    "  #   s= s + s_rhos*active_in_q # learning rates from an unsauccessful attempt\n",
    "  return passed, pr, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wAfdTWTpkE1K"
   },
   "outputs": [],
   "source": [
    "def run_data(students, questions, gammas, model_to_train=None):\n",
    "  from collections import defaultdict, Counter\n",
    "  sixs = []\n",
    "  qixs = []\n",
    "  hits = []\n",
    "  outps = []\n",
    "  \n",
    "  r = -1\n",
    "  scores = defaultdict(int)\n",
    "  hit_counter = {}\n",
    "  \n",
    "  s_indices = range(len(students))\n",
    "  # bo_selecta = None\n",
    "  for six in s_indices:\n",
    "    print(\"running student, \", six)\n",
    "    s = students[six]\n",
    "    R = 0  # reset reward for new student\n",
    "    ball_bag = list(range(len(questions)))\n",
    "    while ball_bag:\n",
    "      # print(\"WSH\",s.shape)\n",
    "      # print(numpy.mean(s), numpy.min(s), numpy.max(s))\n",
    "      # for sval in s:\n",
    "      #   print(sval)\n",
    "      # raise Exception(\"WANK\")\n",
    "      # if bo_selecta is None:\n",
    "      bo_selecta = random.choice(ball_bag)\n",
    "      q = questions[bo_selecta]\n",
    "      passed, pr, s_ = progress_one_step(q,s, gammas[six])\n",
    "      students[six] = s_ # Crucially, update the student to make progress...\n",
    "      # print(six, bo_selecta, numpy.mean(students[six]), pr)\n",
    "      if passed:\n",
    "        # print(\"***PASSED***\", six, bo_selecta, pr)\n",
    "        ball_bag.remove(bo_selecta)\n",
    "\n",
    "      # hit_counter[(six, bo_selecta)] += 1\n",
    "      # hit_counter\n",
    "    \n",
    "      if six not in hit_counter:\n",
    "        print(\"INIT'G zeros FOR\", six)\n",
    "        hit_counter[six] = [int(0)]*n_questions #numpy.zeros(n_questions, dtype=\"uint8\")\n",
    "\n",
    "      sixs.append( [int(six)])\n",
    "      qixs.append([int(bo_selecta)])\n",
    "      hits.append( tuple(hit_counter[six]) )\n",
    "      outps.append( [int(passed)] )\n",
    "\n",
    "      # neue = hit_counter[six]\n",
    "      # neue[bo_selecta] += 1\n",
    "      hit_counter[six][bo_selecta] += 1\n",
    "\n",
    "      R += r\n",
    "      # bo_selecta = None\n",
    "    print(R)\n",
    "    scores[six] = R\n",
    "\n",
    "  # if model_to_train:\n",
    "  #   phat = model_to_train.predict([[six], [bo_selecta], [hit_counter[(six,bo_selecta)]] ])\n",
    "  #   mae = abs(pr - phat)\n",
    "  #   print(pr, phat, mae)\n",
    "  #   model_to_train.train_on_batch( [ [six], [bo_selecta], [hit_counter[(six,bo_selecta)]] ], [passed] )\n",
    " \n",
    "  # print(\"Die Arrays werden in Numpy Datentypen verwandelt.\")\n",
    "  # sixs = numpy.array(sixs, dtype=\"uint8\")\n",
    "  # qixs = numpy.array(qixs, dtype=\"uint8\")\n",
    "  # hits = numpy.array(hits, dtype=\"uint8\")\n",
    "  # outps = numpy.array(outps, dtype=\"uint8\")\n",
    "\n",
    "  # print(hits.shape)\n",
    "  # print(outps.shape)\n",
    "\n",
    "  # if model_to_train:\n",
    "    # phat = model_to_train.predict([[six], [bo_selecta], [hit_counter[(six,bo_selecta)]] ])\n",
    "    # mae = abs(pr - phat)\n",
    "    # print(pr, phat, mae)\n",
    "    # model_to_train.fit( inps, outps )\n",
    "  # for (k,v) in scores.items():\n",
    "  #   print(k,v)\n",
    "  print(\"Der Lauf is beendet.\")\n",
    "  vals = list(scores.values())\n",
    "  return numpy.mean(vals), sixs, qixs, hits, outps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P7GZiAAj6KSW"
   },
   "outputs": [],
   "source": [
    "from keras.regularizers import l1\n",
    "from keras.layers import Reshape, Dense, Dropout, add, multiply, subtract, GaussianNoise, GaussianDropout, Input, Lambda, Embedding, concatenate, Flatten, Maximum, Multiply, dot, Layer\n",
    "from keras import backend as K, Model\n",
    "from keras.initializers import RandomUniform, RandomNormal\n",
    "\n",
    "def hard_sigmoid(x):\n",
    "    return np.maximum(0, np.minimum(1, (x + 2) / 4))\n",
    "\n",
    "def binary_regulariser(x):\n",
    "    return K.sum( 1.0-(4.0*K.pow((0.5 - x),2)) )\n",
    "    # return K.sum(K.log(2*x) +K.log(2*(1-x)))\n",
    "    # return K.log(2*x) +K.log(2*(1-x))\n",
    "    \n",
    "from keras import backend as K\n",
    "def recall_m(y_true, y_pred):\n",
    "#     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) + K.epsilon()\n",
    "    true_positives = K.sum(y_true * y_pred) + K.epsilon()\n",
    "    possible_positives = K.sum(y_true) + K.epsilon()\n",
    "    recall = true_positives / possible_positives\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "#     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) + K.epsilon()\n",
    "    true_positives = K.sum(y_true * y_pred) + K.epsilon()\n",
    "    predicted_positives = K.sum(y_pred) + K.epsilon()\n",
    "    precision = true_positives / predicted_positives\n",
    "    return precision\n",
    "\n",
    "def f1_metric(y_true, y_pred, average=\"macro\"):\n",
    "    y_true = K.cast(y_true, \"float32\")\n",
    "    precision_1 = precision_m(y_true, y_pred)\n",
    "    recall_1 = recall_m(y_true, y_pred)\n",
    "#     print(\"p/r 1\", K.eval(precision_1), K.eval(recall_1))\n",
    "    f1_1 = 2.0*precision_1*recall_1 / (precision_1+recall_1)\n",
    "#     print(\"f1_1\", K.eval(f1_1))\n",
    "    if average==\"macro\":\n",
    "        precision_0 = precision_m((1-y_true), (1-y_pred))\n",
    "        recall_0 = recall_m((1-y_true), (1-y_pred))\n",
    "#         print(\"p/r 0\", K.eval(precision_0), K.eval(recall_0))\n",
    "        f1_0 = 2.0*precision_0*recall_0 / (precision_0+recall_0)\n",
    "#         print(\"f1_0\", K.eval(f1_0))\n",
    "        f1 = (f1_1+f1_0)/2.0\n",
    "#         print(\"f1  \", K.eval(f1))\n",
    "        return f1\n",
    "    else:\n",
    "        return f1_1\n",
    "\n",
    "def f1_loss(y_true, y_pred, average=\"macro\"):\n",
    "    return (1.0 - f1_metric(y_true, y_pred, average=average))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P7GZiAAj6KSW"
   },
   "outputs": [],
   "source": [
    "#5e-5/row_w\n",
    "def generate_MLTM_raw_model(n_questions, n_students, row_w, ptqs=None, \n",
    "                        loss=\"binary_crossentropy\",\n",
    "                        metrics=None, init50=True, deep_HEU=False, reg=None, reg_w=None,\n",
    "                        pos_only=False):\n",
    "    print(\"MLTM RAW:\\nROW W is \", row_w)\n",
    "    print(\"reg is\",reg)\n",
    "    print(n_questions, n_students)\n",
    "\n",
    "    from keras.initializers import RandomNormal, RandomUniform, Constant\n",
    "    from keras.regularizers import L2\n",
    "    from keras.constraints import NonNeg\n",
    "    psi_sel = Input(shape=(1,), name=\"psi_select\", dtype=\"int32\")\n",
    "    qn_sel = Input(shape=(1,), name=\"q_select\", dtype=\"int32\")\n",
    "    hit_counter = Input(shape=(n_questions, ), name=\"hit_counter\", dtype=\"float32\")\n",
    "\n",
    "    sp = pr_to_spread(.5, row_w, as_A_and_D=False)\n",
    "#     long_clip = WeightClip(-math.inf, math.inf)\n",
    "#     long_clip = WeightClip(-5,5+sp)\n",
    "#     if pos_only:\n",
    "#     long_clip = WeightClip(0, 6+2*sp)\n",
    "\n",
    "#     q_init = RandomNormal(mean=sp) if init50 else \"uniform\"\n",
    "#     q_init = RandomNormal(mean=-sp) if init50 else \"uniform\"\n",
    "    q_init = RandomUniform(minval=sp-0.5, maxval=sp+0.5) if init50 else \"uniform\"\n",
    "#     q_init = \"uniform\"\n",
    "\n",
    "    if reg_w is None:\n",
    "        reg_w = 0\n",
    "        \n",
    "    reg_pen = 0.001/(((2*(1 if sp==0 else sp))**2)*(n_students*row_w))\n",
    "    print(\"reg_pen is\", reg_pen)\n",
    "    \n",
    "    qn_emb = Embedding(n_questions, row_w, \n",
    "#                        embeddings_regularizer=L2(100.0/(n_questions*row_w)**2) if \"l1\" in reg else None, \n",
    "#                        embeddings_regularizer=L2( reg_pen ),\n",
    "                       embeddings_initializer=q_init,\n",
    "                       name=\"qn_embedding\")\n",
    "#     qn_row = Flatten()(long_clip(qn_emb(qn_sel)))\n",
    "    qn_row = Flatten()(qn_emb(qn_sel))\n",
    "#     qn_row = Dense(row_w)(qn_row)\n",
    "    \n",
    "#     if reg_w is None:\n",
    "#         this_w = 32e-8\n",
    "#     else:\n",
    "#         this_w = (reg_w/row_w)\n",
    "    #embeddings_initializer=RandomNormal(mean=1+sp)\n",
    "    s_init = RandomNormal(mean=2*sp) if init50 else \"uniform\"\n",
    "#     s_init = \"uniform\"\n",
    "#     pos_clip = WeightClip(0, math.inf)\n",
    "#     gamma_row = Flatten()(NonNeg()(Embedding(n_students, row_w, name=\"gammas\")(psi_sel)))\n",
    "    gamma_row = Flatten()(Embedding(n_students, row_w, name=\"gammas\")(psi_sel))\n",
    "    \n",
    "    alpha_row = Flatten()(Embedding(n_students, row_w, \n",
    "                                                embeddings_initializer=s_init, \n",
    "#                                                 embeddings_regularizer=L2(200.0/(n_students*row_w)**2) if \"l2\" in reg else None, \n",
    "                                                embeddings_regularizer=L2( reg_pen ),\n",
    "                                                name=\"alphas\")(psi_sel))\n",
    "#     alpha_row = Dense(row_w)(alpha_row)\n",
    "#     if reg:\n",
    "#         alpha_row = tensorflow.keras.layers.ActivityRegularization(l2=0.01/row_w)(alpha_row)\n",
    "  \n",
    "    kc_practice = Dense(row_w, use_bias=True, name=\"qk_loadings\")(hit_counter)\n",
    "#     kc_practice = Dense(row_w, use_bias=False, kernel_constraint=NonNeg(), name=\"qk_loadings\",)(hit_counter)\n",
    "#     kc_practice = Lambda(lambda x: K.log(x+1e-6))(kc_practice)\n",
    "#     kc_practice = Dense(row_w, kernel_constraint=NonNeg())(kc_practice)\n",
    "#     kc_practice = tensorflow.keras.layers.ActivityRegularization(l1=0.00001/row_w)(kc_practice)\n",
    "    \n",
    "#     psi_row = long_clip( add( [alpha_row, multiply([kc_practice, gamma_row])]) )\n",
    "#     psi_row = Dense(row_w, activation=\"linear\")(psi_row)\n",
    "\n",
    "    psi_row = add( [alpha_row, multiply([kc_practice, gamma_row])])\n",
    "#     psi_row = alpha_row\n",
    "    difs = subtract([psi_row, qn_row], name=\"difs\")\n",
    "#     difs = tensorflow.keras.layers.ActivityRegularization(l2=0.001/row_w)(difs)\n",
    "#     difs = Dense(row_w, activity_regularizer=L2(0.1/row_w))(difs)\n",
    "\n",
    "    Prs = Lambda(lambda z: K.sigmoid(z))(difs)\n",
    "    \n",
    "#     logs = Lambda(lambda ps: K.log(ps), name=\"log_step\")(Prs)\n",
    "#     summed_logs = Lambda(lambda ps: K.sum(ps, axis=-1, keepdims=True), name=\"sum_step\")(logs)\n",
    "#     score = Lambda(lambda ps: K.exp(ps), name=\"exp_step\")(summed_logs)\n",
    "\n",
    "    score = Lambda(lambda prs: K.prod(prs, axis=-1, keepdims=True))(Prs)\n",
    "#     score = Lambda(lambda prs: tensorflow.math.reduce_logsumexp(prs, axis=-1, keepdims=True))(Prs)\n",
    "\n",
    "    model = Model(inputs=[qn_sel, psi_sel, hit_counter], outputs=score)\n",
    "    model.compile(optimizer=\"adam\", loss=loss, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_MLTM0_model(n_questions, n_students, row_w, \n",
    "                        loss=\"binary_crossentropy\",\n",
    "                        metrics=None,\n",
    "                        l2=False):\n",
    "    print(\"ROW W is \", row_w)\n",
    "    # def generate_qs_model(qn_table, psi_table, optimiser, _mode=\"MXFN\", loss=\"MSE\"):  \n",
    "    from keras.initializers import RandomNormal, RandomUniform, Constant\n",
    "    from keras.regularizers import L1,L2\n",
    "    psi_sel = Input(shape=(1,), name=\"psi_select\", dtype=\"int32\")\n",
    "    qn_sel = Input(shape=(1,), name=\"q_select\", dtype=\"int32\")\n",
    "    hit_counter = Input(shape=(n_questions, ), name=\"hit_counter\", dtype=\"float32\")\n",
    "\n",
    "    sp = pr_to_spread(.5, row_w, as_A_and_D=False)\n",
    "    pos_clip = WeightClip(0.0000, 100)\n",
    "#     bin_clip = WeightClip(0.0000, 1.0000)\n",
    "\n",
    "    base = 1\n",
    "    q_init = RandomNormal(mean=base)\n",
    "    qn_emb = Embedding(n_questions, row_w, embeddings_initializer=q_init, \n",
    "#                        activity_regularizer=L1(1/row_w), \n",
    "                       name=\"qn_embedding\")\n",
    "    qn_row = Flatten(name=\"qn_row_out\")(pos_clip(qn_emb(qn_sel)))\n",
    "    \n",
    "# Q MAsking STilL reQuired\n",
    "    k=1000\n",
    "    qmask = Lambda(lambda x: K.clip(x*k,0.0000,1.0000))(qn_row)\n",
    "    \n",
    "    \n",
    "    w2 = 32e-8/row_w\n",
    "    print(\"using penalty\", w2)\n",
    "    alpha_row = Embedding(n_students, row_w, \n",
    "                          embeddings_initializer=RandomNormal(mean=base+sp), name=\"alphas\",\n",
    "                          embeddings_regularizer=L2(w2),\n",
    "                          )(psi_sel)\n",
    "    gamma_row = Embedding(n_students, row_w, name=\"gammas\")(psi_sel)\n",
    "    alpha_row = Flatten()(alpha_row)\n",
    "    gamma_row = Flatten()(gamma_row)\n",
    "\n",
    "    kc_practice = Dense(row_w, name=\"qk_loadings\", use_bias=False)(hit_counter)\n",
    "    psi_row = add( [alpha_row, multiply([kc_practice, gamma_row])], name=\"psi_row_out\")\n",
    "\n",
    "    difs = subtract([psi_row, qn_row], name=\"difs\")\n",
    "    Prs = Lambda(lambda z: K.sigmoid(z))(difs)\n",
    "    Prs = Lambda(lambda x: K.pow(x[0],K.abs(x[1])) )([Prs, qmask])\n",
    "\n",
    "    score = Lambda(lambda ps: K.prod(ps, axis=1, keepdims=True))(Prs)\n",
    "    \n",
    "    # p_LFA = σ(a_s + Σ k ∊ skills(q): 𝜸_k*n_sk - d_k)\n",
    "\n",
    "    model = Model(inputs=[qn_sel, psi_sel, hit_counter], outputs=score)\n",
    "    model.compile(optimizer=\"adam\", loss=loss, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def my_regularizer(x):\n",
    "#     return K.sum(K.square(x), axis=1)\n",
    "\n",
    "def generate_MLTMb_model(n_questions, n_students, row_w, \n",
    "                        loss=\"binary_crossentropy\",\n",
    "                        metrics=None,\n",
    "                        reg=None,\n",
    "                        reg_w = None):\n",
    "    print(\"MLTMb model with reg\", reg)\n",
    "    print(\"ROW W is \", row_w)\n",
    "    # def generate_qs_model(qn_table, psi_table, optimiser, _mode=\"MXFN\", loss=\"MSE\"):  \n",
    "    from keras.initializers import RandomNormal, RandomUniform, Constant\n",
    "    from keras.regularizers import L1,L2\n",
    "    psi_sel = Input(shape=(1,), name=\"psi_select\", dtype=\"int32\")\n",
    "    qn_sel = Input(shape=(1,), name=\"q_select\", dtype=\"int32\")\n",
    "    hit_counter = Input(shape=(n_questions, ), name=\"hit_counter\", dtype=\"float32\")\n",
    "\n",
    "    sp = pr_to_spread(.5, row_w, as_A_and_D=False)\n",
    "\n",
    "    zer0 = Lambda(lambda x: K.cast(K.clip(x,0,0), dtype=\"int32\"))\n",
    "    bin_clip = WeightClip(0.0000, 1.0000)\n",
    "#     lam_clip = Lambda(lambda x: K.clip(x,0.0000,1.0000))\n",
    "#     long_clip = WeightClip(-100,100)\n",
    "    \n",
    "    etas = Embedding(1, row_w, input_length=1, \n",
    "                     embeddings_initializer=RandomNormal(0),\n",
    "                     name=\"skill_diffs\")\n",
    "    \n",
    "    l2_w = reg_w if (reg_w is not None) else 1.5e-06\n",
    "    effs = Embedding(n_questions, row_w, \n",
    "#                      embeddings_constraint=WeightClip(0, 1), \n",
    "                     embeddings_initializer=RandomUniform(0.999,1), \n",
    "                     name=\"qn_embedding\",\n",
    "#                      embeddings_initializer = RandomNormal(1),\n",
    "#                      embeddings_regularizer=L2(10000.0/(n_questions*row_w)**2) if reg==\"l1\" else None,\n",
    "#                      embeddings_regularizer=L1(l2_w/(n_questions*row_w)) #if reg==\"l1\" else None,\n",
    "                        activity_regularizer=L1(1.55e-06) #if reg==\"l1\" else None,\n",
    "                     )\n",
    "    \n",
    "#     effs = Embedding(n_questions, row_w, name=\"qn_embedding\")\n",
    "\n",
    "#     qn_row = Flatten(name=\"qn_row_out\")(long_clip(etas(zer0(qn_sel))))\n",
    "    qn_row = Flatten(name=\"qn_row_out\")(etas(zer0(qn_sel)))\n",
    "\n",
    "#     delta_loading_is_qmask = Flatten(name=\"qmask_out\")(lam_clip(bin_clip(effs(qn_sel))))\n",
    "#     delta_loading_is_qmask = Flatten()(effs(qn_sel))\n",
    "    delta_loading_is_qmask = Flatten(name=\"qmask_out\")(bin_clip(effs(qn_sel)))\n",
    "\n",
    "# Q MAsking STilL reQuired\n",
    "    \n",
    "    w2 = 32*1.5e-6 / row_w\n",
    "    print(\"l2 penalty is\", w2)\n",
    "    alpha_row = Embedding(n_students, row_w, \n",
    "                          embeddings_initializer=RandomNormal(sp), name=\"alphas\",\n",
    "#                           embeddings_regularizer=L2(l2_w/(n_students*row_w)**2) if reg==\"l2\" else None,\n",
    "#                           embeddings_regularizer=L2(l2_w) if reg==\"l2\" else None,\n",
    "#                           embeddings_regularizer=L2(0.01) if reg==\"l2\" else None,\n",
    "#                             embeddings_regularizer=L2(w2)\n",
    "                         )(psi_sel)\n",
    "    gamma_row = Embedding(n_students, row_w, name=\"gammas\", \n",
    "                          embeddings_initializer=RandomNormal(0),\n",
    "                         )(psi_sel)\n",
    "    alpha_row = Flatten()(alpha_row)\n",
    "    gamma_row = Flatten()(gamma_row)\n",
    "\n",
    "    kc_practice = Dense(row_w, name=\"qk_loadings\", use_bias=False)(hit_counter)\n",
    "    psi_row = add( [alpha_row, multiply([kc_practice, gamma_row])], name=\"psi_row_out\")\n",
    "\n",
    "    difs = subtract([psi_row, qn_row], name=\"difs\")\n",
    "    Prs = Lambda(lambda z: K.sigmoid(z))(difs)\n",
    "    Prs = Lambda(lambda x: K.pow(x[0],K.abs(x[1])) )([Prs, delta_loading_is_qmask])\n",
    "    \n",
    "    score = Lambda(lambda ps: K.prod(ps, axis=1, keepdims=True))(Prs)\n",
    "    \n",
    "    # p_LFA = σ(a_s + Σ k ∊ skills(q): 𝜸_k*n_sk - d_k)\n",
    "\n",
    "    model = Model(inputs=[qn_sel, psi_sel, hit_counter], outputs=score)\n",
    "    model.compile(optimizer=\"adam\", loss=loss, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_CFM_model(n_questions, n_students, row_w, loss=\"binary_crossentropy\", \n",
    "                     sfocus=False, metrics=None, reg=None, reg_w=None):\n",
    "    print(\"Using CFM model!\")\n",
    "\n",
    "    print(\"ROW W is \", row_w)\n",
    "    from keras.initializers import RandomNormal, RandomUniform, Constant\n",
    "    from keras.layers import Add, Reshape, Multiply\n",
    "    from keras.regularizers import L1, L2\n",
    "    \n",
    "    psi_sel = Input(shape=(1,), name=\"psi_select\", dtype=\"int32\")\n",
    "    qn_sel = Input(shape=(1,), name=\"q_select\", dtype=\"int32\")\n",
    "    hit_counter = Input(shape=(n_questions, ), name=\"hit_counter\", dtype=\"float32\")\n",
    "\n",
    "    zer0 = Lambda(lambda x: K.cast(K.clip(x,0,0), dtype=\"int32\"))\n",
    "    bin_clip = WeightClip(0.0000, 1.0000)\n",
    "#     bin_clip = Lambda(lambda c: K.clip(c,0,1))\n",
    "    \n",
    "    reg_w = 0 if reg_w is None else reg_w\n",
    "    \n",
    "    sp = pr_to_spread(.5, row_w, as_A_and_D=False)\n",
    "    B = Embedding(1 , row_w, name=\"skill_diffs\", embeddings_initializer=RandomNormal(0, stddev=0.1))\n",
    "    theta_i = Flatten()(Embedding(n_students, 1, name=\"alphas\", \n",
    "                                  embeddings_initializer=RandomNormal(sp, stddev=0.1),\n",
    "#                                   embeddings_regularizer=L2(10),\n",
    "                                  embeddings_regularizer= L2(1.45e-05),# if (\"l2\" in reg) else None,\n",
    "#                                   embeddings_regularizer= L2(reg_w) if (\"l2\" in reg) else None,\n",
    "#                                   embeddings_regularizer= L2(1),# if (\"l2\" in reg) else None,\n",
    "                                 )  (psi_sel))\n",
    "\n",
    "    print(\"theta_i shape\", theta_i.shape)\n",
    "    \n",
    "    Q = Embedding(n_questions, row_w, name=\"qn_embedding\", \n",
    "#                        embeddings_constraint=WeightClip(0, 1), \n",
    "                         embeddings_initializer=RandomUniform(0.99,1),\n",
    "#                          embeddings_regularizer = L1(reg_w / (row_w*n_questions) )# if (\"l1\" in reg) else None,\n",
    "                 )\n",
    "        \n",
    "#     gamma_k = Flatten()(Embedding(1, row_w, name=\"gammas\", embeddings_initializer=RandomNormal(0),)  (zer0(qn_sel)))\n",
    "    gamma_k = Flatten()(Embedding(1, row_w, name=\"gammas\")  (zer0(qn_sel)))\n",
    "\n",
    "    beta_k = Flatten()(B( zer0(qn_sel) ))\n",
    "    q_jk = Flatten()(bin_clip( Q(qn_sel) ))\n",
    "\n",
    "    print(\"shape kc-gammas\", gamma_k.shape)\n",
    "    T_jk = Dense(row_w, name=\"qk_loadings\", use_bias=False)(hit_counter)\n",
    "\n",
    "    prog = multiply([gamma_k, T_jk])\n",
    "    \n",
    "    skill_now = Add()([theta_i, prog])\n",
    "    logit_difs = subtract([skill_now, beta_k])\n",
    "    \n",
    "    print(\"logit shape\", logit_difs.shape)\n",
    "    Prs = Lambda(lambda z: K.sigmoid(z))(logit_difs)\n",
    "    \n",
    "#     Prs = Lambda(lambda mx: K.pow(mx[0],mx[1]))([Prs, q_jk])\n",
    "    Prs = Lambda(lambda mx: mx[1]*mx[0] + (1-mx[1]) )([Prs, q_jk])\n",
    "        \n",
    "    print(\"Prs shape\", Prs.shape)\n",
    "    score = Lambda(lambda ps: K.prod(ps, axis=1, keepdims=True))(Prs)\n",
    "    \n",
    "    # p_LFA = σ(a_s + Σ k ∊ skills(q): 𝜸_k*n_sk - d_k)\n",
    "\n",
    "    model = Model(inputs=[qn_sel, psi_sel, hit_counter], outputs=score)\n",
    "    from keras.optimizers import SGD, RMSprop, Nadam, Adam\n",
    "    #   optr = SGD(learning_rate=1.0)#, momentum=0.01, nesterov=True)\n",
    "    #   optr = RMSprop()\n",
    "    optr = Adam()\n",
    "    model.compile(optimizer=optr, loss=loss, metrics=metrics)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cgHm-iOt55vE"
   },
   "outputs": [],
   "source": [
    "def create_AFM_model(n_questions, n_students, row_w, loss=\"binary_crossentropy\", \n",
    "                     sfocus=False, metrics=None, reg=None):\n",
    "    print(\"Using {}AFM model!\".format(\"S\" if sfocus else \"\"))\n",
    "    print(\"reg is\", reg)\n",
    "    \n",
    "    #AFM\n",
    "    # p_ij = sig( a_i + sum[k in KC(j)] b_k + g_k*n_ik )\n",
    "    \n",
    "    #sAFM\n",
    "    # p_ij = sig( sum[k in KC(j)] a_ik + b_k + g_k*n_ik )\n",
    "    \n",
    "    print(\"ROW W is \", row_w)\n",
    "    from keras.initializers import RandomNormal, RandomUniform, Constant\n",
    "    from keras.layers import Add, Reshape, Multiply\n",
    "    from keras.constraints import NonNeg\n",
    "    from keras.regularizers import L2,L1\n",
    "    hit_counter = Input(shape=(n_questions, ), name=\"hit_counter\", dtype=\"float32\")\n",
    "    psi_sel = Input(shape=(1,), name=\"psi_select\", dtype=\"int32\")\n",
    "    qn_sel = Input(shape=(1,), name=\"q_select\", dtype=\"int32\")\n",
    "\n",
    "    zer0 = Lambda(lambda x: K.cast(K.clip(x,0,0), dtype=\"int32\"))\n",
    "    bin_clip = WeightClip(0.0000,1.0000)\n",
    "    \n",
    "    skill_d_ws = Embedding(1 , row_w, name=\"skill_diffs\", embeddings_initializer=RandomNormal(0, stddev=0.1))\n",
    "#   a0 = 5\n",
    "    qn_mx = Embedding(n_questions , row_w, name=\"qn_embedding\", \n",
    "#                        embeddings_constraint=WeightClip(0, 1), \n",
    "                       embeddings_initializer=RandomUniform(0.99,1),\n",
    "#                        activity_regularizer=\"l1\" if (\"l1\" in reg) else None,\n",
    "                     )\n",
    "    \n",
    "    gamma_row = Embedding(1, row_w, name=\"gammas\",\n",
    "#                     embeddings_initializer=RandomNormal(0),\n",
    "                    )(zer0(psi_sel))\n",
    "    a0 = Embedding(n_students, 1, name=\"alphas\", \n",
    "                  embeddings_initializer=RandomNormal(0, stddev=0.1),\n",
    "                  embeddings_regularizer= L2(1.45e-05) if (\"l2\" in reg) else None,\n",
    "                  )(psi_sel)\n",
    "\n",
    "    etas = Flatten()( skill_d_ws(zer0(qn_sel)) ) \n",
    "    \n",
    "    q_mask = Flatten()( bin_clip(qn_mx(qn_sel)) )\n",
    "\n",
    "    gamma_row = Flatten()(gamma_row)\n",
    "    print(\"shape kc-gammas\", gamma_row.shape)\n",
    "    kc_practice = Dense(row_w, name=\"qk_loadings\", use_bias=False)(hit_counter)\n",
    "    print(\"shape kc-practice\", kc_practice.shape)\n",
    "    prac_modifier = Multiply()([kc_practice, gamma_row])\n",
    "\n",
    "    difs = subtract([prac_modifier, etas])\n",
    "    difs = Multiply()([difs, q_mask]) # mask off irrelevant KCs\n",
    "    \n",
    "    print(\"Difs shape\", difs.shape)\n",
    "    summed = Lambda(lambda ps: K.sum(ps, axis=1, keepdims=True), name=\"lambda_sums\")(difs)\n",
    "    print(\"Summed shape\", summed.shape)\n",
    "    \n",
    "    a0 = Flatten()(a0)\n",
    "    logit = Add()([a0, summed]) #in AFM we add a0 after the subtraction and masking\n",
    "        \n",
    "    print(\"logit shape\", logit.shape)\n",
    "    score = Lambda(lambda z: K.sigmoid(z))(logit)\n",
    "    \n",
    "    # p_LFA = σ(a_s + Σ k ∊ skills(q): 𝜸_k*n_sk - d_k)\n",
    "\n",
    "    model = Model(inputs=[qn_sel, psi_sel, hit_counter], outputs=score)\n",
    "    from keras.optimizers import SGD, RMSprop, Nadam, Adam\n",
    "    #   optr = SGD(learning_rate=1.0)#, momentum=0.01, nesterov=True)\n",
    "    #   optr = RMSprop()\n",
    "    optr = Adam()\n",
    "    model.compile(optimizer=optr, loss=loss, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def create_AFMg_orig_model(n_questions, n_students, row_w, loss=\"binary_crossentropy\", metrics=None, reg=None):\n",
    "    print(\"Using AFMg model!\")\n",
    "\n",
    "    from keras.initializers import RandomNormal, RandomUniform, Constant\n",
    "    from keras.layers import Add, Reshape, Multiply\n",
    "    from keras.constraints import NonNeg\n",
    "    from keras.optimizers import Adam\n",
    "    from keras.regularizers import L2\n",
    "    psi_sel = Input(shape=(1,), name=\"psi_select\", dtype=\"int32\")\n",
    "    qn_sel = Input(shape=(1,), name=\"q_select\", dtype=\"int32\")\n",
    "    hit_counter = Input(shape=(n_questions, ), name=\"hit_counter\", dtype=\"float32\")\n",
    "\n",
    "    zer0 = Lambda(lambda x: K.cast(K.clip(x,0,0), dtype=\"int32\"))\n",
    "    bin_clip = WeightClip(0.0000,1.0000)\n",
    "\n",
    "    q_row = Flatten()(bin_clip(Embedding(n_questions , row_w, name=\"qn_embedding\", \n",
    "                                         embeddings_initializer=RandomUniform(.99,1),\n",
    "                                         activity_regularizer= \"l1\" if (\"l1\" in reg) else None)(qn_sel)))\n",
    "\n",
    "    skill_d_ws = Embedding(1 , row_w, name=\"skill_diffs\")\n",
    "    base_deltas = Flatten()(skill_d_ws(zer0(qn_sel)))\n",
    "#     masked_deltas = multiply([q_row, base_deltas])\n",
    "#     delta = Lambda(lambda ps: K.sum(ps, axis=1, keepdims=True), name=\"sum_deltas\")(masked_deltas)\n",
    "\n",
    "    \n",
    "    gamma_row = Flatten()(Embedding(n_students, row_w, name=\"gammas\")(psi_sel))\n",
    "    \n",
    "    kc_practice = Dense(row_w, name=\"qk_loadings\", use_bias=False)(hit_counter)\n",
    "    prac_kc_mods = multiply([kc_practice, gamma_row])\n",
    "#     masked_kc_mods = multiply([prac_kc_mods, q_row])\n",
    "#     practice = Lambda(lambda ps: K.sum(ps, axis=1, keepdims=True), name=\"sum_kc_pracs\")(masked_kc_mods)\n",
    "    \n",
    "    kcwise_difs = subtract([prac_kc_mods, base_deltas])\n",
    "    masked_kcwise_difs = multiply([kcwise_difs, q_row])\n",
    "    dif = Lambda(lambda ps: K.sum(ps, axis=1, keepdims=True), name=\"sum_kc_pracs\")(masked_kcwise_difs)\n",
    "    \n",
    "    a0 = Flatten()(Embedding(n_students, 1, name=\"alphas\",          \n",
    "#                             embeddings_regularizer=L2(1/n_students) if (\"l2\" in reg) else None,\n",
    "                            embeddings_regularizer=L2(1.45e-05) if (\"l2\" in reg) else None,\n",
    "                            )(psi_sel))\n",
    "    logit_dif = add([a0, dif])\n",
    "\n",
    "    score = Lambda(lambda z: K.sigmoid(z))(logit_dif)\n",
    "    \n",
    "    model = Model(inputs=[qn_sel, psi_sel, hit_counter], outputs=score)\n",
    "    optr = Adam()\n",
    "    model.compile(optimizer=optr, loss=loss, metrics=metrics)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_AFMg_model(n_questions, n_students, row_w, loss=\"binary_crossentropy\", metrics=None, reg=None):\n",
    "    print(\"Using AFMg+ model!\")\n",
    "\n",
    "    from keras.initializers import RandomNormal, RandomUniform, Constant\n",
    "    from keras.layers import Add, Reshape, Multiply\n",
    "    from keras.constraints import NonNeg\n",
    "    from keras.optimizers import Adam\n",
    "    from keras.regularizers import L2\n",
    "    psi_sel = Input(shape=(1,), name=\"psi_select\", dtype=\"int32\")\n",
    "    qn_sel = Input(shape=(1,), name=\"q_select\", dtype=\"int32\")\n",
    "    hit_counter = Input(shape=(n_questions, ), name=\"hit_counter\", dtype=\"float32\")\n",
    "\n",
    "    zer0 = Lambda(lambda x: K.cast(K.clip(x,0,0), dtype=\"int32\"))\n",
    "    bin_clip = WeightClip(0.0000,1.0000)\n",
    "    pos_clip = WeightClip(0, math.inf)\n",
    "\n",
    "    sp = pr_to_spread(.5, row_w, as_A_and_D=False)\n",
    "    \n",
    "    q_row = Flatten()(bin_clip(Embedding(n_questions , row_w, name=\"qn_embedding\", \n",
    "                                         embeddings_initializer=RandomUniform(.99,1),\n",
    "                                        )(qn_sel)))\n",
    "#     if reg:\n",
    "    q_row = tensorflow.keras.layers.ActivityRegularization(l1=0.1/row_w)(q_row)\n",
    "\n",
    "    skill_d_ws = Embedding(1 , row_w, name=\"skill_diffs\", embeddings_initializer=RandomNormal(0))\n",
    "    base_deltas = Flatten()(skill_d_ws(zer0(qn_sel)))\n",
    "#     masked_deltas = multiply([q_row, base_deltas])\n",
    "#     delta = Lambda(lambda ps: K.sum(ps, axis=1, keepdims=True), name=\"sum_deltas\")(masked_deltas)\n",
    "\n",
    "    gamma_row = Flatten()(pos_clip(Embedding(n_students, row_w, name=\"gammas\", embeddings_initializer=RandomNormal(1))(psi_sel)))\n",
    "    \n",
    "    kc_practice = Dense(row_w, name=\"qk_loadings\", use_bias=False, kernel_constraint=NonNeg())(hit_counter)\n",
    "    prac_kc_mods = multiply([kc_practice, gamma_row])\n",
    "#     masked_kc_mods = multiply([prac_kc_mods, q_row])\n",
    "#     practice = Lambda(lambda ps: K.sum(ps, axis=1, keepdims=True), name=\"sum_kc_pracs\")(masked_kc_mods)\n",
    "    a0 = Flatten()(Embedding(n_students, row_w, name=\"alphas\",      \n",
    "                             embeddings_initializer=RandomNormal(sp),\n",
    "#                             embeddings_regularizer=L2(1/n_students) if (\"l2\" in reg) else None,\n",
    "#                              embeddings_regularizer=L2(1.45e-05) if (\"l2\" in reg) else None,\n",
    "                            )(psi_sel))\n",
    "    psi_row = add([a0, prac_kc_mods])\n",
    "    \n",
    "    kcwise_difs = subtract([psi_row, base_deltas])\n",
    "    Prs = Lambda(lambda z: K.sigmoid(z))(kcwise_difs)\n",
    "    \n",
    "#     masked_kcwise_difs = multiply([kcwise_difs, Pr])\n",
    "    Prs = Lambda(lambda ps: ps[0]*ps[1])([Prs, q_row])\n",
    "#     Prs = Lambda(lambda ps: K.pow(ps[0], ps[1]))([Prs, q_row])\n",
    "#     Prs = Lambda(lambda ps: ps[0]*ps[1] + (1-ps[1]) )([Prs, q_row])\n",
    "#     dif = Lambda(lambda ps: K.sum(ps, axis=1, keepdims=True), name=\"sum_kc_pracs\")(masked_kcwise_difs)\n",
    "  \n",
    "#     h = Dense(3, activation=\"relu\")(Prs)\n",
    "#     h = Dense(3, activation=\"relu\")(h)\n",
    "#     score = Dense(1, activation=\"sigmoid\")(h)\n",
    "\n",
    "    score = Lambda(lambda ps: K.prod(ps, axis=1, keepdims=True), name=\"score\")(Prs)\n",
    "\n",
    "    model = Model(inputs=[qn_sel, psi_sel, hit_counter], outputs=score)\n",
    "    optr = Adam()\n",
    "    model.compile(optimizer=optr, loss=loss, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RASCH_model(n_questions, n_students, loss=\"binary_crossentropy\", metrics=None, reg=None):\n",
    "    print(\"Using univariate Rasch model!\")\n",
    "\n",
    "    from keras.initializers import RandomNormal, RandomUniform, Constant\n",
    "    from keras.layers import Add, Reshape, Multiply\n",
    "    from keras.constraints import NonNeg\n",
    "    from keras.regularizers import L2\n",
    "    from keras.optimizers import Adam\n",
    "    psi_sel = Input(shape=(1,), name=\"psi_select\", dtype=\"int32\")\n",
    "    qn_sel = Input(shape=(1,), name=\"q_select\", dtype=\"int32\")\n",
    "    hit_counter = Input(shape=(n_questions, ), name=\"hit_counter\", dtype=\"float32\")\n",
    "\n",
    "#     if reg:\n",
    "#         rw = reg\n",
    "#     else:\n",
    "#         rw = 0.0001\n",
    "    \n",
    "    delta = Flatten()(Embedding(n_questions , 1, name=\"qn_embedding\")(qn_sel))\n",
    "    gamma_row = Flatten()(Embedding(n_students, 1, name=\"gammas\")(psi_sel))\n",
    "    a0 = Flatten()(Embedding(n_students, 1, \n",
    "#                              embeddings_regularizer = L2(rw),\n",
    "                             name=\"alphas\")(psi_sel))\n",
    "    \n",
    "#     log_hits_plus_one = hit_counter\n",
    "#     hcp1 = Lambda(lambda x: x)(hit_counter)\n",
    "#     kc_practice = Dense(10, use_bias=True)(hit_counter)\n",
    "    kc_practice = Dense(1, name=\"qk_loadings\", use_bias=False)(hit_counter)\n",
    "#     kc_practice = Lambda(lambda x: K.log(x+1))(kc_practice)\n",
    "\n",
    "    alpha = add([a0, multiply([kc_practice, gamma_row])])\n",
    "\n",
    "    logit_dif = subtract([alpha, delta])\n",
    "\n",
    "    score = Lambda(lambda z: K.sigmoid(z))(logit_dif)\n",
    "    \n",
    "\n",
    "    model = Model(inputs=[qn_sel, psi_sel, hit_counter], outputs=score)\n",
    "    optr = Adam()\n",
    "    model.compile(optimizer=optr, loss=loss, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_DEEPGAMMA_model(n_questions, n_students, row_w, ptqs=None, loss=f1_loss, non_neg=False,\n",
    "                       metrics=None, deep=False, concat=False, reg=None, reg_w=None):\n",
    "    print(\"MLP model\")\n",
    "    if reg==\"l2\":\n",
    "        print(\"L2 reg'n\", reg)\n",
    "    psi_sel = Input(shape=(1,), name=\"psi_select\", dtype=\"int32\")\n",
    "    qn_sel = Input(shape=(1,), name=\"q_select\", dtype=\"int32\")\n",
    "    hit_counter = Input(shape=(n_questions, ), name=\"hit_counter\", dtype=\"float32\")\n",
    "\n",
    "    sp = pr_to_spread(.5, row_w, as_A_and_D=False)\n",
    "\n",
    "#     q_init = RandomNormal(mean=0) if init50 else \"uniform\"\n",
    "    qn_emb = Embedding(n_questions, row_w, name=\"qn_embedding\", embeddings_initializer=RandomNormal(0))\n",
    "    qn_row = Flatten()(qn_emb(qn_sel))\n",
    "\n",
    "    l2_w = reg_w if (reg_w is not None) else 0.1\n",
    "    \n",
    "    from keras.regularizers import L2\n",
    "    #embeddings_initializer=RandomNormal(mean=1+sp)\n",
    "#     s_init = RandomNormal(mean=sp) #if init50 else \"uniform\"\n",
    "    gamma_row = Flatten()(Embedding(n_students, row_w, name=\"gammas\")(psi_sel))\n",
    "    alpha_row = Flatten()(Embedding(n_students, row_w, name=\"alphas\",\n",
    "                                embeddings_initializer=RandomNormal(sp),\n",
    "#                                 embeddings_regularizer=L2(l2_w/(n_students*row_w)) if reg==\"l2\" else None,\n",
    "#                                 embeddings_regularizer=L2(0.00001/(n_students*row_w)),# if reg==\"l2\" else None, \n",
    "                                )(psi_sel))\n",
    "#     alpha_row = tensorflow.keras.layers.ActivityRegularization(l2=0.01/row_w)(alpha_row)\n",
    "\n",
    "\n",
    "    kc_practice = Dense(row_w, name=\"qk_loadings\")(hit_counter)\n",
    "    learnage = Dense(row_w, activation=\"relu\")(concatenate([gamma_row, kc_practice]))\n",
    "#     learnage = Dense(row_w)(learnage)\n",
    "        \n",
    "#     kc_practice = Dense(row_w, name=\"qk_loadings\", use_bias=False, activation=\"relu\")(hit_counter)\n",
    "#     kc_practice = Lambda(lambda x: K.log(x+1))(kc_practice)\n",
    "#     log_hits_plus_one = Lambda(lambda x: K.log(x+1))(hit_counter)\n",
    "#     kc_practice = Dense(row_w, name=\"qk_loadings\", use_bias=False)(log_hits_plus_one)\n",
    "#     print(\"shape kc-practice\", kc_practice.shape)\n",
    "\n",
    "    psi_row = add( [alpha_row, learnage])\n",
    "#     psi_row = Dense(row_w, activation=\"relu\")(concatenate([alpha_row, learnage]))\n",
    "    \n",
    "    difs = subtract([psi_row, qn_row], name=\"difs\")\n",
    "#     difs = L2(0.001)(difs)\n",
    "    \n",
    "    Prs = difs\n",
    "#     Prs = Lambda(lambda z: K.sigmoid(z))(difs)\n",
    "    \n",
    "#     logs = Lambda(lambda ps: K.log(ps), name=\"log_step\")(Prs)\n",
    "#     summed_logs = Lambda(lambda ps: K.sum(ps, axis=-1, keepdims=True), name=\"sum_step\")(logs)\n",
    "#     score = Lambda(lambda ps: K.exp(ps), name=\"exp_step\")(summed_logs)\n",
    "\n",
    "#     score = Lambda(lambda prs: K.prod(prs, axis=-1, keepdims=True))(Prs)\n",
    "    Prs = Dense(5, activation=\"relu\")(Prs)\n",
    "    score = Dense(1, activation=\"sigmoid\")(Prs)\n",
    "\n",
    "    model = Model(inputs=[qn_sel, psi_sel, hit_counter], outputs=score)\n",
    "    \n",
    "    model.compile(optimizer=\"adam\", loss=loss, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_MLPraw_model(n_questions, n_students, row_w, ptqs=None, loss=f1_loss, non_neg=False,\n",
    "                       metrics=None, deep=False, concat=False, reg=None, reg_w=None, inc_dif=False):\n",
    "    print(\"MLP model\")\n",
    "    if reg==\"l2\":\n",
    "        print(\"L2 reg'n\", reg)\n",
    "    psi_sel = Input(shape=(1,), name=\"psi_select\", dtype=\"int32\")\n",
    "    qn_sel = Input(shape=(1,), name=\"q_select\", dtype=\"int32\")\n",
    "    hit_counter = Input(shape=(n_questions, ), name=\"hit_counter\", dtype=\"float32\")\n",
    "\n",
    "    qn_emb = Embedding(n_questions, row_w, name=\"qn_embedding\",\n",
    "#                        embeddings_initializer=RandomNormal(0)\n",
    "                      )\n",
    "    qn_row = Flatten()(qn_emb(qn_sel))\n",
    "#     qn_row = Dropout(0.5)(qn_row)\n",
    "\n",
    "    l2_w = reg_w if (reg_w is not None) else 0.1\n",
    "    \n",
    "    from keras.regularizers import L2\n",
    "    #embeddings_initializer=RandomNormal(mean=1+sp)\n",
    "#     s_init = RandomNormal(mean=sp) #if init50 else \"uniform\"\n",
    "    gamma_row = Flatten()(Embedding(n_students, row_w, name=\"gammas\")(psi_sel))\n",
    "    alpha_row = Flatten()(Embedding(n_students, row_w, name=\"alphas\",\n",
    "#                                 embeddings_initializer=RandomNormal(sp),\n",
    "#                                 embeddings_regularizer=L2(l2_w/(n_students*row_w)) if reg==\"l2\" else None,\n",
    "#                                 embeddings_regularizer=L2(5e-5/row_w),# if reg==\"l2\" else None, \n",
    "                                )(psi_sel))\n",
    "\n",
    "#     kch = Dropout(0.1)(hit_counter)\n",
    "    kch = hit_counter\n",
    "    kc_practice = Dense(row_w, name=\"qk_loadings\", use_bias=False)(kch)\n",
    "#     kc_practice = Dropout(0.5)(kc_practice)\n",
    "    print(\"shape kc-practice\", kc_practice.shape)\n",
    "    \n",
    "    psi_row = add( [alpha_row, multiply([kc_practice, gamma_row])])\n",
    "#     psi_row = Dropout(0.5)(psi_row)\n",
    "\n",
    "#     if deep:\n",
    "#         h = Dense(max(2,row_w//20), activation=\"relu\")(difs)\n",
    "# #         h = Dense(5, activation=\"relu\")(h)\n",
    "#     else:\n",
    "#     h = difs\n",
    "\n",
    "    hw = max(min(row_w,2),row_w//5)\n",
    "    print(\"hidden w is\", hw)\n",
    "#     h_act = \"sigmoid\"\n",
    "\n",
    "    difs = subtract([psi_row, qn_row], name=\"difs\")\n",
    "    if inc_dif==\"DenP\":\n",
    "#         ha = Dense(row_w)(psi_row)\n",
    "#         hd = Dense(row_w)(qn_row)\n",
    "#         h = concatenate([ha, hd], axis=1, name=\"concat_h\")\n",
    "        h = Dense(row_w)(difs)\n",
    "#         h = Dropout(0.75)(h)\n",
    "        score = Dense(1, activation=\"sigmoid\")(h)\n",
    "    elif inc_dif==\"Dense\":\n",
    "        ha = Dense(row_w)(psi_row)\n",
    "        hd = Dense(row_w)(qn_row)\n",
    "        h = concatenate([ha, hd], axis=1, name=\"concat_h\")\n",
    "#         h = Dropout(0.75)(h)\n",
    "        score = Dense(1, activation=\"sigmoid\")(h)\n",
    "    elif inc_dif==\"dif\":\n",
    "        h = difs\n",
    "#         h = Dense(hw, activation=h_act)(difs)\n",
    "        h = Dropout(0.75)(difs)\n",
    "#         h = Dense(hw, activation=h_act)(h)\n",
    "#         h = Dense(hw, activation=h_act)(h)\n",
    "        score = Dense(1, activation=\"sigmoid\")(h)\n",
    "    elif inc_dif==\"AD\":\n",
    "        del difs\n",
    "        h = concatenate([psi_row, qn_row], axis=1, name=\"concat_h\")\n",
    "        h = Dropout(0.75)(h)\n",
    "#         h = Dense(hw, activation=h_act)(h)\n",
    "#         h = Dense(hw, activation=h_act)(h)\n",
    "#         h = Dense(hw, activation=h_act)(h)\n",
    "        score = Dense(1, activation=\"sigmoid\")(h)\n",
    "    elif inc_dif==\"AD+dif\":\n",
    "#         difs = subtract([psi_row, qn_row], name=\"difs\")\n",
    "        h = concatenate([psi_row, qn_row, difs], axis=1, name=\"concat_h\")\n",
    "        h = Dropout(0.75)(h)\n",
    "#         h = Dense(hw, activation=h_act)(h)\n",
    "#         h = Dense(hw, activation=h_act)(h)\n",
    "#         h = Dense(hw, activation=h_act)(h)\n",
    "        score = Dense(1, activation=\"sigmoid\")(h)\n",
    "    else:\n",
    "        raise Exception(\"invalid concat model in MLPraw model gen:\", inc_dif)\n",
    "\n",
    "    model = Model(inputs=[qn_sel, psi_sel, hit_counter], outputs=score)\n",
    "    \n",
    "    model.compile(optimizer=\"adam\", loss=loss, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_MLP_model(n_questions, n_students, row_w, ptqs=None, loss=f1_loss, non_neg=False,\n",
    "                       metrics=None, deep=False, concat=False, reg=None, reg_w=None):\n",
    "    print(\"MLP model\")\n",
    "    if reg==\"l2\":\n",
    "        print(\"L2 reg'n\", reg)\n",
    "    psi_sel = Input(shape=(1,), name=\"psi_select\", dtype=\"int32\")\n",
    "    qn_sel = Input(shape=(1,), name=\"q_select\", dtype=\"int32\")\n",
    "    hit_counter = Input(shape=(n_questions, ), name=\"hit_counter\", dtype=\"float32\")\n",
    "\n",
    "    sp = pr_to_spread(.5, row_w, as_A_and_D=False)\n",
    "\n",
    "#     q_init = RandomNormal(mean=0) if init50 else \"uniform\"\n",
    "    qn_emb = Embedding(n_questions, row_w, name=\"qn_embedding\", embeddings_initializer=RandomNormal(0))\n",
    "    qn_row = Flatten()(qn_emb(qn_sel))\n",
    "\n",
    "    l2_w = reg_w if (reg_w is not None) else 0.1\n",
    "    \n",
    "    from keras.regularizers import L2\n",
    "    #embeddings_initializer=RandomNormal(mean=1+sp)\n",
    "#     s_init = RandomNormal(mean=sp) #if init50 else \"uniform\"\n",
    "    gamma_row = Flatten()(Embedding(n_students, row_w, name=\"gammas\")(psi_sel))\n",
    "    alpha_row = Flatten()(Embedding(n_students, row_w, name=\"alphas\",\n",
    "                                embeddings_initializer=RandomNormal(sp),\n",
    "#                                 embeddings_regularizer=L2(l2_w/(n_students*row_w)) if reg==\"l2\" else None,\n",
    "#                                 embeddings_regularizer=L2(5e-5/row_w),# if reg==\"l2\" else None, \n",
    "                                )(psi_sel))\n",
    "    alpha_row = tensorflow.keras.layers.ActivityRegularization(l2=0.01/row_w)(alpha_row)\n",
    "\n",
    "\n",
    "    kc_practice = Dense(row_w, name=\"qk_loadings\", use_bias=False)(hit_counter)\n",
    "\n",
    "#     kc_practice = Dense(row_w, name=\"qk_loadings\", use_bias=False, activation=\"relu\")(hit_counter)\n",
    "#     kc_practice = Lambda(lambda x: K.log(x+1))(kc_practice)\n",
    "#     log_hits_plus_one = Lambda(lambda x: K.log(x+1))(hit_counter)\n",
    "#     kc_practice = Dense(row_w, name=\"qk_loadings\", use_bias=False)(log_hits_plus_one)\n",
    "#     print(\"shape kc-practice\", kc_practice.shape)\n",
    "    \n",
    "    psi_row = add( [alpha_row, multiply([kc_practice, gamma_row])])\n",
    "\n",
    "    difs = subtract([psi_row, qn_row], name=\"difs\")\n",
    "#     difs = L2(0.001)(difs)\n",
    "    \n",
    "    score = Dense(1, activation=\"sigmoid\")(difs)\n",
    "\n",
    "    model = Model(inputs=[qn_sel, psi_sel, hit_counter], outputs=score)\n",
    "    \n",
    "    model.compile(optimizer=\"adam\", loss=loss, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_MLPd_model(n_questions, n_students, row_w, ptqs=None, loss=f1_loss, non_neg=False,\n",
    "                       metrics=None, deep=False, concat=False, reg=None):\n",
    "    print(\"MLP DEEP model\")\n",
    "    if reg==\"l2\":\n",
    "        print(\"L2 reg'n\")\n",
    "    psi_sel = Input(shape=(1,), name=\"psi_select\", dtype=\"int32\")\n",
    "    qn_sel = Input(shape=(1,), name=\"q_select\", dtype=\"int32\")\n",
    "    hit_counter = Input(shape=(n_questions, ), name=\"hit_counter\", dtype=\"float32\")\n",
    "\n",
    "#     sp = pr_to_spread(.5, row_w, as_A_and_D=False)\n",
    "\n",
    "#     long_clip = WeightClip(-100,100)\n",
    "\n",
    "#     q_init = RandomNormal(mean=0) if init50 else \"uniform\"\n",
    "    qn_emb = Embedding(n_questions, row_w, name=\"qn_embedding\")\n",
    "    qn_row = Flatten()(qn_emb(qn_sel))\n",
    "\n",
    "    from keras.regularizers import L2\n",
    "    from keras.layers import Dropout\n",
    "    gamma_row = Flatten()(Embedding(n_students, row_w, name=\"gammas\")(psi_sel))\n",
    "    alpha_row = Flatten()(Embedding(n_students, row_w, name=\"alphas\",\n",
    "                                   embeddings_initializer=RandomNormal(1),\n",
    "                                   embeddings_regularizer=L2(1.0/(n_students*row_w)) if reg==\"l2\" else None )(psi_sel))\n",
    "\n",
    "    kc_practice = Dense(row_w, name=\"qk_loadings\")(hit_counter)\n",
    "    print(\"shape kc-practice\", kc_practice.shape)\n",
    "    \n",
    "    psi_row = add( [alpha_row, multiply([kc_practice, gamma_row])])\n",
    "\n",
    "    difs = subtract([psi_row, qn_row], name=\"difs\")\n",
    "    h = Dense(row_w/2, activation=\"relu\")(difs)\n",
    "#     h = Dropout(0.2)(h)\n",
    "    h = Dense(4, activation=\"linear\")(h)\n",
    "#     h = Dropout(0.2)(h)\n",
    "    score = Dense(1, activation=\"sigmoid\")(h)\n",
    "\n",
    "    model = Model(inputs=[qn_sel, psi_sel, hit_counter], outputs=score)\n",
    "    \n",
    "    model.compile(optimizer=\"adam\", loss=loss, metrics=metrics)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_CONC_model(n_questions, n_students, row_w, ptqs=None, loss=f1_loss, non_neg=False,\n",
    "                       metrics=None, deep=False, concat=False, reg=None):\n",
    "    print(\"MLP CONC model\")\n",
    "    if reg==\"l2\":\n",
    "        print(\"L2 reg'n\")\n",
    "    psi_sel = Input(shape=(1,), name=\"psi_select\", dtype=\"int32\")\n",
    "    qn_sel = Input(shape=(1,), name=\"q_select\", dtype=\"int32\")\n",
    "    hit_counter = Input(shape=(n_questions, ), name=\"hit_counter\", dtype=\"float32\")\n",
    "\n",
    "#     sp = pr_to_spread(.5, row_w, as_A_and_D=False)\n",
    "\n",
    "    long_clip = WeightClip(-100,100)\n",
    "\n",
    "#     q_init = RandomNormal(mean=0) if init50 else \"uniform\"\n",
    "    qn_emb = Embedding(n_questions, row_w, name=\"qn_embedding\")\n",
    "    qn_row = Flatten()(long_clip(qn_emb(qn_sel)))\n",
    "\n",
    "    from keras.regularizers import L2\n",
    "    from keras.layers import Dropout, concatenate\n",
    "    gamma_row = Flatten()(Embedding(n_students, row_w, name=\"gammas\")(psi_sel))\n",
    "    alpha_row = Flatten()(Embedding(n_students, row_w, name=\"alphas\",\n",
    "                                   embeddings_initializer=RandomNormal(1),\n",
    "                                   embeddings_regularizer=L2(1/(n_students*row_w)) if reg==\"l2\" else None )(psi_sel))\n",
    "\n",
    "    kc_practice = Dense(row_w, name=\"qk_loadings\")(hit_counter)\n",
    "    print(\"shape kc-practice\", kc_practice.shape)\n",
    "    \n",
    "    psi_row = add( [alpha_row, multiply([kc_practice, gamma_row])])\n",
    "\n",
    "    conc = concatenate([psi_row, qn_row], name=\"conc\")\n",
    "#     h = Dense(10, activation=\"relu\")(conc)\n",
    "#     h = Dropout(0.2)(h)\n",
    "#     h = Dense(4, activation=\"relu\")(h)\n",
    "#     h = Dropout(0.2)(h)\n",
    "    score = Dense(1, activation=\"sigmoid\")(conc)\n",
    "\n",
    "    model = Model(inputs=[qn_sel, psi_sel, hit_counter], outputs=score)\n",
    "    \n",
    "    model.compile(optimizer=\"adam\", loss=loss, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from types import ModuleType, FunctionType\n",
    "from gc import get_referents\n",
    "\n",
    "# Custom objects know their class.\n",
    "# Function objects seem to know way too much, including modules.\n",
    "# Exclude modules as well.\n",
    "BLACKLIST = type, ModuleType, FunctionType\n",
    "\n",
    "\n",
    "def getsize(obj):\n",
    "    \"\"\"sum size of object & members.\"\"\"\n",
    "    if isinstance(obj, BLACKLIST):\n",
    "        raise TypeError('getsize() does not take argument of type: '+ str(type(obj)))\n",
    "    seen_ids = set()\n",
    "    size = 0\n",
    "    objects = [obj]\n",
    "    while objects:\n",
    "        need_referents = []\n",
    "        for obj in objects:\n",
    "            if not isinstance(obj, BLACKLIST) and id(obj) not in seen_ids:\n",
    "                seen_ids.add(id(obj))\n",
    "                size += sys.getsizeof(obj)\n",
    "                need_referents.append(obj)\n",
    "        objects = get_referents(*need_referents)\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "FdR1WiKecC8g",
    "outputId": "447bf9bb-72bb-4e31-e3f2-2667c0c0ae88",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset 5 : [[1.4645131]] 3.1089039466440367 1.4084517039946103\n",
      "[[ 1.37173388 -1.02864352  7.55703414 ... -1.49886535  1.53442336\n",
      "   1.55596703]\n",
      " [ 2.4635658   0.53089028 -5.45221188 ... -0.74608016  2.82944632\n",
      "   1.92081998]\n",
      " [ 1.55326451 -3.58004045 -1.89803685 ... -0.54779934  1.07594026\n",
      "  -1.57406956]\n",
      " ...\n",
      " [ 2.1023064   2.8166152  -0.4222581  ...  2.81755714 -6.81089099\n",
      "  -5.47685088]\n",
      " [-3.94892705 -1.47331223  1.72161864 ...  5.54785844  0.01453553\n",
      "  -1.07487148]\n",
      " [-2.68058906  0.55106707  0.27516104 ... -0.01492254  0.32854899\n",
      "  -0.09848295]]\n",
      "[[-10.         -10.         -10.         ... -10.         -10.\n",
      "  -10.        ]\n",
      " [-10.         -10.          -2.23522303 ... -10.         -10.\n",
      "  -10.        ]\n",
      " [-10.         -10.         -10.         ... -10.         -10.\n",
      "  -10.        ]\n",
      " ...\n",
      " [-10.         -10.         -10.         ... -10.         -10.\n",
      "  -10.        ]\n",
      " [-10.         -10.         -10.         ... -10.         -10.\n",
      "  -10.        ]\n",
      " [-10.         -10.         -10.         ... -10.         -10.\n",
      "  -10.        ]]\n"
     ]
    }
   ],
   "source": [
    "emb_w = 2\n",
    "import gc, zlib\n",
    "\n",
    "from collections import Counter\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping, LambdaCallback\n",
    "data_to_run = [5,]\n",
    "c= Counter()\n",
    "for a in data_to_run:\n",
    "    (tw,a0,a1, students_temp, qz_temp) = pickle.load(open(home+\"/synth_data/MLTM_10000_1000_(100_1_5)_{}.p\".format(a), \"rb\"))\n",
    "    print(\"loaded dataset\",a,\":\", a0,a1,tw)             \n",
    "    \n",
    "    n_students = 1000 # len(students2)\n",
    "    n_questions = 1000 #len(questions)\n",
    "    spars = 0.1\n",
    "\n",
    "    # from keras.models import load_model\n",
    "    fname = \"MLTM_1000_1000_(100_1_5)_sp100_5_0\"\n",
    "    #   m = load_model(home+\"/models/\" + fname, custom_objects={'WeightClip': WeightClip})\n",
    "    \n",
    "    print(students_temp)\n",
    "    print(qz_temp)\n",
    "    \n",
    "    \n",
    "#     c[a] += 1\n",
    "\n",
    "#     n_students = 500 # len(students2)\n",
    "#     n_questions = 500 #len(questions)\n",
    "#     students2 = students_temp[0:n_students]\n",
    "#     questions = qz_temp[0:n_questions]\n",
    "\n",
    "#     curr_min = numpy.min(students2)\n",
    "#     start = curr_min - .1\n",
    "\n",
    "#     #   pre_trained_qns = m.get_weights()[1]\n",
    "#     #   print(pre_trained_qns.shape)\n",
    "#     m = None\n",
    "\n",
    "#     print(0.01**(1/3))\n",
    "#     # p = 1/(1+e-z)\n",
    "#     print(pr_to_spread(0.215, as_A_and_D=False))#  -1.295\n",
    "#     # print(a0)\n",
    "\n",
    "#     longitude = 12 # number of steps we assume students have been in play\n",
    "#     # for s in students2:\n",
    "#     #   print((s-start)/longitude)\n",
    "\n",
    "#     students_start = numpy.zeros_like(students2) + start\n",
    "#     # gammas = (students2 - start)/longitude\n",
    "#     gammas = numpy.random.uniform(low=0.01, high=2.4, size=(n_students, emb_w))\n",
    "#     print(\"Gammas:\", numpy.min(gammas), numpy.max(gammas), numpy.mean(gammas), numpy.median(gammas))\n",
    "\n",
    "#     fname = \"Longitudinal_{}_{}_(100_1_5)_run={}\".format(n_students, n_questions, a)\n",
    "#     # try:\n",
    "\n",
    "#     # try:\n",
    "#     #   o_hits\n",
    "#     # except NameError:\n",
    "# #     if not o_hits:\n",
    "# #     (sixs, qixs, hits, hout) = pickle.load(open(home+\"/synth_data/\" + fname + \".p\", \"rb\"))\n",
    "# #     (sixs, qixs, hout) = pickle.load(open(home+\"/real_data/XL1041.p\", \"rb\"))\n",
    "#     # render_student_histories(sixs, qixs, hits, hout)\n",
    "#     questions = None\n",
    "#     students2 = None\n",
    "#     students_temp = None\n",
    "#     qz_temp = None\n",
    "\n",
    "#     # pre_trained_qns = None\n",
    "#     gc.collect()\n",
    "#     o_hits=[]\n",
    "#     t_hits=[]\n",
    "\n",
    "#     odata, vdata, tdata, sid_six_lookup, qid_qix_lookup = split_next_step(sixs, qixs, hout, \n",
    "#                                                                           max_students = 100000,\n",
    "#                                                                           min_hist = 40,\n",
    "#                                                                           max_hist = None)#, alternate=True, balance_training=False)\n",
    "#     (o_sixs, o_qixs, o_hits, o_out), (v_sixs, v_qixs, v_hits, v_out), (t_sixs, t_qixs, t_hits, t_out) = (odata, vdata, tdata)\n",
    "#     print(\"len o_sixes\", len(o_sixs))\n",
    "\n",
    "#     n_questions = len(set(qixs))\n",
    "#     n_students = len(set(sixs))\n",
    "\n",
    "#     # raise Exception(\"DELIBERATE EXCEPTION CALLED\")\n",
    "\n",
    "# #     def uncomp(chits):\n",
    "# #         for hix, chrow in enumerate(chits):\n",
    "# #             chits[hix] = pickle.loads(zlib.decompress(chrow))\n",
    "# #         chits = numpy.array(chits, dtype=\"uint8\")\n",
    "# #         return chits\n",
    "\n",
    "# #     print(\"uncomping\")\n",
    "# #     o_hits = uncomp(o_chits)\n",
    "# #     v_hits = uncomp(v_chits)\n",
    "# #     t_hits = uncomp(t_chits)\n",
    "# #     print(\"straight outta comp-ton\")\n",
    "\n",
    "#   # o_chits, v_chits, t_chits = None, None, None\n",
    "\n",
    "#   # except:\n",
    "#   # # if True:\n",
    "#   #   av_sc, sixs, qixs, hits, hout = run_data(students_start, questions, gammas, model_to_train=None)\n",
    "#   #   #   if s > 100:\n",
    "#   #   #     break\n",
    "#   #     # print(h)\n",
    "#   #     # print(r)\n",
    "#   #     # print(\"***\")\n",
    "\n",
    "#   #   gc.collect()\n",
    "#   #   sixs = numpy.array(sixs, dtype=\"uint16\")\n",
    "#   #   qixs = numpy.array(qixs, dtype=\"uint16\")\n",
    "#   #   hits = numpy.array(hits, dtype=\"uint8\")\n",
    "#   #   for hix, hrow in enumerate(hits):\n",
    "#   #     compd = zlib.compress(pickle.dumps(hrow))\n",
    "#   #     chits[hix] = compd\n",
    "#   #   hout = numpy.array(hout, dtype=\"uint8\")\n",
    "#   #   pickle.dump((sixs, qixs, chits, hout), open(home+\"/synth_data/\" + fname + \".p\", \"wb\"))\n",
    "#   #   chits = None\n",
    "\n",
    "#   # for h in hits[0:10]:\n",
    "#   #     print(h)\n",
    "\n",
    "#   # raise Exception(\"GAR\")\n",
    "\n",
    "#   # for s,q,h,r in zip(sixs, qixs, hits, hout):\n",
    "#   #   print(s,q, r)\n",
    "#   # print(hout)\n",
    "#   # print(int(sum(hout)))\n",
    "#   # print(len(hout))\n",
    "#   # raise Exception(\"DELIBERATE EXCEPTION CALLED\")\n",
    "\n",
    "#   # qlayer = m.get_layer(\"qn_embedding\")\n",
    "#   # print(qlayer.shape)\n",
    "#   # m.get_layer(\"qn_embedding\").set_weights(pre_trained_qns)\n",
    "#   # m.get_layer(\"qn_embedding\").trainable=False\n",
    "\n",
    "\n",
    "#   # hin2 = hin #numpy.array(hin).reshape(-1,(1,1,n_questions))\n",
    "#   # hout2 = numpy.array(hout).reshape(-1,1)\n",
    "\n",
    "#     n_to_keep = 10000\n",
    "\n",
    "#   # qixs = qixs[0:n_to_keep]\n",
    "#   # sixs = sixs[0:n_to_keep]  \n",
    "#   # hits = hits[0:n_to_keep]\n",
    "#   # hout = hout[0:n_to_keep]\n",
    "\n",
    "#   # n_questions = len(numpy.unique(o_qixs))\n",
    "#   # n_students  = len(numpy.unique(o_sixs))\n",
    "\n",
    "\n",
    "\n",
    "#   # n_questions = max(max(o_qixs),max(v_qixs),max(t_qixs))+1\n",
    "#   # n_students = max(max(o_sixs),max(v_sixs),max(t_sixs))+1\n",
    "\n",
    "\n",
    "\n",
    "#   # m = generate_longitudinal_model(n_questions, n_students, emb_w, None) #pre_trained_qns[0:n_questions])\n",
    "\n",
    "#     n_to_test = 1000 #len(hout)//10\n",
    "#     test_choices = numpy.random.choice(range(len(hout)), size=n_to_test)\n",
    "#   # t_in = hin2[test_choices, :]\n",
    "\n",
    "#   # t_hits = hits[test_choices]\n",
    "#   # o_hits = numpy.delete(hits, test_choices, axis=0)\n",
    "\n",
    "#   # qs_in_trimmed_data =  numpy.unique(qixs)\n",
    "#   # t_hits = numpy.array([hits[ix] for ix in test_choices])\n",
    "#   # o_hits = numpy.delete(hits, test_choices, axis=0)\n",
    "#   # hits=None\n",
    "#   # t_hits = t_hits[:, qs_in_trimmed_data]\n",
    "#   # t_hits = t_hits.reshape(-1, len(qs_in_trimmed_data))\n",
    "#   # o_hits = o_hits[:, qs_in_trimmed_data]\n",
    "#   # o_hits = o_hits.reshape(-1, len(qs_in_trimmed_data))\n",
    "\n",
    "#     gc.collect()\n",
    "\n",
    "#   # t_out = hout[test_choices, :]\n",
    "#   # t_out = numpy.array([hout[ix] for ix in test_choices]).reshape(-1,1)\n",
    "#   # o_out = numpy.delete(hout, test_choices, axis=0).reshape(-1,1)\n",
    "#   # hout=None\n",
    "\n",
    "\n",
    "#   # t_sixs = numpy.array([sixs[ix] for ix in test_choices]).reshape(-1,1)\n",
    "#   # # t_sixs = sixs[test_choices, :]\n",
    "#   # o_sixs = numpy.delete(sixs, test_choices, axis=0).reshape(-1,1)\n",
    "#   # sixs=None\n",
    "\n",
    "#   # t_qixs = numpy.array([qixs[ix] for ix in test_choices]).reshape(-1,1)\n",
    "#   # # t_qixs = qixs[test_choices, :]\n",
    "#   # o_qixs = numpy.delete(qixs, test_choices, axis=0).reshape(-1,1)\n",
    "\n",
    "#     print(\"MM\", numpy.min(o_qixs), numpy.max(o_qixs))\n",
    "#     qixs=None\n",
    "  \n",
    "#   # o_in  = numpy.delete(hin2, test_choices, axis=0)\n",
    "  \n",
    "#   # hazard_model = Model(inputs=m.input,\n",
    "#   #                         outputs=m.get_layer(\"alphas\").output)\n",
    "# #         intermediate_output = intermediate_layer_model.predict([qz,sz])\n",
    "       \n",
    "    \n",
    "    \n",
    "# #         print_prs = LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "# # #                                        print(numpy.min(intermediate_layer_model.predict([qz,sz])),\n",
    "# # #                                              numpy.max(intermediate_layer_model.predict([qz,sz]))))\n",
    "# #                                        print(prs_model.predict([qz[0:10],sz[0:10]])))\n",
    "\n",
    "#   # ixs = o_out > 0.5\n",
    "#     ixs = [True if o_out[n]>=0.5 else False for n in range(len(o_out))]\n",
    "#   # print(ixs)\n",
    "#   # print(o_out[ixs])\n",
    "#   # print(o_qixs[ixs])\n",
    "#   # print(o_sixs[ixs])\n",
    "#   # print(o_hits[ixs])\n",
    "#     print_hazard = LambdaCallback(on_epoch_end=lambda batch, logs:\n",
    "#                                   # print(hazard_model.predict([o_qixs[ixs], o_sixs[ixs], o_hits[ixs]])))\n",
    "#                                   print(m.predict([o_qixs[ixs], o_sixs[ixs], o_hits[ixs]]), o_out[ixs]))\n",
    "\n",
    "#         # print_zmask = LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "#         #                                print(z_model.predict([qz[0:10],sz[0:10]])))\n",
    "\n",
    "#   # n_ones  = int(numpy.sum(o_out, axis=0))\n",
    "#   # n_zeros = len(o_out) - n_ones\n",
    "\n",
    "#   # n_ones =  sum([1 if o_out[n]>=0.5 else 0 for n in range(len(o_out))])\n",
    "#   # n_zeros = sum([1 if o_out[n]<0.5 else 0 for n in range(len(o_out))])\n",
    "\n",
    "# #   geschichte = m.fit([o_qixs, o_sixs, o_hits], o_out, epochs=10000, validation_split=0.01, callbacks=[es], shuffle=True, class_weight=class_weightz)\n",
    "\n",
    "# print(type(o_sixs))\n",
    "# print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "iOhg-f9pbesM",
    "outputId": "10322f14-0681-4aad-e50f-cce06b8e3c60",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.metrics import binary_accuracy\n",
    "def gen_and_train(odata, vdata, t_data, emb_w=10, draw=False, cog_model=\"MLP\", q_weight=None, \n",
    "                  monitor=None, balance_classes=True, \n",
    "                  loss=None, metrics=None, reg_w=None):\n",
    "    from keras.callbacks import EarlyStopping, LambdaCallback\n",
    "\n",
    "    reg=\"\"\n",
    "    \n",
    "    (o_sixs, o_qixs, o_hits, o_out), (v_sixs, v_qixs, v_hits, v_out) = (odata, vdata)\n",
    "    \n",
    "#     emb_w = 4\n",
    "    m = None\n",
    "    \n",
    "    config_dict = {}    \n",
    "    config_dict[\"cog_model\"] = cog_model\n",
    "    config_dict[\"emb_w\"]     = emb_w\n",
    "    \n",
    "#     n_ones = int(sum(numpy.ravel(numpy.round(o_out))))\n",
    "#     n_zeros = len(o_out) - n_ones\n",
    "#     print(len(o_out))\n",
    "#     print(\"1s:\", n_ones)\n",
    "#     print(\"0s:\", n_zeros)\n",
    "\n",
    "# #     # minority_w = .75\n",
    "#     if n_zeros > n_ones:\n",
    "#         zero_w = 1.0\n",
    "#         one_w = n_zeros/n_ones\n",
    "#     else:\n",
    "#         zero_w = n_ones/n_zeros\n",
    "#         one_w = 1.0\n",
    "\n",
    "#     class_weightz0 = {\n",
    "#         0: zero_w,\n",
    "#         1: one_w,\n",
    "#     }\n",
    "    \n",
    "#     print(\"cw0\")\n",
    "#     print(class_weightz0)\n",
    "\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    class_weightz = compute_class_weight(\"balanced\", [0,1], o_out)\n",
    "    class_weightz = class_weightz / min(class_weightz)\n",
    "    print(\"class weights:\", class_weightz)\n",
    "    cw0 = {}\n",
    "    for cwix,cw in enumerate(class_weightz):\n",
    "        cw0[cwix] = cw\n",
    "    class_weightz = cw0\n",
    "    print(\"class weights (dict):\", class_weightz)\n",
    "    \n",
    "    # o_out = o_out.reshape(-1,1)\n",
    "    o_out = o_out.astype(float)\n",
    "\n",
    "    n_questions = max(max(o_qixs),max(v_qixs),max(t_qixs))+1\n",
    "    n_students = max(max(o_sixs),max(v_sixs),max(t_sixs))+1\n",
    "\n",
    "#     n_questions = int(o_hits.shape[1])\n",
    "#     n_students = int(max(o_sixs)+1)\n",
    "    print(\"nq, ns\")\n",
    "    print(n_questions, n_students)\n",
    "    \n",
    "    if metrics is None:\n",
    "        metrics = [binary_crossentropy, binary_accuracy, mean_absolute_error, mean_squared_error, f1_loss]\n",
    "#     lozz = \"binary_crossentropy\"\n",
    "    if loss==\"f1_loss\":\n",
    "        loss = f1_loss \n",
    "#     elif loss ==\"f1_loss_micro\":\n",
    "#         loss = f1_loss_micro\n",
    "    if cog_model==\"MLPraw\":\n",
    "        m = generate_MLPraw_model(n_questions, n_students, emb_w, ptqs=None, loss=loss, non_neg=False, \n",
    "                                   metrics=metrics, deep=False, reg=reg, reg_w=reg_w, inc_dif=\"dif\")\n",
    "    elif cog_model==\"MLPrawAD\":\n",
    "        m = generate_MLPraw_model(n_questions, n_students, emb_w, ptqs=None, loss=loss, non_neg=False, \n",
    "                                   metrics=metrics, deep=False, reg=reg, reg_w=reg_w, inc_dif=\"AD\")\n",
    "    elif cog_model==\"MLPrawADD\":\n",
    "        m = generate_MLPraw_model(n_questions, n_students, emb_w, ptqs=None, loss=loss, non_neg=False, \n",
    "                                   metrics=metrics, deep=False, reg=reg, reg_w=reg_w, inc_dif=\"AD+dif\")\n",
    "    elif cog_model==\"MLPrawDen\":\n",
    "        m = generate_MLPraw_model(n_questions, n_students, emb_w, ptqs=None, loss=loss, non_neg=False, \n",
    "                               metrics=metrics, deep=False, reg=reg, reg_w=reg_w, inc_dif=\"Dense\")\n",
    "    elif cog_model==\"MLPrawDP\":\n",
    "        m = generate_MLPraw_model(n_questions, n_students, emb_w, ptqs=None, loss=loss, non_neg=False, \n",
    "                               metrics=metrics, deep=False, reg=reg, reg_w=reg_w, inc_dif=\"DenP\")\n",
    "    elif cog_model==\"DEEPGAMMA\":\n",
    "        m = generate_DEEPGAMMA_model(n_questions, n_students, emb_w, ptqs=None, loss=loss, non_neg=False, \n",
    "                               metrics=metrics, deep=False, reg=reg, reg_w=reg_w)\n",
    "    elif cog_model[0:3]==\"CFM\":\n",
    "        sfoc = False\n",
    "        reg = \"\"\n",
    "        rw  = None\n",
    "        if len(cog_model)==4:\n",
    "            if cog_model[3] == \"z\":\n",
    "                reg = \"l2\"\n",
    "                rw = reg_w\n",
    "            elif cog_model[3] == \"l\":\n",
    "                reg = \"l1\"\n",
    "                rw = reg_w\n",
    "            else:\n",
    "                reg = \"\"\n",
    "                rw  = None\n",
    "        m = create_CFM_model(n_questions, n_students, emb_w, loss=loss, sfocus=sfoc, metrics=metrics, reg=reg, reg_w=rw)\n",
    "#     elif cog_model[0:5]==\"AFMsg\":\n",
    "#         m = create_AFMsg_model(n_questions, n_students, emb_w, loss=loss, metrics=metrics)\n",
    "    elif cog_model==\"RASCHz\":\n",
    "        rw = 5e-8\n",
    "        m = create_RASCH_model(n_questions, n_students, loss=loss, metrics=metrics, reg=rw)\n",
    "    elif cog_model==\"RASCH\":\n",
    "        m = create_RASCH_model(n_questions, n_students, loss=loss, metrics=metrics)\n",
    "    elif cog_model.startswith(\"AFMx\"):\n",
    "        if len(cog_model)==5:\n",
    "            if cog_model[4]==\"z\":\n",
    "                reg=\"l2\"\n",
    "            elif cog_model[4]==\"l\":\n",
    "                reg=\"l1\"\n",
    "            else:\n",
    "                reg=None\n",
    "        m = create_AFMx_model(n_questions, n_students, emb_w, loss=loss, metrics=metrics, reg=reg)\n",
    "    elif cog_model.startswith(\"AFMg\"):\n",
    "        if len(cog_model)==5:\n",
    "            if cog_model[4]==\"z\":\n",
    "                reg=\"l2\"\n",
    "            elif cog_model[4]==\"l\":\n",
    "                reg=\"l1\"\n",
    "            else:\n",
    "                reg=None\n",
    "        m = create_AFMg_model(n_questions, n_students, emb_w, loss=loss, metrics=metrics, reg=reg)\n",
    "    elif cog_model.startswith(\"AFM\"):\n",
    "        if len(cog_model)==4:\n",
    "            if cog_model[3]==\"z\":\n",
    "                reg=\"l2\"\n",
    "            elif cog_model[3]==\"l\":\n",
    "                reg=\"l1\"\n",
    "            else:\n",
    "                reg=None\n",
    "        m = create_AFM_model(n_questions, n_students, emb_w, loss=loss, sfocus=False, metrics=metrics, reg=reg)\n",
    "\n",
    "    elif cog_model==\"MLTM0z\":\n",
    "        m = generate_MLTM0_model(n_questions, n_students, emb_w, loss=loss, metrics=metrics, l2=True)\n",
    "    elif cog_model==\"MLTM0\":\n",
    "        m = generate_MLTM0_model(n_questions, n_students, emb_w, loss=loss, metrics=metrics, l2=False)\n",
    "    elif cog_model==\"MLTMz\":\n",
    "        m = generate_MLTM_raw_model(n_questions, n_students, emb_w, loss=loss, metrics=metrics, reg=\"l2\", reg_w=reg_w)\n",
    "    elif cog_model==\"MLTMl\":\n",
    "        m = generate_MLTM_raw_model(n_questions, n_students, emb_w, loss=loss, metrics=metrics, reg=\"l1\")\n",
    "    elif cog_model==\"MLTMp\":\n",
    "        print(\"TARTOLA\")\n",
    "        m = generate_MLTMp_model(n_questions, n_students, emb_w, loss=loss, metrics=metrics)    \n",
    "    elif cog_model==\"MLTMa\":\n",
    "        m = generate_MLTMa_model(n_questions, n_students, emb_w, loss=loss, metrics=metrics)\n",
    "    elif cog_model.startswith(\"MLTMb\"):\n",
    "        print(\"KOG MOD\", cog_model)\n",
    "        print(cog_model[-1])\n",
    "        reg = \"l2\" if cog_model[-1]=='z' else (\"l1\" if cog_model[-1]=='l' else None)\n",
    "        print(\"REGG:\", reg)\n",
    "        m = generate_MLTMb_model(n_questions, n_students, emb_w, loss=loss, metrics=metrics, \n",
    "                                 reg=reg, reg_w=reg_w)\n",
    "    elif cog_model==\"MLTM\":\n",
    "        print(\"WAnkOPHONE\")\n",
    "        print(\"RAW w init50\")\n",
    "        m = generate_MLTM_raw_model(n_questions, n_students, emb_w, ptqs=None, loss=loss, metrics=metrics, \n",
    "                                    init50=True, reg=None)\n",
    "    elif cog_model.startswith(\"CONC\"):\n",
    "        if cog_model[-1]==\"z\":\n",
    "            reg=\"l2\"\n",
    "        else:\n",
    "            reg=None\n",
    "        m = generate_CONC_model(n_questions, n_students, emb_w, ptqs=None, loss=loss, non_neg=False, metrics=metrics, reg=reg)\n",
    "    elif cog_model.startswith(\"MLP\"):\n",
    "        if cog_model[-1]==\"z\":\n",
    "            reg=\"l2\"\n",
    "        else:\n",
    "            reg=None\n",
    "        if cog_model.startswith(\"MLPd\"):\n",
    "            m = generate_MLPd_model(n_questions, n_students, emb_w, ptqs=None, loss=loss, non_neg=False, metrics=metrics, reg=reg)\n",
    "#         elif cog_model.startswith(\"MLPs\"):\n",
    "#             print(\"MLPS MODELSSQQQ\")\n",
    "#             m = generate_MLP2_model(n_questions, n_students, emb_w, ptqs=None, loss=loss, non_neg=False, metrics=metrics, reg=reg)\n",
    "        else:\n",
    "            m = generate_MLP_model(n_questions, n_students, emb_w, ptqs=None, loss=loss, non_neg=False, \n",
    "                                   metrics=metrics, deep=False, reg=reg, reg_w=reg_w)\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"Unknown cognitive model {}\".format(cog_model))\n",
    "\n",
    "    # t_hits = t_hits[:, 0:n_questions]\n",
    "\n",
    "    print(\"TRAINING:\")\n",
    "    print(\"Unique students:\", n_students)\n",
    "    print(\"Unique questions:\", n_questions)\n",
    "    print(\"Total activity:\", len(o_out), \"(\",numpy.sum(o_out),\")\")\n",
    "\n",
    "    print(\"ov shape\", o_hits.shape, v_hits.shape)\n",
    "\n",
    "    if monitor==\"val_accuracy\":\n",
    "        mon_mode = \"max\"\n",
    "        bl = -math.inf\n",
    "    elif monitor ==\"val_f1_loss\":\n",
    "        mon_mode = \"min\"\n",
    "        bl = math.inf\n",
    "    elif monitor[-5:] == \"error\":\n",
    "        mon_mode = \"min\"\n",
    "        bl = math.inf\n",
    "    elif monitor[-4:] ==\"loss\":\n",
    "        mon_mode = \"min\"\n",
    "        bl = math.inf\n",
    "    elif monitor == \"binary_crossentropy\":\n",
    "        mon_mode = \"min\"\n",
    "        bl = math.inf\n",
    "        \n",
    "    print(\"monitoring info\", monitor, mon_mode)\n",
    "#     es = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True, baseline=math.inf)\n",
    "    \n",
    "    es = EarlyStopping(monitor=monitor, patience=15, restore_best_weights=True, mode=mon_mode, baseline=bl)\n",
    "    \n",
    "#     es = EarlyStopping(monitor=\"val_accuracy\", patience=10, restore_best_weights=True, baseline=-math.inf)\n",
    "#     es = EarlyStopping(monitor=\"val_f1_m\", patience=10, restore_best_weights=True, mode=\"max\")\n",
    "\n",
    "#     print(\"check assertions\")\n",
    "#     assert not numpy.any(numpy.isnan(o_qixs))\n",
    "#     assert not numpy.any(numpy.isnan(o_sixs))\n",
    "#     assert not numpy.any(numpy.isnan(o_hits))\n",
    "#     assert not numpy.any(numpy.isnan(o_out))\n",
    "\n",
    "    bs = len(o_out)\n",
    "    \n",
    "#     from keras.utils import plot_model\n",
    "#     if draw:\n",
    "#         print(\"plotting model\")\n",
    "#         plot_model(m, to_file=cog_model+str(emb_w)+\".png\", show_shapes=True, show_layer_names=True)\n",
    "#         return m, None, config_dict\n",
    "    \n",
    "\n",
    "    o_qixs = o_qixs.reshape(-1,1)\n",
    "    o_sixs = o_sixs.reshape(-1,1)\n",
    "    o_hits = o_hits.reshape(-1, n_questions)\n",
    "    o_out = o_out.reshape(-1,1)\n",
    "\n",
    "    v_qixs = v_qixs.reshape(-1,1)\n",
    "    v_sixs = v_sixs.reshape(-1,1)\n",
    "    v_hits = v_hits.reshape(-1, n_questions)\n",
    "    v_out = v_out.reshape(-1,1)\n",
    "\n",
    "    \n",
    "    lcb = keras.callbacks.LambdaCallback(\n",
    "#         on_epoch_begin= lambda e,l: print(\"Begin\", m.get_layer(\"qn_embedding\").get_weights()[0]),\n",
    "        on_epoch_end= lambda e,l: print(\"Begin\", m.get_layer(\"qn_embedding\").get_weights()[0]),\n",
    "    )\n",
    "    \n",
    "    print(m.summary())\n",
    "    print(\"fitting\")\n",
    "\n",
    "    assert class_weightz is not None\n",
    "    assert o_qixs is not None\n",
    "    assert o_sixs is not None\n",
    "    assert o_hits is not None\n",
    "    assert o_out is not None\n",
    "    assert v_qixs is not None\n",
    "    assert v_sixs is not None\n",
    "    assert v_hits is not None\n",
    "    assert v_out is not None\n",
    "    assert es is not None\n",
    "    \n",
    "    geschichte = m.fit([o_qixs, o_sixs, o_hits], o_out, \n",
    "                       verbose=1,\n",
    "#                        batch_size=320, epochs=10000, \n",
    "                       batch_size=320, epochs=1000,\n",
    "#                        validation_split=0.1,\n",
    "                       validation_data=((v_qixs, v_sixs, v_hits), v_out), \n",
    "                       callbacks=[es], \n",
    "                       shuffle=True,\n",
    "                       class_weight=(class_weightz if balance_classes else None))\n",
    "    print(\"fertig\", cog_model, emb_w, loss, monitor)\n",
    "    return m, geschichte, config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.layer_utils import count_params\n",
    "from sklearn.metrics import log_loss\n",
    "def AIC(y, y_hat, n_params, n_obs=1):\n",
    "#     y_hat = model.predict(X)\n",
    "#     resid = y - y_hat\n",
    "#     sse = numpy.sum(numpy.power(resid,2)) / n_obs\n",
    "# y_true = np.array([0, 1, 1])\n",
    "# y_pred = np.array([0.1, 0.2, 0.9])\n",
    "\n",
    "#     print(y)\n",
    "#     print(y_hat)\n",
    "\n",
    "    ll = -log_loss(y, y_hat)\n",
    "    # 0.60671964791658428\n",
    "#     print(\"LL\", ll)\n",
    "    k   = n_params\n",
    "    aic = 2*k - 2* math.log(ll)\n",
    "    return aic\n",
    "\n",
    "def run_acc_mae_test(m, o_data, t_data, config_dict, print_clfn_report=False):\n",
    "    from sklearn.metrics import accuracy_score, mean_absolute_error, f1_score\n",
    "    print(config_dict[\"cog_model\"], \"£MB_W\", config_dict[\"emb_w\"])\n",
    "    t_sixs, t_qixs, t_hits, _ = t_data\n",
    "    o_sixs, o_qixs, o_hits, _ = o_data\n",
    "    p_hats = numpy.round( m.predict( [t_qixs, t_sixs, t_hits] ) )\n",
    "    p_trues = t_out\n",
    "    t_f1 = f1_score(p_trues, p_hats, average=\"macro\")\n",
    "    f1_micro = f1_score(p_trues, p_hats, average=\"micro\")\n",
    "    t_acc = accuracy_score(p_trues, p_hats)\n",
    "    t_mae = mean_absolute_error(p_trues, p_hats)\n",
    "    n_params = count_params(m.trainable_weights)\n",
    "    aic = 0 #AIC(p_trues, p_hats, n_params)\n",
    "    print(\"macro\", t_f1  )\n",
    "    print(\"micro\", f1_micro)\n",
    "    print( t_acc )\n",
    "    print( t_mae )\n",
    "    if print_clfn_report:\n",
    "        from sklearn.metrics import classification_report\n",
    "        print(classification_report(p_trues, p_hats))\n",
    "    return t_f1, t_acc, t_mae, aic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# def distance(a,b_list, cosine=False):\n",
    "# #     return numpy.sqrt(numpy.sum(numpy.power(a-b,2)))\n",
    "#     a = numpy.array(a).reshape(1,-1)\n",
    "#     if not cosine:\n",
    "#         #return scipy.spatial.distance.euclidean(a,b)\n",
    "#         return scipy.spatial.distance.cdist(a,b_list, metric=\"euclidean\")\n",
    "#     else:\n",
    "#         dasErgebnis = scipy.spatial.distance.cdist(a,b_list, metric=\"cosine\")\n",
    "# #         for x in reswlt.flatten():\n",
    "# #             if math.isnan(x):\n",
    "# #                 print(\"Nan is cosine distance result\")\n",
    "# #                 print(a)\n",
    "# #                 print(list(b_list))\n",
    "# #                 raise Exception(\"NAN in cos distance\")\n",
    "#         if numpy.isnan(numpy.sum(dasErgebnis)):\n",
    "#             dasErgebnis = numpy.nan_to_num(dasErgebnis, copy=False, nan=1.0, posinf=None, neginf=None)\n",
    "#         return dasErgebnis\n",
    "\n",
    "    \n",
    "def distance(a, b_list):\n",
    "    a = numpy.array(a).reshape(1,-1)\n",
    "    b_list = numpy.array(b_list).reshape(-1,a.shape[1]) # len(b_list) x width(a)\n",
    "    return scipy.spatial.distance.cdist(a,b_list, metric=\"euclidean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def build_adjacency_matrix(points_list, cosine=False):\n",
    "    dim = len(points_list) # num of points is the dim of the adj_mx\n",
    "#     print(\"dim is\", dim)\n",
    "    flat_dmx = [] # numpy.zeros((dim,dim)) # empty mx\n",
    "    \n",
    "    for a_ix in range(dim-1):\n",
    "        a = points_list[a_ix]\n",
    "#         print(a_ix, \"a\\n\",a)\n",
    "        to_comp = points_list[a_ix+1:]\n",
    "#         print(\"# to comp\")\n",
    "#         print(len(to_comp))\n",
    "#         print(to_comp)\n",
    "        ds = distance(a, to_comp).ravel()\n",
    "#         print(\"# little ds\", len(ds))\n",
    "#         print(ds)\n",
    "        flat_dmx.extend(list(ds))\n",
    "#     flat_dmx = list(numpy.array(flat_dmx).ravel())\n",
    "    \n",
    "    \n",
    "#     for ax,a in enumerate(points_list):\n",
    "#         for bx,b in enumerate(points_list):\n",
    "#             if ax<bx: #only fill in the top right triangle of the matrix\n",
    "#                 flat_dmx.append(distance(a,b, cosine=cosine))\n",
    "# #                 adj_mx[ax,bx] = distance(a,b)\n",
    "    print(\"flat dmx length is \", len(flat_dmx))\n",
    "    return flat_dmx\n",
    "\n",
    "\n",
    "def find_pairwise_rbo_in_adj_mx_list(adj_mx_list):\n",
    "    ranking_list = []\n",
    "    for amx in adj_mx_list:\n",
    "#         rankings = list(numpy.argsort(amx))\n",
    "        rankings = amx\n",
    "        ranking_list.append(rankings)\n",
    "    \n",
    "    rbos = []\n",
    "    seen = set()\n",
    "    for ix,r1 in enumerate(ranking_list):\n",
    "        for jx,r2 in enumerate(ranking_list):\n",
    "            if ix!=jx:\n",
    "                if ((ix,jx) not in seen) and ((jx,ix) not in seen):\n",
    "#                     print(\"r1\", r1)\n",
    "#                     print(\"r2\", r2)\n",
    "                    this_rbo = spearmanr(r1, r2)[0]\n",
    "#                     this_rbo = rbo_score(r1, r2, p=0.98)\n",
    "                    rbos.append(this_rbo)\n",
    "                    seen.add((ix,jx))\n",
    "                    seen.add((jx,ix))\n",
    "\n",
    "    print(\"correlations\", rbos)\n",
    "    mean_rbo = numpy.mean(rbos)\n",
    "#     median_rbo = numpy.median(rbos)\n",
    "    sd_rbo = numpy.std(rbos)\n",
    "    return mean_rbo, sd_rbo\n",
    "\n",
    "\n",
    "def find_pairwise_std_in_adj_mx_list(adj_mx_list):\n",
    "#     for a in adj_mx_list:\n",
    "#         plt.hist(a)\n",
    "    plt.show()\n",
    "    \n",
    "    mean_dist = numpy.mean(adj_mx_list)\n",
    "    \n",
    "    adj_mx_list = numpy.array(adj_mx_list)\n",
    "#     mean_distances = numpy.mean(adj_mx_list, axis=1).reshape(-1,1)\n",
    "#     min_distances = numpy.min(adj_mx_list, axis=1).reshape(-1,1)\n",
    "    n_adj_mx_list = adj_mx_list / mean_dist\n",
    "    vr = numpy.std(adj_mx_list, axis=0)\n",
    "    n_vr = numpy.std(n_adj_mx_list, axis=0) \n",
    "#     plt.hist(vr)\n",
    "#     plt.show()\n",
    "#     plt.hist(n_vr)\n",
    "#     plt.show()\n",
    "#     print(vr)\n",
    "#     print(n_vr)\n",
    "#     print(mean_distances)\n",
    "    vr   = numpy.mean(vr)\n",
    "    n_vr = numpy.mean(n_vr)\n",
    "    return vr, n_vr\n",
    "    \n",
    "flax = [\n",
    "    [1,2,3],\n",
    "    [1,2,3],\n",
    "    [1,3,10]\n",
    "]\n",
    "find_pairwise_std_in_adj_mx_list(flax)\n",
    "\n",
    "\n",
    "flax2 = [\n",
    "    [1,2,5],\n",
    "    [1,5,3],\n",
    "    [5,3,10]\n",
    "]\n",
    "find_pairwise_std_in_adj_mx_list(flax2)\n",
    "\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from random import sample\n",
    "from numpy.random import uniform\n",
    "import numpy as np\n",
    "from math import isnan\n",
    " \n",
    "# def hopkins(X):\n",
    "#     d = X.shape[1]\n",
    "#     #d = len(vars) # columns\n",
    "#     n = len(X) # rows\n",
    "#     m = int(0.1 * n) # heuristic from article [1]\n",
    "#     nbrs = NearestNeighbors(n_neighbors=1).fit(X)\n",
    " \n",
    "#     rand_X = sample(range(0, n, 1), m)\n",
    " \n",
    "#     ujd = []\n",
    "#     wjd = []\n",
    "#     for j in range(0, m):\n",
    "#         u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)\n",
    "#         ujd.append(u_dist[0][1])\n",
    "#         w_dist, _ = nbrs.kneighbors(X[rand_X[j]].reshape(1, -1), 2, return_distance=True)\n",
    "#         wjd.append(w_dist[0][1])\n",
    " \n",
    "#     H = sum(ujd) / (sum(ujd) + sum(wjd))\n",
    "#     if isnan(H):\n",
    "#         print (ujd, wjd)\n",
    "#         H = 0\n",
    " \n",
    "#     return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def compare_adj_matrices(adj_mcs, handle=None):\n",
    "    \n",
    "#     vr, n_vr = find_pairwise_std_in_adj_mx_list(adj_mcs)\n",
    "    vr, sd = find_pairwise_rbo_in_adj_mx_list(adj_mcs)\n",
    "    \n",
    "    max_val=0\n",
    "    for m in adj_mcs:\n",
    "        this_max = numpy.amax(m)\n",
    "        if this_max>max_val:\n",
    "            max_val = this_max\n",
    "    if max_val == 0:\n",
    "        raise Exception(\"Distance matrix is all zeros\")\n",
    "            \n",
    "    flattened_mxs = adj_mcs\n",
    "#     mean_dist = numpy.mean(adj_mcs)\n",
    "    \n",
    "    if handle:\n",
    "        print(\"For\", handle)\n",
    "    print(\"number of flattened adj mxs = \", len(flattened_mxs))\n",
    "    print(\"number of elements per flattened mx = \", len(flattened_mxs[0]))\n",
    "    print(\"distances min/max/mu/med:\", numpy.min(adj_mcs), numpy.max(adj_mcs), numpy.mean(adj_mcs), numpy.median(adj_mcs))\n",
    "#     print(\"stdev for technique\", numpy.std(adj_mcs))\n",
    "#     print(\"stdev/mu\", numpy.std(adj_mcs)/ mean_dist)\n",
    "    \n",
    "#     if normalise:\n",
    "    \n",
    "#     sc = MinMaxScaler()\n",
    "#     norm_flattened_mxs = sc.fit_transform(flattened_mxs)\n",
    "\n",
    "#     norm_flattened_mxs = []\n",
    "#     for mx in flattened_mxs:\n",
    "#         mxn = numpy.array(mx) / mean_dist\n",
    "#         norm_flattened_mxs.append(mxn)\n",
    "\n",
    "#     corr =0\n",
    "#     cnt  =0\n",
    "#     seen=set()\n",
    "#     for ix,src_mx in enumerate(flattened_mxs):\n",
    "#         for jx,des_mx in enumerate(flattened_mxs):\n",
    "#             print(\"ixjx\",ix,jx)\n",
    "#             if ix!=jx and (ix,jx) not in seen and (jx,ix) not in seen:\n",
    "#                 seen.add((ix,jx))\n",
    "#                 seen.add((jx,ix))\n",
    "#                 cnt  += 1\n",
    "#                 this_corr = spearmanr(src_mx, des_mx, axis=0)[0]\n",
    "#                 print(\"raw corr=\", this_corr)\n",
    "#                 corr += this_corr\n",
    "#             else:\n",
    "#                 print(\"seen\", ix,jx)\n",
    "\n",
    "#     print(\"corr=\",corr,\"cnt=\",cnt)\n",
    "#     corr = corr/cnt\n",
    "    \n",
    "#     n_corr =0\n",
    "#     cnt    =0\n",
    "#     seen=set()\n",
    "#     for ix,src_mx in enumerate(norm_flattened_mxs):\n",
    "#         for jx,des_mx in enumerate(norm_flattened_mxs):\n",
    "#             print(\"n ixjx\",ix,jx)\n",
    "#             if ix!=jx and (ix,jx) not in seen and (jx,ix) not in seen:\n",
    "#                 seen.add((ix,jx))\n",
    "#                 seen.add((jx,ix))\n",
    "#                 cnt  += 1\n",
    "#                 this_corr = spearmanr(src_mx, des_mx, axis=0)[0]\n",
    "#                 print(\"normed corr=\", this_corr)\n",
    "#                 n_corr += this_corr\n",
    "#             else:\n",
    "#                 print(\"seen\", ix,jx)\n",
    "#     print(\"n_corr=\",n_corr,\"ncnt=\",cnt)\n",
    "#     n_corr = n_corr/cnt\n",
    "    \n",
    "#     mu = numpy.mean(flattened_mxs, axis=0)\n",
    "#     s2 = numpy.var(flattened_mxs, axis=0)\n",
    "\n",
    "#     n_mu = numpy.mean(norm_flattened_mxs, axis=0)\n",
    "#     n_s2 = numpy.var(norm_flattened_mxs, axis=0)\n",
    "    \n",
    "#     print(\"Correlation=\", corr)\n",
    "#     print(\"Norm Corr/n=\", n_corr)\n",
    "    print(\"Average var       =\", vr, \"({})\".format(sd))\n",
    "#     print(\"Norm Av var       =\", n_vr)\n",
    "#             print(\"Norm/d corr/n Med =\", correltn_md)\n",
    "#             print(\"Norm/d corr/n Mean=\", correltn_mn)\n",
    "    return vr, sd, 0, 0 #TODO deprecate these zeroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flat dmx length is  6\n",
      "flat dmx length is  6\n",
      "ad1 [2.8284271247461903, 2.23606797749979, 2.23606797749979, 5.0, 5.0, 0.0]\n",
      "ad2 [2.8284271247461903, 2.23606797749979, 2.23606797749979, 5.0, 5.0, 0.0]\n",
      "correlations [0.9999999999999999]\n",
      "number of flattened adj mxs =  2\n",
      "number of elements per flattened mx =  6\n",
      "distances min/max/mu/med: 0.0 5.0 2.8834271799576285 2.53224755112299\n",
      "Average var       = 0.9999999999999999 (0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9999999999999999, 0.0, 0, 0)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mx1 = [ [3, 3], [3, 5], [3,3], [5,8] ]\n",
    "mx1 = [ [2, 3], [4, 5], [1,1], [1,1] ]\n",
    "mx2 = [ [2, 3], [4, 5], [1,1], [1,1] ]\n",
    "\n",
    "# mx1 = [[4],[3],[2],[1],[0]]\n",
    "# mx2 = [[0],[1],[1],[1],[4]]\n",
    "\n",
    "ad1 = build_adjacency_matrix(mx1)\n",
    "ad2 = build_adjacency_matrix(mx2)\n",
    "\n",
    "# ad1 = [[0],[2],[1]]\n",
    "# ad2 = [[2],[1],[0]]\n",
    "      \n",
    "print(\"ad1\", ad1)\n",
    "print(\"ad2\", ad2)\n",
    "\n",
    "compare_adj_matrices( [ad1, ad2] )\n",
    "\n",
    "# list1 = ['a','b','c','d','e']\n",
    "# list2 = ['b','a','c','d','e']\n",
    "# list3 = ['a','b','c','e','d']\n",
    "    \n",
    "# score1 = rbo_score(list1, list2)\n",
    "# print(score1)\n",
    "# # assert score1 == 0.8\n",
    "# score2 = rbo_score(list1, list3)\n",
    "# print(score2)\n",
    "# # assert score2 == 0.95\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig\n",
      "[[ 2 45]\n",
      " [30 62]]\n",
      "samez:\n",
      " SpearmanrResult(correlation=array([[nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan]]), pvalue=array([[nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan]]))\n",
      "(4, 4)\n",
      "randz:\n",
      " SpearmanrResult(correlation=array([[ 1.        ,  0.00834949, -0.05825257,  0.03508272],\n",
      "       [ 0.00834949,  1.        , -0.02431038, -0.01282159],\n",
      "       [-0.05825257, -0.02431038,  1.        ,  0.13143306],\n",
      "       [ 0.03508272, -0.01282159,  0.13143306,  1.        ]]), pvalue=array([[0.        , 0.93429159, 0.56482397, 0.72894962],\n",
      "       [0.93429159, 0.        , 0.81026602, 0.89924979],\n",
      "       [0.56482397, 0.81026602, 0.        , 0.19241477],\n",
      "       [0.72894962, 0.89924979, 0.19241477, 0.        ]]))\n",
      "(4, 4)\n",
      "scalz:\n",
      " SpearmanrResult(correlation=array([[1., 1., 1., 1.],\n",
      "       [1., 1., 1., 1.],\n",
      "       [1., 1., 1., 1.],\n",
      "       [1., 1., 1., 1.]]), pvalue=array([[0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjm49/.venvs/isaac/lib/python3.6/site-packages/numpy/lib/function_base.py:2559: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(666)\n",
    "n_to_comp = 100\n",
    "mx_side = 2\n",
    "orig  = numpy.random.randint(0,100, size=(mx_side, mx_side))\n",
    "samez = [ orig ] * n_to_comp\n",
    "randz = [ orig ] + [ numpy.random.randint(0,100, size=(mx_side, mx_side)) for _ in range(n_to_comp-1) ]\n",
    "scalz = [ orig*(i+1) for i in range(n_to_comp) ]\n",
    "\n",
    "print(\"orig\")\n",
    "print(orig)\n",
    "\n",
    "# print(\"random\")\n",
    "# print(randz)\n",
    "\n",
    "# print(\"scaled\")\n",
    "# print(scalz)\n",
    "\n",
    "samez_flat = [numpy.ravel(s) for s in samez]\n",
    "# print(samez_flat)\n",
    "sp_r = spearmanr( samez_flat, axis=0 )\n",
    "print(\"samez:\\n\",sp_r)\n",
    "print(sp_r[0].shape)\n",
    "\n",
    "randz_flat = [numpy.ravel(r) for r in randz]\n",
    "# print(randz_flat)\n",
    "sp_r = spearmanr(randz_flat, axis=0)\n",
    "print(\"randz:\\n\", sp_r)\n",
    "print(sp_r[0].shape)\n",
    "\n",
    "scalz_flat = [numpy.ravel(c) for c in scalz]\n",
    "# print(scalz_flat)\n",
    "sp_r = spearmanr( scalz_flat, axis=0)\n",
    "print(\"scalz:\\n\",sp_r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_history_results(history_results, o_data, t_data):\n",
    "    import copy \n",
    "    rankin = []\n",
    "    max_acc = -math.inf\n",
    "    \n",
    "    for item in history_results:\n",
    "#     config_dict, m, h = item\n",
    "        config_dict, m, h = item\n",
    "        config_dict = copy.copy(config_dict)\n",
    "        cog_model = config_dict[\"cog_model\"]\n",
    "        emb_w = config_dict[\"emb_w\"]\n",
    "        try:\n",
    "            q_weight = config_dict[\"q_weight\"]\n",
    "            config_dict[\"cog_model\"] = config_dict[\"cog_model\"] + \" \" + str(emb_w) + \" \" + str(q_weight)\n",
    "        except:\n",
    "            pass\n",
    "#             raise Exception(\"No such key: q_weight\")\n",
    "\n",
    "        v_loss = h.history[\"val_loss\"]\n",
    "        v_acc  = h.history[\"val_accuracy\"]\n",
    "        v_mse   = h.history[\"val_mean_absolute_error\"]\n",
    "        o_loss   = h.history[\"loss\"]\n",
    "        o_acc    = h.history[\"accuracy\"]\n",
    "        o_mse    = h.history[\"mean_absolute_error\"]\n",
    "        plot_acc = v_acc#[0:-10]\n",
    "        plot_loss = v_loss#[0:-10]\n",
    "        plot_mse = v_mse#[0:-10]\n",
    "\n",
    "        t_acc, t_mae = run_acc_mae_test(m, o_data, t_data, config_dict)\n",
    "\n",
    "        mod_name = cog_model +\" \"+ str(emb_w)\n",
    "        try:\n",
    "            mod_name = mod_name + \" \"+ str(q_weight)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        tup = (t_mae, t_acc, min(v_mse), max(v_acc), mod_name)\n",
    "        rankin.append( tup ) \n",
    "        if max_acc < max(v_acc):\n",
    "            max_acc = max(v_acc)\n",
    "            max_mod = cog_model +\" \"+ str(emb_w)\n",
    "            min_mse = min(v_mse)\n",
    "            min_loss = min(v_loss)\n",
    "            best_m = m\n",
    "        \n",
    "    print(\"**\", max_mod, max_acc, min_loss, min_mse)\n",
    "    m = best_m\n",
    "    \n",
    "    rankin = sorted(rankin)\n",
    "    for r in rankin:\n",
    "               print(r[-1],\"&\", numpy.round(r[1],2),\"&\", numpy.round(r[0],2),\"\\\\\\\\\")\n",
    "#         print(r[-1],\"&\", numpy.round(r[1],2),\"({})\".format(numpy.round(r[3],2)),\"&\", numpy.round(r[0],2), \"({})\".format(numpy.round(r[2],2)),\"\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####\n",
    "# ## NEXT FEW CELLS: NeurIPS Compo 2020 dataset\n",
    "# #####\n",
    "\n",
    "# test_df = pandas.DataFrame.from_csv(\"./starter_kit/submission_templates/submission_task_1_2.csv\", index_col=\"QuestionId\")\n",
    "# sids = test_df[\"UserId\"]\n",
    "# qids = test_df.index\n",
    "\n",
    "# test_sids = set(sids)\n",
    "# test_qids = set(qids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_sixs = numpy.array([uniq_sids.index(s) for s in sids]).reshape(-1,1)\n",
    "# t_qixs = numpy.array([uniq_qids.index(q) for q in qids]).reshape(-1,1)\n",
    "\n",
    "# t_hits = []\n",
    "# for six,qix in zip(t_sixs,t_qixs):\n",
    "#     print(six,qix)\n",
    "#     zs = last_h[six]\n",
    "#     chits.append(zs)\n",
    "#     zs[qix] += 1\n",
    "#     print(sum(zs))\n",
    "# t_hits = numpy.array(t_hits)\n",
    "# t_hits = pca.transform(t_hits)\n",
    "\n",
    "# fnm = home+\"/models/\" + handle.replace(\"/\",\"~\") + \"_\" + str(rep)\n",
    "# m = keras.models.load_model(fnm, custom_objects={'WeightClip': WeightClip}, compile=False)\n",
    "# results = m.predict([t_qixs, t_sixs, t_hits])\n",
    "\n",
    "# print(\"----\")\n",
    "# for r in results:\n",
    "#     print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_df = pandas.DataFrame.from_csv(\"./starter_kit/data/train_data/train_task_1_2.csv\", index_col=\"QuestionId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_sids)\n",
    "# print(raw_df[\"UserId\"].isin(test_sids))\n",
    "# print(raw_df.index.isin(test_qids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_relevant = raw_df.loc[ (raw_df[\"UserId\"] in test_sids & raw_df.index in test_qids) ]\n",
    "# print(len(test_relevant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_sids = sids\n",
    "# test_qids = qids\n",
    "\n",
    "# indices = numpy.random.choice(len(raw_df), size=50000, replace=False)\n",
    "\n",
    "\n",
    "# sids = raw_df[\"UserId\"]#.iloc[indices]\n",
    "# qids = raw_df.index#[indices]\n",
    "# hout = raw_df[\"IsCorrect\"]#.iloc[indices]\n",
    "# print(len(sids))\n",
    "# del raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniq_qids = list(numpy.unique(qids))\n",
    "# print(len(uniq_qids))\n",
    "# qixlookup = {}\n",
    "# for ix,uq in enumerate(uniq_qids):\n",
    "#     qixlookup[uq] = ix\n",
    "    \n",
    "# chits = []\n",
    "# last_h = {}\n",
    "\n",
    "# # six_lookup = {}\n",
    "# uniq_sids = list(numpy.unique(sids))\n",
    "# sixlookup={}\n",
    "# for ix,us in enumerate(uniq_sids):\n",
    "#     sixlookup[us] = ix\n",
    "\n",
    "# print(\"built lookups\")\n",
    "    \n",
    "# for sid in uniq_sids:\n",
    "#     six = sixlookup[sid]\n",
    "#     last_h[six] = [0]*len(uniq_qids)\n",
    "\n",
    "# print(\"init'd last_h lookup\")\n",
    "    \n",
    "# v_indices = numpy.random.choice(len(sids), size=1000, replace=False)\n",
    "\n",
    "# sixs = numpy.array([sixlookup[s] for s in sids])\n",
    "# print(\"sixs remap done\")\n",
    "# qixs = numpy.array([qixlookup[q] for q in qids])\n",
    "# print(\"qixs remap done\")\n",
    "# hout = numpy.array(hout)\n",
    "# print(\"EOND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heu = generate_heu_autoencoder(len(uniq_qids), 100)\n",
    "# es = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "        \n",
    "# for six,qix in zip(sixs,qixs):\n",
    "#     print(six,qix)\n",
    "#     zs = last_h[six]\n",
    "#     chits.append(zs)\n",
    "#     zs[qix] += 1\n",
    "#     print(sum(zs))\n",
    "    \n",
    "#     if len(chits > 1000):\n",
    "#         heu.fit(o_hits, o_hits, callbacks=[es], validation_split=0.05, epochs=10000, batch_size=len(chits), shuffle=True)\n",
    "#         chits = []\n",
    "# heu.fit(o_hits, o_hits, callbacks=[es], validation_split=0.05, epochs=10000, batch_size=len(chits), shuffle=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./real_data/examliftb1_LFA_strat_1000000\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(658050, 1130) None\n",
      "(1254, 1130)\n",
      "(1258, 1130)\n"
     ]
    }
   ],
   "source": [
    "seen = None\n",
    "max_s = 1000000\n",
    "# if dataset_handle == \"cmu_geom_steplevel\":\n",
    "#     (sixs, qixs, hout) = pickle.load(open(home+\"/real_data/cmu_geom_steplevel.p\", \"rb\"))\n",
    "#     min_hist = 0\n",
    "# elif dataset_handle == \"examliftb1\":\n",
    "#     (sixs, qixs, hout) = pickle.load(open(home+\"/real_data/XL1041.p\", \"rb\"))\n",
    "#     min_hist = 40\n",
    "# else:\n",
    "#     raise Exception(\"Invalid dataset handle \" + str(dataset_handle))\n",
    "\n",
    "# dataset_handle = \"isaac\" \n",
    "dataset_handle = \"examliftb1\" \n",
    "# dataset_handle = \"cmu_geom_steplevel\" \n",
    "\n",
    "# dataset_name = \"examliftb1\"+str(max_s)\n",
    "dataset_name = dataset_handle+\"_LFA_strat_\"+str(max_s)#+\"_stepped\"\n",
    "fnm = home+\"/real_data/\" + dataset_name\n",
    "# fnm = home+\"/real_data/\" + dataset_name\n",
    "print(fnm)\n",
    "with open(fnm, 'rb') as f:\n",
    "     data_bundle = pickle.load(f)\n",
    "        \n",
    "# ./real_data/examliftb1100000\n",
    "\n",
    "odata, vdata, tdata, sid_six_lookup, qid_qix_lookup = data_bundle\n",
    "(o_sixs, o_qixs, o_hits, o_out), (v_sixs, v_qixs, v_hits, v_out), (t_sixs, t_qixs, t_hits, t_out) = (odata, vdata, tdata)\n",
    "print(o_hits.shape, print(type(o_hits)))\n",
    "print(v_hits.shape)\n",
    "print(t_hits.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "./real_data/isaac_LFA_strat_110000_stepped\n",
    "<class 'scipy.sparse.csr.csr_matrix'>\n",
    "(110028, 6294) None\n",
    "(110, 6294)\n",
    "(111, 6294)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2057/2057 [==============================] - 10s 4ms/step - loss: 0.4563 - binary_crossentropy: 0.4563 - binary_accuracy: 0.7848 - mean_absolute_error: 0.2962 - mean_squared_error: 0.1484 - f1_loss: 0.3752 - val_loss: 0.5323 - val_binary_crossentropy: 0.5323 - val_binary_accuracy: 0.7352 - val_mean_absolute_error: 0.3329 - val_mean_squared_error: 0.1751 - val_f1_loss: 0.3456\n",
      "Epoch 73/1000\n",
      "2057/2057 [==============================] - 10s 4ms/step - loss: 0.4564 - binary_crossentropy: 0.4564 - binary_accuracy: 0.7847 - mean_absolute_error: 0.2961 - mean_squared_error: 0.1484 - f1_loss: 0.3743 - val_loss: 0.5399 - val_binary_crossentropy: 0.5399 - val_binary_accuracy: 0.7329 - val_mean_absolute_error: 0.3327 - val_mean_squared_error: 0.1771 - val_f1_loss: 0.3463\n",
      "Epoch 74/1000\n",
      "2057/2057 [==============================] - 10s 4ms/step - loss: 0.4565 - binary_crossentropy: 0.4565 - binary_accuracy: 0.7847 - mean_absolute_error: 0.2963 - mean_squared_error: 0.1485 - f1_loss: 0.3748 - val_loss: 0.5343 - val_binary_crossentropy: 0.5343 - val_binary_accuracy: 0.7329 - val_mean_absolute_error: 0.3329 - val_mean_squared_error: 0.1759 - val_f1_loss: 0.3459\n",
      "Epoch 75/1000\n",
      "2057/2057 [==============================] - 10s 4ms/step - loss: 0.4562 - binary_crossentropy: 0.4562 - binary_accuracy: 0.7857 - mean_absolute_error: 0.2961 - mean_squared_error: 0.1482 - f1_loss: 0.3747 - val_loss: 0.5323 - val_binary_crossentropy: 0.5323 - val_binary_accuracy: 0.7376 - val_mean_absolute_error: 0.3329 - val_mean_squared_error: 0.1751 - val_f1_loss: 0.3456\n",
      "Epoch 76/1000\n",
      "2057/2057 [==============================] - 10s 4ms/step - loss: 0.4576 - binary_crossentropy: 0.4576 - binary_accuracy: 0.7849 - mean_absolute_error: 0.2968 - mean_squared_error: 0.1488 - f1_loss: 0.3747 - val_loss: 0.5420 - val_binary_crossentropy: 0.5420 - val_binary_accuracy: 0.7337 - val_mean_absolute_error: 0.3322 - val_mean_squared_error: 0.1780 - val_f1_loss: 0.3463\n",
      "Epoch 77/1000\n",
      "2057/2057 [==============================] - 11s 4ms/step - loss: 0.4566 - binary_crossentropy: 0.4566 - binary_accuracy: 0.7856 - mean_absolute_error: 0.2959 - mean_squared_error: 0.1484 - f1_loss: 0.3752 - val_loss: 0.5376 - val_binary_crossentropy: 0.5376 - val_binary_accuracy: 0.7321 - val_mean_absolute_error: 0.3329 - val_mean_squared_error: 0.1769 - val_f1_loss: 0.3464\n",
      "Epoch 78/1000\n",
      "2057/2057 [==============================] - 11s 5ms/step - loss: 0.4548 - binary_crossentropy: 0.4548 - binary_accuracy: 0.7858 - mean_absolute_error: 0.2951 - mean_squared_error: 0.1478 - f1_loss: 0.3737 - val_loss: 0.5342 - val_binary_crossentropy: 0.5342 - val_binary_accuracy: 0.7344 - val_mean_absolute_error: 0.3323 - val_mean_squared_error: 0.1758 - val_f1_loss: 0.3455\n",
      "Epoch 79/1000\n",
      "2057/2057 [==============================] - 11s 4ms/step - loss: 0.4570 - binary_crossentropy: 0.4570 - binary_accuracy: 0.7846 - mean_absolute_error: 0.2966 - mean_squared_error: 0.1486 - f1_loss: 0.3744 - val_loss: 0.5384 - val_binary_crossentropy: 0.5384 - val_binary_accuracy: 0.7321 - val_mean_absolute_error: 0.3336 - val_mean_squared_error: 0.1767 - val_f1_loss: 0.3467\n",
      "Epoch 80/1000\n",
      "2057/2057 [==============================] - 10s 4ms/step - loss: 0.4561 - binary_crossentropy: 0.4561 - binary_accuracy: 0.7845 - mean_absolute_error: 0.2960 - mean_squared_error: 0.1483 - f1_loss: 0.3743 - val_loss: 0.5346 - val_binary_crossentropy: 0.5346 - val_binary_accuracy: 0.7321 - val_mean_absolute_error: 0.3334 - val_mean_squared_error: 0.1756 - val_f1_loss: 0.3461\n",
      "Epoch 81/1000\n",
      "2057/2057 [==============================] - 10s 4ms/step - loss: 0.4557 - binary_crossentropy: 0.4557 - binary_accuracy: 0.7854 - mean_absolute_error: 0.2958 - mean_squared_error: 0.1482 - f1_loss: 0.3738 - val_loss: 0.5372 - val_binary_crossentropy: 0.5372 - val_binary_accuracy: 0.7321 - val_mean_absolute_error: 0.3327 - val_mean_squared_error: 0.1767 - val_f1_loss: 0.3461\n",
      "Epoch 82/1000\n",
      "2057/2057 [==============================] - 10s 4ms/step - loss: 0.4578 - binary_crossentropy: 0.4578 - binary_accuracy: 0.7839 - mean_absolute_error: 0.2970 - mean_squared_error: 0.1490 - f1_loss: 0.3749 - val_loss: 0.5308 - val_binary_crossentropy: 0.5308 - val_binary_accuracy: 0.7337 - val_mean_absolute_error: 0.3345 - val_mean_squared_error: 0.1749 - val_f1_loss: 0.3464\n",
      "Epoch 83/1000\n",
      "2057/2057 [==============================] - 10s 4ms/step - loss: 0.4564 - binary_crossentropy: 0.4564 - binary_accuracy: 0.7850 - mean_absolute_error: 0.2961 - mean_squared_error: 0.1484 - f1_loss: 0.3747 - val_loss: 0.5343 - val_binary_crossentropy: 0.5343 - val_binary_accuracy: 0.7360 - val_mean_absolute_error: 0.3324 - val_mean_squared_error: 0.1759 - val_f1_loss: 0.3455\n",
      "Epoch 84/1000\n",
      "2057/2057 [==============================] - 10s 4ms/step - loss: 0.4569 - binary_crossentropy: 0.4569 - binary_accuracy: 0.7843 - mean_absolute_error: 0.2964 - mean_squared_error: 0.1487 - f1_loss: 0.3749 - val_loss: 0.5359 - val_binary_crossentropy: 0.5359 - val_binary_accuracy: 0.7321 - val_mean_absolute_error: 0.3324 - val_mean_squared_error: 0.1764 - val_f1_loss: 0.3457\n",
      "Epoch 85/1000\n",
      "2057/2057 [==============================] - 10s 4ms/step - loss: 0.4572 - binary_crossentropy: 0.4572 - binary_accuracy: 0.7838 - mean_absolute_error: 0.2967 - mean_squared_error: 0.1488 - f1_loss: 0.3746 - val_loss: 0.5371 - val_binary_crossentropy: 0.5371 - val_binary_accuracy: 0.7313 - val_mean_absolute_error: 0.3330 - val_mean_squared_error: 0.1764 - val_f1_loss: 0.3462\n",
      "fertig RASCH 1 binary_crossentropy val_f1_loss\n",
      "None :F1s v/t  0.6890247100549634 0.7290099854483416 0.7234159211326326\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1d691766d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RASCH1//(xe/px)#1000000 None None \t\t 0.719843520768993\n",
      "RASCH1//(xe/px)#1000000 None None \t\t 0.7265670193637008\n",
      "RASCH1//(xe/px)#1000000 None None \t\t 0.7234159211326326\n",
      "GO FOR RASCH1//(xe/px)#1000000\n",
      "checking for cached file ./lfa_models/RASCH1~~(xe~px)#1000000_3\n",
      "class weights: [2.67326092 1.        ]\n",
      "class weights (dict): {0: 2.6732609156777154, 1: 1.0}\n",
      "nq, ns\n",
      "1130 2512\n",
      "Using univariate Rasch model!\n",
      "TRAINING:\n",
      "Unique students: 2512\n",
      "Unique questions: 1130\n",
      "Total activity: 658050 ( 478904.0 )\n",
      "ov shape (658050, 1130) (1254, 1130)\n",
      "monitoring info val_f1_loss min\n",
      "Model: \"model_19\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "psi_select (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hit_counter (InputLayer)        [(None, 1130)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gammas (Embedding)              (None, 1, 1)         2512        psi_select[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "alphas (Embedding)              (None, 1, 1)         2512        psi_select[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "qk_loadings (Dense)             (None, 1)            1130        hit_counter[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_58 (Flatten)            (None, 1)            0           gammas[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "q_select (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_59 (Flatten)            (None, 1)            0           alphas[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_19 (Multiply)          (None, 1)            0           qk_loadings[0][0]                \n",
      "                                                                 flatten_58[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "qn_embedding (Embedding)        (None, 1, 1)         1130        q_select[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 1)            0           flatten_59[0][0]                 \n",
      "                                                                 multiply_19[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_57 (Flatten)            (None, 1)            0           qn_embedding[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "subtract_19 (Subtract)          (None, 1)            0           add_19[0][0]                     \n",
      "                                                                 flatten_57[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)              (None, 1)            0           subtract_19[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 7,284\n",
      "Trainable params: 7,284\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "fitting\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-902229cf0878>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m \u001b[0mhistory_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_bundle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_through_cog_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_bundle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_bundle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mo_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msid_six_lookup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqid_qix_lookup\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_bundle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-128-902229cf0878>\u001b[0m in \u001b[0;36mstep_through_cog_models\u001b[0;34m(data_bundle, cog_models)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;31m#                                     continue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                                 \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_and_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0modata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcog_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcog_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_w\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbalance_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlozz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_w\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;31m#                                 m.summary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;31m#                                 t_sixs, t_qixs, t_hits, _ = t_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-127-127c91cf7f23>\u001b[0m in \u001b[0;36mgen_and_train\u001b[0;34m(odata, vdata, t_data, emb_w, draw, cog_model, q_weight, monitor, balance_classes, loss, metrics, reg_w)\u001b[0m\n\u001b[1;32m    270\u001b[0m                        \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                        \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m                        class_weight=(class_weightz if balance_classes else None))\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fertig\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcog_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeschichte\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/isaac/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/isaac/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/isaac/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_x_y_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/isaac/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \"\"\"\n\u001b[0;32m--> 691\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/isaac/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   3155\u001b[0m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3156\u001b[0m     \u001b[0mbatched_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3157\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_batched_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3158\u001b[0m     self._structure = nest.map_structure(\n\u001b[1;32m   3159\u001b[0m         lambda component_spec: component_spec._unbatch(), batched_spec)  # pylint: disable=protected-access\n",
      "\u001b[0;32m~/.venvs/isaac/lib/python3.6/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mto_batched_tensor_list\u001b[0;34m(element_spec, element)\u001b[0m\n\u001b[1;32m    364\u001b[0m   return _to_tensor_list_helper(\n\u001b[1;32m    365\u001b[0m       lambda state, spec, component: state + spec._to_batched_tensor_list(\n\u001b[0;32m--> 366\u001b[0;31m           component), element_spec, element)\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/isaac/lib/python3.6/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36m_to_tensor_list_helper\u001b[0;34m(encode_fn, element_spec, element)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m   return functools.reduce(\n\u001b[0;32m--> 340\u001b[0;31m       reduce_fn, zip(nest.flatten(element_spec), nest.flatten(element)), [])\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/isaac/lib/python3.6/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mreduce_fn\u001b[0;34m(state, value)\u001b[0m\n\u001b[1;32m    335\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mencode_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m   return functools.reduce(\n",
      "\u001b[0;32m~/.venvs/isaac/lib/python3.6/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(state, spec, component)\u001b[0m\n\u001b[1;32m    364\u001b[0m   return _to_tensor_list_helper(\n\u001b[1;32m    365\u001b[0m       lambda state, spec, component: state + spec._to_batched_tensor_list(\n\u001b[0;32m--> 366\u001b[0;31m           component), element_spec, element)\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/isaac/lib/python3.6/site-packages/tensorflow/python/framework/sparse_tensor.py\u001b[0m in \u001b[0;36m_to_batched_tensor_list\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    369\u001b[0m     return [gen_sparse_ops.serialize_many_sparse(\n\u001b[1;32m    370\u001b[0m         \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m         out_type=dtypes.variant)]\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/isaac/lib/python3.6/site-packages/tensorflow/python/ops/gen_sparse_ops.py\u001b[0m in \u001b[0;36mserialize_many_sparse\u001b[0;34m(sparse_indices, sparse_values, sparse_shape, out_type, name)\u001b[0m\n\u001b[1;32m    491\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m    492\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SerializeManySparse\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         sparse_shape, \"out_type\", out_type)\n\u001b[0m\u001b[1;32m    494\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#RESET\n",
    "model_lookup = {} #defaultdict(list)\n",
    "max_acc = 0\n",
    "handles = []\n",
    "\n",
    "from keras.metrics import binary_accuracy, binary_crossentropy, mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import f1_score\n",
    "import os\n",
    "mon_lookup = {\n",
    "    \"binary_crossentropy\" : \"xe\",\n",
    "    \"mean_squared_error\"  : \"mse\",\n",
    "    \"mean_absolute_error\" : \"mae\",\n",
    "    \"f1_loss\" : \"f1\",\n",
    "    \"binary_accuracy\" : \"acc\",\n",
    "    \"log_likelihood\" : \"ll\",\n",
    "    \"loss\" : \"px\"\n",
    "}\n",
    "\n",
    "# max_s = 250000\n",
    "print(\"max_s\", max_s)\n",
    "\n",
    "\n",
    "def step_through_cog_models(data_bundle=None, cog_models=None):\n",
    "# def step_through_cog_models(sixs, qixs, hout, cog_models=None, data_bundle=None):\n",
    "#     cog_models = [\"MLP\", \"MLP+\",\"MLTM\",\"LFA\",\"SLFA\"]\n",
    "#     cog_models = [\"MLP\", \"MLP+\",\"MLTM\", \"MLTM+\", \"MLTM_no_init\"]\n",
    "#     cog_models = [\"MLP\", \"MLP+\",\"MLTM\", \"MLTM+\", \"LFA\",\"SLFA\"]\n",
    "    #(\"MLTM+\",32,10,\"val_f1_m\",False)\n",
    "    metrics = [binary_crossentropy, binary_accuracy, mean_absolute_error, mean_squared_error, f1_loss]\n",
    "\n",
    "    if cog_models is None:\n",
    "        core_models = [\"CFM\", \"RASCH\", \"AFM\", \"MLTM\"]\n",
    "        variant_models = [\"AFMg\", \"MLTMb\", \"MLTM0\"]\n",
    "#         ffnn_models = [\"MLPs\", \"MLPsz\"] #\"MLP\", \"MLPd\"]#, \"CONC\"]\n",
    "        ffnn_models = [\"MLPs\", \"MLP\", \"MLPd\"]#, \"CONC\"]\n",
    "        regd_models = [s+\"z\" for s in [\"MLTM\",\"MLP\",\"MLPd\",\"AFM\",\"AFMg\",\"CFM\",\"MLTMb\",\"MLTM0\"]]\n",
    "#         cog_models = variant_models\n",
    "        cog_models = regd_models\n",
    "#         cog_models = [\"AFM\", \"AFMg\", \"AFMx\", \"MLTM\", \"MLTMa\",\"MLTMb\", \"CFM\", \"MLP\", \"MLPd\"]\n",
    "#         cog_models = [\"MLTMz\"] #\"MLTMb\", \"MLP\", \"CFM\", \"RASCH\", \"AFM\", \"AFMg\", \"AFMx\", \"MLTM\", \"MLTMa\",\"MLTMb\",\"MLTM0\", \"MLP\", \"MLPd\"]\n",
    "#         cog_models = [\"AFMg\"]\n",
    "#         cog_models = [\"MLTMz\"]#,\"AFMg\"]\n",
    "        cog_models = [\"RASCH\"]\n",
    "#         cog_models = [\"MLTMb\",\"MLTMbz\",\"MLTM0\",\"MLTM0z\"]\n",
    "#         cog_models = [\"MLPrawDen\", \"MLPrawDP\"]#, \"MLPrawAD\", \"MLPrawADD\"]\n",
    "#         cog_models = ffnn_models\n",
    "#         emb_ws = [400,300,256,128,64,32,16, 8]\n",
    "        emb_ws = [ 64, ]#, 32, 64]\n",
    "#         emb_ws = [24, 32]\n",
    "#         emb_ws = [32, 64, 128]\n",
    "#         emb_ws = [96]# 32, 64, 96]\n",
    "#         emb_ws = [8,16,32, 64,128]#, 256, 300, 400]\n",
    "#         emb_ws = [32,64,128,256,300,400,500]\n",
    "\n",
    "#         emb_ws = [8,]#, 16, 32, 64, 128, 256, 300, 400,]\n",
    "\n",
    "#         q_ws = [None]\n",
    "#         losses = [\"binary_crossentropy\",\"mean_squared_error\",\"f1_loss\"]#, \"binary_crossentropy\"]\n",
    "#         losses = [(\"binary_crossentropy\", \"val_loss\")]\n",
    "        losses = [\n",
    "                    (\"binary_crossentropy\", \"val_loss\"), \n",
    "#                   (\"binary_crossentropy\", \"val_f1_loss\"), \n",
    "#                   (\"mean_squared_error\", \"val_loss\"),\n",
    "#                   (\"f1_loss\", \"val_f1_loss\")\n",
    "                 ]\n",
    "#         losses = [(\"f1_loss\", \"val_f1_loss\")]\n",
    "        q_ws = [None]\n",
    "\n",
    "#         reg_ws = numpy.random.normal(loc=1e-5, scale=1e-5, size=20) # [1e-8, 1e-7, 1e-6, 0.00001]#, 0.0001, 0.001]\n",
    "        reg_ws = [None]\n",
    "#         reg_ws = [None, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "#         monitor_settings = [\"val_mean_absolute_error\", \"val_mean_squared_error\"]\n",
    "#         monitor_settings = [\"val_loss\", \"val_f1_m\", \"val_mean_squared_error\"]\n",
    "        balance_settings = [ False, ]\n",
    "#         q_ws = [None, 1, 5, 10, 50, 100, 500]\n",
    "#         q_ws = [None,1,10,100,1000]\n",
    "#     emb_ws = [14, ]# [1,2,4,6,8,10,12,14,16]\n",
    "#     emb_ws = [1,2,4,6,8,10,12,14,16]\n",
    "    n_reps = 5\n",
    "\n",
    "    payload = []\n",
    "        \n",
    "\n",
    "    seen=[]\n",
    "    \n",
    "    \n",
    "    odata, vdata, tdata, sid_six_lookup, qid_qix_lookup = data_bundle\n",
    "    (o_sixs, o_qixs, o_hits, o_out), (v_sixs, v_qixs, v_hits, v_out), (t_sixs, t_qixs, t_hits, t_out) = (odata, vdata, tdata)\n",
    "    o_hits = o_hits.astype(\"int8\")\n",
    "    \n",
    "    print(len(v_out), sum(v_out))\n",
    "    print(len(t_out), sum(t_out))\n",
    "#     raise Exception(\"le what\")\n",
    "    \n",
    "    f1s = []\n",
    "    overwrite_disc=True\n",
    "    for rep in range(n_reps):\n",
    "        for cog_model in cog_models:# zip(cog_models, q_ws):\n",
    "            for w in emb_ws:\n",
    "                if cog_model==\"RASCH\":\n",
    "                    w=1\n",
    "                for qw in q_ws:\n",
    "                    for rw in reg_ws:\n",
    "                        w0 = rw\n",
    "                        for bal in balance_settings:\n",
    "                            for lozz,mon in losses:\n",
    "                                best_f1_regw = (0,0)\n",
    "#                                 rw = None #rw / (w*n_students)\n",
    "                                #w1 = rw if (rw is not None) else None\n",
    "#                                 w1 = 5.5e-8\n",
    "                                w1 = rw\n",
    "    \n",
    "#                                 if max_w = None:\n",
    "#                                     max_w = (rw, 0)                \n",
    "#                                 w1s = numpy.random.normal(max_w[0], scale=w1, size=10)\n",
    "                    \n",
    "                                qcode = \"\" if (qw is None) else \"q\"+str(qw)\n",
    "                                moncode = mon_lookup[mon[mon.index(\"_\")+1:]]\n",
    "                                losscode = mon_lookup[lozz]\n",
    "                                balstr = \"bal\" if bal else \"\"\n",
    "                                \n",
    "                                handle = \"{}{}/{}/{}({}/{})\".format(cog_model, w, qcode, balstr, losscode, moncode)\n",
    "                                memkey = handle+str(w1)\n",
    "                                if max_s != 100000:\n",
    "                                    handle += \"#\"+str(max_s)\n",
    "                                print(\"GO FOR\", handle)\n",
    "                                \n",
    "                                fn = home+\"/lfa_models/\" + handle.replace(\"/\",\"~\") + \"_\" + str(rep)\n",
    "                                \n",
    "                                print(\"checking for cached file\", fn)\n",
    "                                if os.path.isfile(fn):\n",
    "                                    print(fn, \"found\")\n",
    "#                                     continue         \n",
    "                                \n",
    "                                m, h, config_dict = gen_and_train(odata, vdata, tdata, draw=True, cog_model=cog_model, emb_w=w, q_weight=qw, monitor=mon, balance_classes=bal, loss=lozz, metrics=metrics, reg_w=w1)\n",
    "#                                 m.summary()\n",
    "#                                 t_sixs, t_qixs, t_hits, _ = t_data\n",
    "#                                 o_sixs, o_qixs, o_hits, _ = o_data\n",
    "\n",
    "                                op_hats = numpy.round( m.predict( [o_qixs, o_sixs, o_hits] ) )\n",
    "                                op_trues = numpy.round(o_out)    \n",
    "                                o_f1 = f1_score(op_trues, op_hats, average=\"macro\")\n",
    "\n",
    "                                vp_hats = numpy.round( m.predict( [v_qixs, v_sixs, v_hits] ) )\n",
    "                                vp_trues = numpy.round(v_out)    \n",
    "                                v_f1 = f1_score(vp_trues, vp_hats, average=\"macro\")\n",
    "    \n",
    "                                p_hats = numpy.round( m.predict( [t_qixs, t_sixs, t_hits] ) )\n",
    "                                p_trues = numpy.round(t_out)    \n",
    "                                t_f1 = f1_score(p_trues, p_hats, average=\"macro\")\n",
    "                \n",
    "#                                 if t_f1 > max_w[1]:\n",
    "#                                     max_w = (w1,t_f1)\n",
    "                \n",
    "                                print(w1, \":F1s v/t \", o_f1, v_f1, t_f1)\n",
    "                                f1s.append((handle, w0, w1, v_f1, t_f1))\n",
    "                \n",
    "                                config_dict[\"q_weight\"]=qw\n",
    "                                config_dict[\"monitor_value\"]=mon\n",
    "                                config_dict[\"balance_classes\"]=bal\n",
    "                                config_dict[\"loss\"]=lozz\n",
    "                                \n",
    "                                config_dict[\"handle\"] = handle\n",
    "                                \n",
    "                                payload.append( (config_dict, _, _) )\n",
    "                                \n",
    "                                plt.plot(h.history['loss'], label='xH (trn)')\n",
    "                                plt.plot(h.history['val_loss'], label='xH (val)')\n",
    "                                plt.plot(h.history['f1_loss'], label='F1L (trn)')\n",
    "                                plt.plot(h.history['val_f1_loss'], label='F1L (val)')\n",
    "                                plt.title(handle)\n",
    "#                                 plt.ylabel('MAE value')\n",
    "#                                 plt.xlabel('No. epoch')\n",
    "                                plt.legend(loc=\"upper right\")\n",
    "                                plt.show()\n",
    "                                \n",
    "                                if overwrite_disc:\n",
    "                                    try:\n",
    "                                        m.save(home+\"/lfa_models/\" + handle.replace(\"/\",\"~\") + \"_\" + str(rep), save_format=\"h5\")\n",
    "                                    except OSError as ose:\n",
    "                                        print(ose)\n",
    "                                        import shutil\n",
    "                                        shutil.rmtree(fn)\n",
    "                                        m.save(home+\"/lfa_models/\" + handle.replace(\"/\",\"~\") + \"_\" + str(rep), save_format=\"h5\")\n",
    "                                                                \n",
    "        for h, ww,w,v,t in f1s:\n",
    "    #         print(h, ww,w,\"\\t\", v,\"\\t\", t)\n",
    "            print(h, ww,w,\"\\t\\t\", t)\n",
    "\n",
    "    mn = numpy.median([tup[4] for tup in f1s])\n",
    "    mae = numpy.mean([numpy.abs(tup[4]-mn) for tup in f1s])\n",
    "    print(mn, mae)\n",
    "    \n",
    "    return payload, data_bundle\n",
    "\n",
    "#########################################\n",
    "\n",
    "# try:\n",
    "#     del data_bundle\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "# try:\n",
    "#     data_bundle\n",
    "#     print(\"DATA FOUJND\")\n",
    "#     (sixs, qixs, hout) = (None, None, None)\n",
    "# except:\n",
    "#     data_bundle = None\n",
    "# (sixs, qixs, hout) = pickle.load(open(home+\"/real_data/XL1041.p\", \"rb\"))\n",
    "    \n",
    "\n",
    "history_results, data_bundle = step_through_cog_models(data_bundle=data_bundle)\n",
    "(o_data, v_data, t_data, sid_six_lookup, qid_qix_lookup) = data_bundle\n",
    "\n",
    "(o_sixs, o_qixs, o_hits, o_out), (v_sixs, v_qixs, v_hits, v_out), (t_sixs, t_qixs, t_hits, t_out) = (o_data, v_data, t_data)\n",
    "print(numpy.array(o_hits).shape)\n",
    "print(numpy.array(v_hits).shape)\n",
    "print(numpy.array(t_hits).shape)\n",
    "\n",
    "print(\"DATAGEN DONE\")\n",
    "print(max(o_qixs), max(v_qixs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# f1_kde={}\n",
    "# acc_kde={}\n",
    "# upto=10\n",
    "# for h in seen:\n",
    "#     print(h)\n",
    "#     pears = numpy.array(agg[h])\n",
    "#     f1s = [p[0] for p in pears]\n",
    "#     mu_f1 = numpy.mean(f1s)\n",
    "#     accs = [p[1] for p in pears]\n",
    "#     mu_acc = numpy.mean(accs)\n",
    "#     print(h, len(pears))\n",
    "#     print(pears)\n",
    "# #     print(av_f1, av_acc)\n",
    "#     print(\"\\t\", numpy.round(mu_f1,4), numpy.round(mu_acc,4))\n",
    "#     f1_kde[h] = f1s[0:upto]\n",
    "#     acc_kde[h] = accs[0:upto]\n",
    "\n",
    "# print(\"Plot of resultant F1 score\")\n",
    "# # print(f1_kde)\n",
    "# for row in f1_kde:\n",
    "#     print(row)\n",
    "# ax1 = pandas.DataFrame(f1_kde).plot.kde(bw_method=.8, figsize=(8,8))\n",
    "# ax1.set_xlabel(\"$F_{1}$\")\n",
    "# ax1.set_title(\"Distribution of prediction $F_{1}$ across models\")\n",
    "\n",
    "# print(\"Plot of resultant F1 score\")\n",
    "# print(acc_kde)\n",
    "# ax2 = pandas.DataFrame(acc_kde).plot.kde(bw_method=.8, figsize=(8,8))\n",
    "# ax2.set_xlabel(\"Accuracy\")\n",
    "# ax2.set_title(\"Distribution of prediction accuracy across models\")\n",
    "\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Longitudinal_Datagen.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
