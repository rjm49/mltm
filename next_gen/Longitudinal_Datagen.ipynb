{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "colab_type": "code",
    "id": "Wf0XGW7uQbqT",
    "outputId": "7c4ebc30-d696-46d7-edb4-9ecca5f2b728"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     qn_id     activity_name  \\\n",
      "qn_id                                                          \n",
      "Dialogue 1~1.0              Dialogue 1~1.0        Dialogue 1   \n",
      "Dialogue 1~2.0              Dialogue 1~2.0        Dialogue 1   \n",
      "Dialogue 1~3.0              Dialogue 1~3.0        Dialogue 1   \n",
      "Dialogue 1~4.0              Dialogue 1~4.0        Dialogue 1   \n",
      "Dialogue 1~5.0              Dialogue 1~5.0        Dialogue 1   \n",
      "Minimal pairs 1~1.0    Minimal pairs 1~1.0   Minimal pairs 1   \n",
      "Minimal pairs 1~2.0    Minimal pairs 1~2.0   Minimal pairs 1   \n",
      "Minimal pairs 1~3.0    Minimal pairs 1~3.0   Minimal pairs 1   \n",
      "Minimal pairs 1~4.0    Minimal pairs 1~4.0   Minimal pairs 1   \n",
      "Minimal pairs 1~5.0    Minimal pairs 1~5.0   Minimal pairs 1   \n",
      "Minimal pairs 1~6.0    Minimal pairs 1~6.0   Minimal pairs 1   \n",
      "Minimal pairs 1~7.0    Minimal pairs 1~7.0   Minimal pairs 1   \n",
      "Minimal pairs 1~8.0    Minimal pairs 1~8.0   Minimal pairs 1   \n",
      "Minimal pairs 1~9.0    Minimal pairs 1~9.0   Minimal pairs 1   \n",
      "Minimal pairs 1~10.0  Minimal pairs 1~10.0   Minimal pairs 1   \n",
      "Minimal pairs 1~11.0  Minimal pairs 1~11.0   Minimal pairs 1   \n",
      "Minimal pairs 1~12.0  Minimal pairs 1~12.0   Minimal pairs 1   \n",
      "Minimal pairs 1~13.0  Minimal pairs 1~13.0   Minimal pairs 1   \n",
      "Minimal pairs 1~14.0  Minimal pairs 1~14.0   Minimal pairs 1   \n",
      "Minimal pairs 1~15.0  Minimal pairs 1~15.0   Minimal pairs 1   \n",
      "Speed reading 14~1.0  Speed reading 14~1.0  Speed reading 14   \n",
      "Speed reading 14~2.0  Speed reading 14~2.0  Speed reading 14   \n",
      "Speed reading 14~3.0  Speed reading 14~3.0  Speed reading 14   \n",
      "Speed reading 14~4.0  Speed reading 14~4.0  Speed reading 14   \n",
      "Speed reading 14~5.0  Speed reading 14~5.0  Speed reading 14   \n",
      "Error finding 1~1.0    Error finding 1~1.0   Error finding 1   \n",
      "Error finding 1~2.0    Error finding 1~2.0   Error finding 1   \n",
      "Error finding 1~3.0    Error finding 1~3.0   Error finding 1   \n",
      "Error finding 1~4.0    Error finding 1~4.0   Error finding 1   \n",
      "Error finding 1~5.0    Error finding 1~5.0   Error finding 1   \n",
      "...                                    ...               ...   \n",
      "Spelling 3~6.0              Spelling 3~6.0        Spelling 3   \n",
      "Spelling 3~7.0              Spelling 3~7.0        Spelling 3   \n",
      "Spelling 3~8.0              Spelling 3~8.0        Spelling 3   \n",
      "Spelling 3~9.0              Spelling 3~9.0        Spelling 3   \n",
      "Spelling 3~10.0            Spelling 3~10.0        Spelling 3   \n",
      "Spelling 3~11.0            Spelling 3~11.0        Spelling 3   \n",
      "Spelling 3~12.0            Spelling 3~12.0        Spelling 3   \n",
      "Spelling 3~13.0            Spelling 3~13.0        Spelling 3   \n",
      "Spelling 3~14.0            Spelling 3~14.0        Spelling 3   \n",
      "Spelling 3~15.0            Spelling 3~15.0        Spelling 3   \n",
      "Spelling 3~16.0            Spelling 3~16.0        Spelling 3   \n",
      "Spelling 3~17.0            Spelling 3~17.0        Spelling 3   \n",
      "Spelling 3~18.0            Spelling 3~18.0        Spelling 3   \n",
      "Speed reading 13~1.0  Speed reading 13~1.0  Speed reading 13   \n",
      "Speed reading 13~2.0  Speed reading 13~2.0  Speed reading 13   \n",
      "Speed reading 13~3.0  Speed reading 13~3.0  Speed reading 13   \n",
      "Speed reading 13~4.0  Speed reading 13~4.0  Speed reading 13   \n",
      "Speed reading 13~5.0  Speed reading 13~5.0  Speed reading 13   \n",
      "Error finding 6~1.0    Error finding 6~1.0   Error finding 6   \n",
      "Error finding 6~2.0    Error finding 6~2.0   Error finding 6   \n",
      "Error finding 6~3.0    Error finding 6~3.0   Error finding 6   \n",
      "Error finding 6~4.0    Error finding 6~4.0   Error finding 6   \n",
      "Error finding 6~5.0    Error finding 6~5.0   Error finding 6   \n",
      "Error finding 6~6.0    Error finding 6~6.0   Error finding 6   \n",
      "Error finding 6~7.0    Error finding 6~7.0   Error finding 6   \n",
      "Phrasal verbs 3~1.0    Phrasal verbs 3~1.0   Phrasal verbs 3   \n",
      "Phrasal verbs 3~2.0    Phrasal verbs 3~2.0   Phrasal verbs 3   \n",
      "Phrasal verbs 3~3.0    Phrasal verbs 3~3.0   Phrasal verbs 3   \n",
      "Phrasal verbs 3~4.0    Phrasal verbs 3~4.0   Phrasal verbs 3   \n",
      "Phrasal verbs 3~5.0    Phrasal verbs 3~5.0   Phrasal verbs 3   \n",
      "\n",
      "                                activity_skill  \n",
      "qn_id                                           \n",
      "Dialogue 1~1.0               speaking~dialogue  \n",
      "Dialogue 1~2.0               speaking~dialogue  \n",
      "Dialogue 1~3.0               speaking~dialogue  \n",
      "Dialogue 1~4.0               speaking~dialogue  \n",
      "Dialogue 1~5.0               speaking~dialogue  \n",
      "Minimal pairs 1~1.0    listening~minimal_pairs  \n",
      "Minimal pairs 1~2.0    listening~minimal_pairs  \n",
      "Minimal pairs 1~3.0    listening~minimal_pairs  \n",
      "Minimal pairs 1~4.0    listening~minimal_pairs  \n",
      "Minimal pairs 1~5.0    listening~minimal_pairs  \n",
      "Minimal pairs 1~6.0    listening~minimal_pairs  \n",
      "Minimal pairs 1~7.0    listening~minimal_pairs  \n",
      "Minimal pairs 1~8.0    listening~minimal_pairs  \n",
      "Minimal pairs 1~9.0    listening~minimal_pairs  \n",
      "Minimal pairs 1~10.0   listening~minimal_pairs  \n",
      "Minimal pairs 1~11.0   listening~minimal_pairs  \n",
      "Minimal pairs 1~12.0   listening~minimal_pairs  \n",
      "Minimal pairs 1~13.0   listening~minimal_pairs  \n",
      "Minimal pairs 1~14.0   listening~minimal_pairs  \n",
      "Minimal pairs 1~15.0   listening~minimal_pairs  \n",
      "Speed reading 14~1.0     reading~speed_reading  \n",
      "Speed reading 14~2.0     reading~speed_reading  \n",
      "Speed reading 14~3.0     reading~speed_reading  \n",
      "Speed reading 14~4.0     reading~speed_reading  \n",
      "Speed reading 14~5.0     reading~speed_reading  \n",
      "Error finding 1~1.0   writing~error_correction  \n",
      "Error finding 1~2.0   writing~error_correction  \n",
      "Error finding 1~3.0   writing~error_correction  \n",
      "Error finding 1~4.0   writing~error_correction  \n",
      "Error finding 1~5.0   writing~error_correction  \n",
      "...                                        ...  \n",
      "Spelling 3~6.0                writing~spelling  \n",
      "Spelling 3~7.0                writing~spelling  \n",
      "Spelling 3~8.0                writing~spelling  \n",
      "Spelling 3~9.0                writing~spelling  \n",
      "Spelling 3~10.0               writing~spelling  \n",
      "Spelling 3~11.0               writing~spelling  \n",
      "Spelling 3~12.0               writing~spelling  \n",
      "Spelling 3~13.0               writing~spelling  \n",
      "Spelling 3~14.0               writing~spelling  \n",
      "Spelling 3~15.0               writing~spelling  \n",
      "Spelling 3~16.0               writing~spelling  \n",
      "Spelling 3~17.0               writing~spelling  \n",
      "Spelling 3~18.0               writing~spelling  \n",
      "Speed reading 13~1.0     reading~speed_reading  \n",
      "Speed reading 13~2.0     reading~speed_reading  \n",
      "Speed reading 13~3.0     reading~speed_reading  \n",
      "Speed reading 13~4.0     reading~speed_reading  \n",
      "Speed reading 13~5.0     reading~speed_reading  \n",
      "Error finding 6~1.0   writing~error_correction  \n",
      "Error finding 6~2.0   writing~error_correction  \n",
      "Error finding 6~3.0   writing~error_correction  \n",
      "Error finding 6~4.0   writing~error_correction  \n",
      "Error finding 6~5.0   writing~error_correction  \n",
      "Error finding 6~6.0   writing~error_correction  \n",
      "Error finding 6~7.0   writing~error_correction  \n",
      "Phrasal verbs 3~1.0      reading~phrasal_verbs  \n",
      "Phrasal verbs 3~2.0      reading~phrasal_verbs  \n",
      "Phrasal verbs 3~3.0      reading~phrasal_verbs  \n",
      "Phrasal verbs 3~4.0      reading~phrasal_verbs  \n",
      "Phrasal verbs 3~5.0      reading~phrasal_verbs  \n",
      "\n",
      "[100 rows x 3 columns]\n",
      "started\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from importlib import reload\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from random import shuffle, choice, randint\n",
    "\n",
    "import math\n",
    "import keras\n",
    "import tensorflow\n",
    "\n",
    "import pickle\n",
    "\n",
    "from logistic_utils import pr_to_spread, logistic\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "import sys\n",
    "# sys.path.append('/content/gdrive/My Drive/Colab Notebooks')\n",
    "# from NN_utils import BigTable, WeightClip\n",
    "from NN_utils import WeightClip\n",
    "# def calc_probs_from_embs(students,questions):\n",
    "#     students2 = numpy.repeat(students, len(questions), axis=0)\n",
    "#     questions2 = numpy.tile(questions, (len(students),1))\n",
    "#     zmask = numpy.isclose(questions2,-10).astype(int)\n",
    "#     diffs = students2-questions2\n",
    "#     prs = numpy.exp(diffs)/(1.0+ numpy.exp(diffs))\n",
    "#     prs = numpy.maximum(zmask,prs)\n",
    "#     probs2 = numpy.prod(prs, axis=1).reshape(len(students), len(questions))\n",
    "#     return probs2\n",
    "\n",
    "# def calc_probs(s,q):\n",
    "#     zmask = numpy.isclose(q,-10).astype(int)\n",
    "#     diff = s-q\n",
    "#     prs = 1.0/(1.0+ numpy.exp(-diff))\n",
    "#     prs = numpy.maximum(zmask,prs)\n",
    "#     # print(prs)\n",
    "#     if len(q.shape)>1 and len(q.shape[0]) > 1:\n",
    "#       raise Exception(\"tensor is wrong shape, duh\")\n",
    "#       # pr = pr.reshape(len(q))\n",
    "#     pr = numpy.prod(prs)\n",
    "#     return pr, zmask\n",
    "\n",
    "# home = \"/content/gdrive/My Drive/Colab Notebooks\"\n",
    "home=\".\"\n",
    "\n",
    "mapping = pandas.read_csv(home+\"/real_data/qn_act_map.csv\")\n",
    "mapping.index = mapping.qn_id\n",
    "# mapping.drop(\"qn_id\", axis=1, inplace=True)\n",
    "print(mapping[0:100])\n",
    "print(\"started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_databundle():\n",
    "# if True:\n",
    "#     raw_df = pandas.read_csv(home+\"/real_data/Worksheet_1041.csv\")\n",
    "#     print(raw_df.columns)\n",
    "#     print(len(raw_df))\n",
    "#     raw_df = raw_df[raw_df.event_type==\"answer_submitted\"]\n",
    "#     print(len(raw_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QjdJJEs-Qs1u"
   },
   "outputs": [],
   "source": [
    "def progress_one_step(q,s, s_gammas):\n",
    "  pr, zmask = calc_probs(s,q)\n",
    "  active_in_q = 1-zmask\n",
    "  # print(active_in_q)\n",
    "  # print(\"pr is\", pr)\n",
    "  passed = 0\n",
    "  if (numpy.random.random() <= pr):\n",
    "    passed = 1\n",
    "  s= s + s_gammas*active_in_q # learning rates from a successful attempt\n",
    "  # else:\n",
    "  #   s= s + s_rhos*active_in_q # learning rates from an unsauccessful attempt\n",
    "  return passed, pr, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wAfdTWTpkE1K"
   },
   "outputs": [],
   "source": [
    "def run_data(students, questions, gammas, model_to_train=None):\n",
    "  from collections import defaultdict, Counter\n",
    "  sixs = []\n",
    "  qixs = []\n",
    "  hits = []\n",
    "  outps = []\n",
    "  \n",
    "  r = -1\n",
    "  scores = defaultdict(int)\n",
    "  hit_counter = {}\n",
    "  \n",
    "  s_indices = range(len(students))\n",
    "  # bo_selecta = None\n",
    "  for six in s_indices:\n",
    "    print(\"running student, \", six)\n",
    "    s = students[six]\n",
    "    R = 0  # reset reward for new student\n",
    "    ball_bag = list(range(len(questions)))\n",
    "    while ball_bag:\n",
    "      # print(\"WSH\",s.shape)\n",
    "      # print(numpy.mean(s), numpy.min(s), numpy.max(s))\n",
    "      # for sval in s:\n",
    "      #   print(sval)\n",
    "      # raise Exception(\"WANK\")\n",
    "      # if bo_selecta is None:\n",
    "      bo_selecta = random.choice(ball_bag)\n",
    "      q = questions[bo_selecta]\n",
    "      passed, pr, s_ = progress_one_step(q,s, gammas[six])\n",
    "      students[six] = s_ # Crucially, update the student to make progress...\n",
    "      # print(six, bo_selecta, numpy.mean(students[six]), pr)\n",
    "      if passed:\n",
    "        # print(\"***PASSED***\", six, bo_selecta, pr)\n",
    "        ball_bag.remove(bo_selecta)\n",
    "\n",
    "      # hit_counter[(six, bo_selecta)] += 1\n",
    "      # hit_counter\n",
    "    \n",
    "      if six not in hit_counter:\n",
    "        print(\"INIT'G zeros FOR\", six)\n",
    "        hit_counter[six] = [int(0)]*n_questions #numpy.zeros(n_questions, dtype=\"uint8\")\n",
    "\n",
    "      sixs.append( [int(six)])\n",
    "      qixs.append([int(bo_selecta)])\n",
    "      hits.append( tuple(hit_counter[six]) )\n",
    "      outps.append( [int(passed)] )\n",
    "\n",
    "      # neue = hit_counter[six]\n",
    "      # neue[bo_selecta] += 1\n",
    "      hit_counter[six][bo_selecta] += 1\n",
    "\n",
    "      R += r\n",
    "      # bo_selecta = None\n",
    "    print(R)\n",
    "    scores[six] = R\n",
    "\n",
    "  # if model_to_train:\n",
    "  #   phat = model_to_train.predict([[six], [bo_selecta], [hit_counter[(six,bo_selecta)]] ])\n",
    "  #   mae = abs(pr - phat)\n",
    "  #   print(pr, phat, mae)\n",
    "  #   model_to_train.train_on_batch( [ [six], [bo_selecta], [hit_counter[(six,bo_selecta)]] ], [passed] )\n",
    " \n",
    "  # print(\"Die Arrays werden in Numpy Datentypen verwandelt.\")\n",
    "  # sixs = numpy.array(sixs, dtype=\"uint8\")\n",
    "  # qixs = numpy.array(qixs, dtype=\"uint8\")\n",
    "  # hits = numpy.array(hits, dtype=\"uint8\")\n",
    "  # outps = numpy.array(outps, dtype=\"uint8\")\n",
    "\n",
    "  # print(hits.shape)\n",
    "  # print(outps.shape)\n",
    "\n",
    "  # if model_to_train:\n",
    "    # phat = model_to_train.predict([[six], [bo_selecta], [hit_counter[(six,bo_selecta)]] ])\n",
    "    # mae = abs(pr - phat)\n",
    "    # print(pr, phat, mae)\n",
    "    # model_to_train.fit( inps, outps )\n",
    "  # for (k,v) in scores.items():\n",
    "  #   print(k,v)\n",
    "  print(\"Der Lauf is beendet.\")\n",
    "  vals = list(scores.values())\n",
    "  return numpy.mean(vals), sixs, qixs, hits, outps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P7GZiAAj6KSW"
   },
   "outputs": [],
   "source": [
    "from keras.regularizers import l1\n",
    "from keras.layers import Reshape, Dense, Dropout, add, multiply, subtract, GaussianNoise, GaussianDropout, Input, Lambda, Embedding, concatenate, Flatten, Maximum, Multiply, dot, Layer\n",
    "from keras import backend as K, Model\n",
    "from keras.initializers import RandomUniform, RandomNormal\n",
    "\n",
    "def hard_sigmoid(x):\n",
    "    return np.maximum(0, np.minimum(1, (x + 2) / 4))\n",
    "\n",
    "def binary_regulariser(x):\n",
    "    return K.sum( 1.0-(4.0*K.pow((0.5 - x),2)) )\n",
    "    # return K.sum(K.log(2*x) +K.log(2*(1-x)))\n",
    "    # return K.log(2*x) +K.log(2*(1-x))\n",
    "    \n",
    "from f1_metric import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P7GZiAAj6KSW"
   },
   "outputs": [],
   "source": [
    "#5e-5/row_w\n",
    "def generate_MLTM_raw_model(n_questions, n_students, row_w, ptqs=None, \n",
    "                        loss=\"binary_crossentropy\",\n",
    "                        metrics=None, init50=True, deep_HEU=False, reg=None, reg_w=None,\n",
    "                        pos_only=False):\n",
    "    print(\"MLTM RAW:\\nROW W is \", row_w)\n",
    "    print(\"reg is\",reg)\n",
    "    print(n_questions, n_students)\n",
    "\n",
    "    from keras.initializers import RandomNormal, RandomUniform, Constant\n",
    "    from keras.regularizers import L2\n",
    "    from keras.constraints import NonNeg\n",
    "    psi_sel = Input(shape=(1,), name=\"psi_select\", dtype=\"int32\")\n",
    "    qn_sel = Input(shape=(1,), name=\"q_select\", dtype=\"int32\")\n",
    "    hit_counter = Input(shape=(n_questions, ), name=\"hit_counter\", dtype=\"float32\")\n",
    "\n",
    "    sp = pr_to_spread(.5, row_w, as_A_and_D=False)\n",
    "#     long_clip = WeightClip(-math.inf, math.inf)\n",
    "#     long_clip = WeightClip(-5,5+sp)\n",
    "#     if pos_only:\n",
    "#     long_clip = WeightClip(0, 6+2*sp)\n",
    "\n",
    "#     q_init = RandomNormal(mean=sp) if init50 else \"uniform\"\n",
    "#     q_init = RandomNormal(mean=-sp) if init50 else \"uniform\"\n",
    "    q_init = RandomUniform(minval=sp-0.5, maxval=sp+0.5) if init50 else \"uniform\"\n",
    "#     q_init = \"uniform\"\n",
    "\n",
    "    if reg_w is None:\n",
    "        reg_w = 0\n",
    "        \n",
    "    reg_pen = 0.001/(((2*(1 if sp==0 else sp))**2)*(n_students*row_w))\n",
    "    print(\"reg_pen is\", reg_pen)\n",
    "    \n",
    "    qn_emb = Embedding(n_questions, row_w, \n",
    "#                        embeddings_regularizer=L2(100.0/(n_questions*row_w)**2) if \"l1\" in reg else None, \n",
    "#                        embeddings_regularizer=L2( reg_pen ),\n",
    "                       embeddings_initializer=q_init,\n",
    "                       name=\"qn_embedding\")\n",
    "#     qn_row = Flatten()(long_clip(qn_emb(qn_sel)))\n",
    "    qn_row = Flatten()(qn_emb(qn_sel))\n",
    "#     qn_row = Dense(row_w)(qn_row)\n",
    "    \n",
    "#     if reg_w is None:\n",
    "#         this_w = 32e-8\n",
    "#     else:\n",
    "#         this_w = (reg_w/row_w)\n",
    "    #embeddings_initializer=RandomNormal(mean=1+sp)\n",
    "    s_init = RandomNormal(mean=2*sp) if init50 else \"uniform\"\n",
    "#     s_init = \"uniform\"\n",
    "#     pos_clip = WeightClip(0, math.inf)\n",
    "#     gamma_row = Flatten()(NonNeg()(Embedding(n_students, row_w, name=\"gammas\")(psi_sel)))\n",
    "    gamma_row = Flatten()(Embedding(n_students, row_w, name=\"gammas\")(psi_sel))\n",
    "    \n",
    "    alpha_row = Flatten()(Embedding(n_students, row_w, \n",
    "                                                embeddings_initializer=s_init, \n",
    "#                                                 embeddings_regularizer=L2(200.0/(n_students*row_w)**2) if \"l2\" in reg else None, \n",
    "                                                embeddings_regularizer=L2( reg_pen ),\n",
    "                                                name=\"alphas\")(psi_sel))\n",
    "#     alpha_row = Dense(row_w)(alpha_row)\n",
    "#     if reg:\n",
    "#         alpha_row = tensorflow.keras.layers.ActivityRegularization(l2=0.01/row_w)(alpha_row)\n",
    "  \n",
    "    kc_practice = Dense(row_w, use_bias=True, name=\"qk_loadings\")(hit_counter)\n",
    "#     kc_practice = Dense(row_w, use_bias=False, kernel_constraint=NonNeg(), name=\"qk_loadings\",)(hit_counter)\n",
    "#     kc_practice = Lambda(lambda x: K.log(x+1e-6))(kc_practice)\n",
    "#     kc_practice = Dense(row_w, kernel_constraint=NonNeg())(kc_practice)\n",
    "#     kc_practice = tensorflow.keras.layers.ActivityRegularization(l1=0.00001/row_w)(kc_practice)\n",
    "    \n",
    "#     psi_row = long_clip( add( [alpha_row, multiply([kc_practice, gamma_row])]) )\n",
    "#     psi_row = Dense(row_w, activation=\"linear\")(psi_row)\n",
    "\n",
    "    psi_row = add( [alpha_row, multiply([kc_practice, gamma_row])])\n",
    "#     psi_row = alpha_row\n",
    "    difs = subtract([psi_row, qn_row], name=\"difs\")\n",
    "#     difs = tensorflow.keras.layers.ActivityRegularization(l2=0.001/row_w)(difs)\n",
    "#     difs = Dense(row_w, activity_regularizer=L2(0.1/row_w))(difs)\n",
    "\n",
    "    Prs = Lambda(lambda z: K.sigmoid(z))(difs)\n",
    "    \n",
    "#     logs = Lambda(lambda ps: K.log(ps), name=\"log_step\")(Prs)\n",
    "#     summed_logs = Lambda(lambda ps: K.sum(ps, axis=-1, keepdims=True), name=\"sum_step\")(logs)\n",
    "#     score = Lambda(lambda ps: K.exp(ps), name=\"exp_step\")(summed_logs)\n",
    "\n",
    "    score = Lambda(lambda prs: K.prod(prs, axis=-1, keepdims=True))(Prs)\n",
    "#     score = Lambda(lambda prs: tensorflow.math.reduce_logsumexp(prs, axis=-1, keepdims=True))(Prs)\n",
    "\n",
    "    model = Model(inputs=[qn_sel, psi_sel, hit_counter], outputs=score)\n",
    "    model.compile(optimizer=\"adam\", loss=loss, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_MLTM0_model(n_questions, n_students, row_w, \n",
    "                        loss=\"binary_crossentropy\",\n",
    "                        metrics=None,\n",
    "                        l2=False):\n",
    "    print(\"ROW W is \", row_w)\n",
    "    # def generate_qs_model(qn_table, psi_table, optimiser, _mode=\"MXFN\", loss=\"MSE\"):  \n",
    "    from keras.initializers import RandomNormal, RandomUniform, Constant\n",
    "    from keras.regularizers import L1,L2\n",
    "    psi_sel = Input(shape=(1,), name=\"psi_select\", dtype=\"int32\")\n",
    "    qn_sel = Input(shape=(1,), name=\"q_select\", dtype=\"int32\")\n",
    "    hit_counter = Input(shape=(n_questions, ), name=\"hit_counter\", dtype=\"float32\")\n",
    "\n",
    "    sp = pr_to_spread(.5, row_w, as_A_and_D=False)\n",
    "    pos_clip = WeightClip(0.0000, 100)\n",
    "#     bin_clip = WeightClip(0.0000, 1.0000)\n",
    "\n",
    "    base = 1\n",
    "    q_init = RandomNormal(mean=base)\n",
    "    qn_emb = Embedding(n_questions, row_w, embeddings_initializer=q_init, \n",
    "#                        activity_regularizer=L1(1/row_w), \n",
    "                       name=\"qn_embedding\")\n",
    "    qn_row = Flatten(name=\"qn_row_out\")(pos_clip(qn_emb(qn_sel)))\n",
    "    \n",
    "# Q MAsking STilL reQuired\n",
    "    k=1000\n",
    "    qmask = Lambda(lambda x: K.clip(x*k,0.0000,1.0000))(qn_row)\n",
    "    \n",
    "    \n",
    "    w2 = 32e-8/row_w\n",
    "    print(\"using penalty\", w2)\n",
    "    alpha_row = Embedding(n_students, row_w, \n",
    "                          embeddings_initializer=RandomNormal(mean=base+sp), name=\"alphas\",\n",
    "                          embeddings_regularizer=L2(w2),\n",
    "                          )(psi_sel)\n",
    "    gamma_row = Embedding(n_students, row_w, name=\"gammas\")(psi_sel)\n",
    "    alpha_row = Flatten()(alpha_row)\n",
    "    gamma_row = Flatten()(gamma_row)\n",
    "\n",
    "    kc_practice = Dense(row_w, name=\"qk_loadings\", use_bias=False)(hit_counter)\n",
    "    psi_row = add( [alpha_row, multiply([kc_practice, gamma_row])], name=\"psi_row_out\")\n",
    "\n",
    "    difs = subtract([psi_row, qn_row], name=\"difs\")\n",
    "    Prs = Lambda(lambda z: K.sigmoid(z))(difs)\n",
    "    Prs = Lambda(lambda x: K.pow(x[0],K.abs(x[1])) )([Prs, qmask])\n",
    "\n",
    "    score = Lambda(lambda ps: K.prod(ps, axis=1, keepdims=True))(Prs)\n",
    "    \n",
    "    # p_LFA = Ïƒ(a_s + Î£ k âˆŠ skills(q): ðœ¸_k*n_sk - d_k)\n",
    "\n",
    "    model = Model(inputs=[qn_sel, psi_sel, hit_counter], outputs=score)\n",
    "    model.compile(optimizer=\"adam\", loss=loss, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def my_regularizer(x):\n",
    "#     return K.sum(K.square(x), axis=1)\n",
    "\n",
    "def generate_MLTMb_model(n_questions, n_students, row_w, \n",
    "                        loss=\"binary_crossentropy\",\n",
    "                        metrics=None,\n",
    "                        reg=None,\n",
    "                        reg_w = None):\n",
    "    print(\"MLTMb model with reg\", reg)\n",
    "    print(\"ROW W is \", row_w)\n",
    "    # def generate_qs_model(qn_table, psi_table, optimiser, _mode=\"MXFN\", loss=\"MSE\"):  \n",
    "    from keras.initializers import RandomNormal, RandomUniform, Constant\n",
    "    from keras.regularizers import L1,L2\n",
    "    psi_sel = Input(shape=(1,), name=\"psi_select\", dtype=\"int32\")\n",
    "    qn_sel = Input(shape=(1,), name=\"q_select\", dtype=\"int32\")\n",
    "    hit_counter = Input(shape=(n_questions, ), name=\"hit_counter\", dtype=\"float32\")\n",
    "\n",
    "    sp = pr_to_spread(.5, row_w, as_A_and_D=False)\n",
    "\n",
    "    zer0 = Lambda(lambda x: K.cast(K.clip(x,0,0), dtype=\"int32\"))\n",
    "    bin_clip = WeightClip(0.0000, 1.0000)\n",
    "#     lam_clip = Lambda(lambda x: K.clip(x,0.0000,1.0000))\n",
    "#     long_clip = WeightClip(-100,100)\n",
    "    \n",
    "    etas = Embedding(1, row_w, input_length=1, \n",
    "                     embeddings_initializer=RandomNormal(0),\n",
    "                     name=\"skill_diffs\")\n",
    "    \n",
    "    l2_w = reg_w if (reg_w is not None) else 1.5e-06\n",
    "    effs = Embedding(n_questions, row_w, \n",
    "#                      embeddings_constraint=WeightClip(0, 1), \n",
    "                     embeddings_initializer=RandomUniform(0.999,1), \n",
    "                     name=\"qn_embedding\",\n",
    "#                      embeddings_initializer = RandomNormal(1),\n",
    "#                      embeddings_regularizer=L2(10000.0/(n_questions*row_w)**2) if reg==\"l1\" else None,\n",
    "#                      embeddings_regularizer=L1(l2_w/(n_questions*row_w)) #if reg==\"l1\" else None,\n",
    "                        activity_regularizer=L1(1.55e-06) #if reg==\"l1\" else None,\n",
    "                     )\n",
    "    \n",
    "#     effs = Embedding(n_questions, row_w, name=\"qn_embedding\")\n",
    "\n",
    "#     qn_row = Flatten(name=\"qn_row_out\")(long_clip(etas(zer0(qn_sel))))\n",
    "    qn_row = Flatten(name=\"qn_row_out\")(etas(zer0(qn_sel)))\n",
    "\n",
    "#     delta_loading_is_qmask = Flatten(name=\"qmask_out\")(lam_clip(bin_clip(effs(qn_sel))))\n",
    "#     delta_loading_is_qmask = Flatten()(effs(qn_sel))\n",
    "    delta_loading_is_qmask = Flatten(name=\"qmask_out\")(bin_clip(effs(qn_sel)))\n",
    "\n",
    "# Q MAsking STilL reQuired\n",
    "    \n",
    "    w2 = 32*1.5e-6 / row_w\n",
    "    print(\"l2 penalty is\", w2)\n",
    "    alpha_row = Embedding(n_students, row_w, \n",
    "                          embeddings_initializer=RandomNormal(sp), name=\"alphas\",\n",
    "#                           embeddings_regularizer=L2(l2_w/(n_students*row_w)**2) if reg==\"l2\" else None,\n",
    "#                           embeddings_regularizer=L2(l2_w) if reg==\"l2\" else None,\n",
    "#                           embeddings_regularizer=L2(0.01) if reg==\"l2\" else None,\n",
    "#                             embeddings_regularizer=L2(w2)\n",
    "                         )(psi_sel)\n",
    "    gamma_row = Embedding(n_students, row_w, name=\"gammas\", \n",
    "                          embeddings_initializer=RandomNormal(0),\n",
    "                         )(psi_sel)\n",
    "    alpha_row = Flatten()(alpha_row)\n",
    "    gamma_row = Flatten()(gamma_row)\n",
    "\n",
    "    kc_practice = Dense(row_w, name=\"qk_loadings\", use_bias=False)(hit_counter)\n",
    "    psi_row = add( [alpha_row, multiply([kc_practice, gamma_row])], name=\"psi_row_out\")\n",
    "\n",
    "    difs = subtract([psi_row, qn_row], name=\"difs\")\n",
    "    Prs = Lambda(lambda z: K.sigmoid(z))(difs)\n",
    "    Prs = Lambda(lambda x: K.pow(x[0],K.abs(x[1])) )([Prs, delta_loading_is_qmask])\n",
    "    \n",
    "    score = Lambda(lambda ps: K.prod(ps, axis=1, keepdims=True))(Prs)\n",
    "    \n",
    "    # p_LFA = Ïƒ(a_s + Î£ k âˆŠ skills(q): ðœ¸_k*n_sk - d_k)\n",
    "\n",
    "    model = Model(inputs=[qn_sel, psi_sel, hit_counter], outputs=score)\n",
    "    model.compile(optimizer=\"adam\", loss=loss, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_CFM_model(n_questions, n_students, row_w, loss=\"binary_crossentropy\", \n",
    "                     sfocus=False, metrics=None, reg=None, reg_w=None):\n",
    "    print(\"Using CFM model!\")\n",
    "\n",
    "    print(\"ROW W is \", row_w)\n",
    "    from keras.initializers import RandomNormal, RandomUniform, Constant\n",
    "    from keras.layers import Add, Reshape, Multiply\n",
    "    from keras.regularizers import L1, L2\n",
    "    \n",
    "    psi_sel = Input(shape=(1,), name=\"psi_select\", dtype=\"int32\")\n",
    "    qn_sel = Input(shape=(1,), name=\"q_select\", dtype=\"int32\")\n",
    "    hit_counter = Input(shape=(n_questions, ), name=\"hit_counter\", dtype=\"float32\")\n",
    "\n",
    "    zer0 = Lambda(lambda x: K.cast(K.clip(x,0,0), dtype=\"int32\"))\n",
    "    bin_clip = WeightClip(0.0000, 1.0000)\n",
    "#     bin_clip = Lambda(lambda c: K.clip(c,0,1))\n",
    "    \n",
    "    reg_w = 0 if reg_w is None else reg_w\n",
    "    \n",
    "    sp = pr_to_spread(.5, row_w, as_A_and_D=False)\n",
    "    B = Embedding(1 , row_w, name=\"skill_diffs\", embeddings_initializer=RandomNormal(0, stddev=0.1))\n",
    "    theta_i = Flatten()(Embedding(n_students, 1, name=\"alphas\", \n",
    "                                  embeddings_initializer=RandomNormal(sp, stddev=0.1),\n",
    "#                                   embeddings_regularizer=L2(10),\n",
    "                                  embeddings_regularizer= L2(1.45e-05),# if (\"l2\" in reg) else None,\n",
    "#                                   embeddings_regularizer= L2(reg_w) if (\"l2\" in reg) else None,\n",
    "#                                   embeddings_regularizer= L2(1),# if (\"l2\" in reg) else None,\n",
    "                                 )  (psi_sel))\n",
    "\n",
    "    print(\"theta_i shape\", theta_i.shape)\n",
    "    \n",
    "    Q = Embedding(n_questions, row_w, name=\"qn_embedding\", \n",
    "#                        embeddings_constraint=WeightClip(0, 1), \n",
    "                         embeddings_initializer=RandomUniform(0.99,1),\n",
    "#                          embeddings_regularizer = L1(reg_w / (row_w*n_questions) )# if (\"l1\" in reg) else None,\n",
    "                 )\n",
    "        \n",
    "#     gamma_k = Flatten()(Embedding(1, row_w, name=\"gammas\", embeddings_initializer=RandomNormal(0),)  (zer0(qn_sel)))\n",
    "    gamma_k = Flatten()(Embedding(1, row_w, name=\"gammas\")  (zer0(qn_sel)))\n",
    "\n",
    "    beta_k = Flatten()(B( zer0(qn_sel) ))\n",
    "    q_jk = Flatten()(bin_clip( Q(qn_sel) ))\n",
    "\n",
    "    print(\"shape kc-gammas\", gamma_k.shape)\n",
    "    T_jk = Dense(row_w, name=\"qk_loadings\", use_bias=False)(hit_counter)\n",
    "\n",
    "    prog = multiply([gamma_k, T_jk])\n",
    "    \n",
    "    skill_now = Add()([theta_i, prog])\n",
    "    logit_difs = subtract([skill_now, beta_k])\n",
    "    \n",
    "    print(\"logit shape\", logit_difs.shape)\n",
    "    Prs = Lambda(lambda z: K.sigmoid(z))(logit_difs)\n",
    "    \n",
    "#     Prs = Lambda(lambda mx: K.pow(mx[0],mx[1]))([Prs, q_jk])\n",
    "    Prs = Lambda(lambda mx: mx[1]*mx[0] + (1-mx[1]) )([Prs, q_jk])\n",
    "        \n",
    "    print(\"Prs shape\", Prs.shape)\n",
    "    score = Lambda(lambda ps: K.prod(ps, axis=1, keepdims=True))(Prs)\n",
    "    \n",
    "    # p_LFA = Ïƒ(a_s + Î£ k âˆŠ skills(q): ðœ¸_k*n_sk - d_k)\n",
    "\n",
    "    model = Model(inputs=[qn_sel, psi_sel, hit_counter], outputs=score)\n",
    "    from keras.optimizers import SGD, RMSprop, Nadam, Adam\n",
    "    #   optr = SGD(learning_rate=1.0)#, momentum=0.01, nesterov=True)\n",
    "    #   optr = RMSprop()\n",
    "    optr = Adam()\n",
    "    model.compile(optimizer=optr, loss=loss, metrics=metrics)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cgHm-iOt55vE"
   },
   "outputs": [],
   "source": [
    "def create_AFM_model(n_questions, n_students, row_w, loss=\"binary_crossentropy\", \n",
    "                     sfocus=False, metrics=None, reg=None):\n",
    "    print(\"Using {}AFM model!\".format(\"S\" if sfocus else \"\"))\n",
    "    print(\"reg is\", reg)\n",
    "    \n",
    "    #AFM\n",
    "    # p_ij = sig( a_i + sum[k in KC(j)] b_k + g_k*n_ik )\n",
    "    \n",
    "    #sAFM\n",
    "    # p_ij = sig( sum[k in KC(j)] a_ik + b_k + g_k*n_ik )\n",
    "    \n",
    "    print(\"ROW W is \", row_w)\n",
    "    from keras.initializers import RandomNormal, RandomUniform, Constant\n",
    "    from keras.layers import Add, Reshape, Multiply\n",
    "    from keras.constraints import NonNeg\n",
    "    from keras.regularizers import L2,L1\n",
    "    hit_counter = Input(shape=(n_questions, ), name=\"hit_counter\", dtype=\"float32\")\n",
    "    psi_sel = Input(shape=(1,), name=\"psi_select\", dtype=\"int32\")\n",
    "    qn_sel = Input(shape=(1,), name=\"q_select\", dtype=\"int32\")\n",
    "\n",
    "    zer0 = Lambda(lambda x: K.cast(K.clip(x,0,0), dtype=\"int32\"))\n",
    "    bin_clip = WeightClip(0.0000,1.0000)\n",
    "    \n",
    "    skill_d_ws = Embedding(1 , row_w, name=\"skill_diffs\", embeddings_initializer=RandomNormal(0, stddev=0.1))\n",
    "#   a0 = 5\n",
    "    qn_mx = Embedding(n_questions , row_w, name=\"qn_embedding\", \n",
    "#                        embeddings_constraint=WeightClip(0, 1), \n",
    "                       embeddings_initializer=RandomUniform(0.99,1),\n",
    "#                        activity_regularizer=\"l1\" if (\"l1\" in reg) else None,\n",
    "                     )\n",
    "    \n",
    "    gamma_row = Embedding(1, row_w, name=\"gammas\",\n",
    "#                     embeddings_initializer=RandomNormal(0),\n",
    "                    )(zer0(psi_sel))\n",
    "    a0 = Embedding(n_students, 1, name=\"alphas\", \n",
    "                  embeddings_initializer=RandomNormal(0, stddev=0.1),\n",
    "                  embeddings_regularizer= L2(1.45e-05) if (\"l2\" in reg) else None,\n",
    "                  )(psi_sel)\n",
    "\n",
    "    etas = Flatten()( skill_d_ws(zer0(qn_sel)) ) \n",
    "    \n",
    "    q_mask = Flatten()( bin_clip(qn_mx(qn_sel)) )\n",
    "\n",
    "    gamma_row = Flatten()(gamma_row)\n",
    "    print(\"shape kc-gammas\", gamma_row.shape)\n",
    "    kc_practice = Dense(row_w, name=\"qk_loadings\", use_bias=False)(hit_counter)\n",
    "    print(\"shape kc-practice\", kc_practice.shape)\n",
    "    prac_modifier = Multiply()([kc_practice, gamma_row])\n",
    "\n",
    "    difs = subtract([prac_modifier, etas])\n",
    "    difs = Multiply()([difs, q_mask]) # mask off irrelevant KCs\n",
    "    \n",
    "    print(\"Difs shape\", difs.shape)\n",
    "    summed = Lambda(lambda ps: K.sum(ps, axis=1, keepdims=True), name=\"lambda_sums\")(difs)\n",
    "    print(\"Summed shape\", summed.shape)\n",
    "    \n",
    "    a0 = Flatten()(a0)\n",
    "    logit = Add()([a0, summed]) #in AFM we add a0 after the subtraction and masking\n",
    "        \n",
    "    print(\"logit shape\", logit.shape)\n",
    "    score = Lambda(lambda z: K.sigmoid(z))(logit)\n",
    "    \n",
    "    # p_LFA = Ïƒ(a_s + Î£ k âˆŠ skills(q): ðœ¸_k*n_sk - d_k)\n",
    "\n",
    "    model = Model(inputs=[qn_sel, psi_sel, hit_counter], outputs=score)\n",
    "    from keras.optimizers import SGD, RMSprop, Nadam, Adam\n",
    "    #   optr = SGD(learning_rate=1.0)#, momentum=0.01, nesterov=True)\n",
    "    #   optr = RMSprop()\n",
    "    optr = Adam()\n",
    "    model.compile(optimizer=optr, loss=loss, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def create_AFMg_orig_model(n_questions, n_students, row_w, loss=\"binary_crossentropy\", metrics=None, reg=None):\n",
    "    print(\"Using AFMg model!\")\n",
    "\n",
    "    from keras.initializers import RandomNormal, RandomUniform, Constant\n",
    "    from keras.layers import Add, Reshape, Multiply\n",
    "    from keras.constraints import NonNeg\n",
    "    from keras.optimizers import Adam\n",
    "    from keras.regularizers import L2\n",
    "    psi_sel = Input(shape=(1,), name=\"psi_select\", dtype=\"int32\")\n",
    "    qn_sel = Input(shape=(1,), name=\"q_select\", dtype=\"int32\")\n",
    "    hit_counter = Input(shape=(n_questions, ), name=\"hit_counter\", dtype=\"float32\")\n",
    "\n",
    "    zer0 = Lambda(lambda x: K.cast(K.clip(x,0,0), dtype=\"int32\"))\n",
    "    bin_clip = WeightClip(0.0000,1.0000)\n",
    "\n",
    "    q_row = Flatten()(bin_clip(Embedding(n_questions , row_w, name=\"qn_embedding\", \n",
    "                                         embeddings_initializer=RandomUniform(.99,1),\n",
    "                                         activity_regularizer= \"l1\" if (\"l1\" in reg) else None)(qn_sel)))\n",
    "\n",
    "    skill_d_ws = Embedding(1 , row_w, name=\"skill_diffs\")\n",
    "    base_deltas = Flatten()(skill_d_ws(zer0(qn_sel)))\n",
    "#     masked_deltas = multiply([q_row, base_deltas])\n",
    "#     delta = Lambda(lambda ps: K.sum(ps, axis=1, keepdims=True), name=\"sum_deltas\")(masked_deltas)\n",
    "\n",
    "    \n",
    "    gamma_row = Flatten()(Embedding(n_students, row_w, name=\"gammas\")(psi_sel))\n",
    "    \n",
    "    kc_practice = Dense(row_w, name=\"qk_loadings\", use_bias=False)(hit_counter)\n",
    "    prac_kc_mods = multiply([kc_practice, gamma_row])\n",
    "#     masked_kc_mods = multiply([prac_kc_mods, q_row])\n",
    "#     practice = Lambda(lambda ps: K.sum(ps, axis=1, keepdims=True), name=\"sum_kc_pracs\")(masked_kc_mods)\n",
    "    \n",
    "    kcwise_difs = subtract([prac_kc_mods, base_deltas])\n",
    "    masked_kcwise_difs = multiply([kcwise_difs, q_row])\n",
    "    dif = Lambda(lambda ps: K.sum(ps, axis=1, keepdims=True), name=\"sum_kc_pracs\")(masked_kcwise_difs)\n",
    "    \n",
    "    a0 = Flatten()(Embedding(n_students, 1, name=\"alphas\",          \n",
    "#                             embeddings_regularizer=L2(1/n_students) if (\"l2\" in reg) else None,\n",
    "                            embeddings_regularizer=L2(1.45e-05) if (\"l2\" in reg) else None,\n",
    "                            )(psi_sel))\n",
    "    logit_dif = add([a0, dif])\n",
    "\n",
    "    score = Lambda(lambda z: K.sigmoid(z))(logit_dif)\n",
    "    \n",
    "    model = Model(inputs=[qn_sel, psi_sel, hit_counter], outputs=score)\n",
    "    optr = Adam()\n",
    "    model.compile(optimizer=optr, loss=loss, metrics=metrics)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_AFMg_model(n_questions, n_students, row_w, loss=\"binary_crossentropy\", metrics=None, reg=None):\n",
    "    print(\"Using AFMg+ model!\")\n",
    "\n",
    "    from keras.initializers import RandomNormal, RandomUniform, Constant\n",
    "    from keras.layers import Add, Reshape, Multiply\n",
    "    from keras.constraints import NonNeg\n",
    "    from keras.optimizers import Adam\n",
    "    from keras.regularizers import L2\n",
    "    psi_sel = Input(shape=(1,), name=\"psi_select\", dtype=\"int32\")\n",
    "    qn_sel = Input(shape=(1,), name=\"q_select\", dtype=\"int32\")\n",
    "    hit_counter = Input(shape=(n_questions, ), name=\"hit_counter\", dtype=\"float32\")\n",
    "\n",
    "    zer0 = Lambda(lambda x: K.cast(K.clip(x,0,0), dtype=\"int32\"))\n",
    "    bin_clip = WeightClip(0.0000,1.0000)\n",
    "    pos_clip = WeightClip(0, math.inf)\n",
    "\n",
    "    sp = pr_to_spread(.5, row_w, as_A_and_D=False)\n",
    "    \n",
    "    q_row = Flatten()(bin_clip(Embedding(n_questions , row_w, name=\"qn_embedding\", \n",
    "                                         embeddings_initializer=RandomUniform(.99,1),\n",
    "                                        )(qn_sel)))\n",
    "#     if reg:\n",
    "    q_row = tensorflow.keras.layers.ActivityRegularization(l1=0.1/row_w)(q_row)\n",
    "\n",
    "    skill_d_ws = Embedding(1 , row_w, name=\"skill_diffs\", embeddings_initializer=RandomNormal(0))\n",
    "    base_deltas = Flatten()(skill_d_ws(zer0(qn_sel)))\n",
    "#     masked_deltas = multiply([q_row, base_deltas])\n",
    "#     delta = Lambda(lambda ps: K.sum(ps, axis=1, keepdims=True), name=\"sum_deltas\")(masked_deltas)\n",
    "\n",
    "    gamma_row = Flatten()(pos_clip(Embedding(n_students, row_w, name=\"gammas\", embeddings_initializer=RandomNormal(1))(psi_sel)))\n",
    "    \n",
    "    kc_practice = Dense(row_w, name=\"qk_loadings\", use_bias=False, kernel_constraint=NonNeg())(hit_counter)\n",
    "    prac_kc_mods = multiply([kc_practice, gamma_row])\n",
    "#     masked_kc_mods = multiply([prac_kc_mods, q_row])\n",
    "#     practice = Lambda(lambda ps: K.sum(ps, axis=1, keepdims=True), name=\"sum_kc_pracs\")(masked_kc_mods)\n",
    "    a0 = Flatten()(Embedding(n_students, row_w, name=\"alphas\",      \n",
    "                             embeddings_initializer=RandomNormal(sp),\n",
    "#                             embeddings_regularizer=L2(1/n_students) if (\"l2\" in reg) else None,\n",
    "#                              embeddings_regularizer=L2(1.45e-05) if (\"l2\" in reg) else None,\n",
    "                            )(psi_sel))\n",
    "    psi_row = add([a0, prac_kc_mods])\n",
    "    \n",
    "    kcwise_difs = subtract([psi_row, base_deltas])\n",
    "    Prs = Lambda(lambda z: K.sigmoid(z))(kcwise_difs)\n",
    "    \n",
    "#     masked_kcwise_difs = multiply([kcwise_difs, Pr])\n",
    "    Prs = Lambda(lambda ps: ps[0]*ps[1])([Prs, q_row])\n",
    "#     Prs = Lambda(lambda ps: K.pow(ps[0], ps[1]))([Prs, q_row])\n",
    "#     Prs = Lambda(lambda ps: ps[0]*ps[1] + (1-ps[1]) )([Prs, q_row])\n",
    "#     dif = Lambda(lambda ps: K.sum(ps, axis=1, keepdims=True), name=\"sum_kc_pracs\")(masked_kcwise_difs)\n",
    "  \n",
    "#     h = Dense(3, activation=\"relu\")(Prs)\n",
    "#     h = Dense(3, activation=\"relu\")(h)\n",
    "#     score = Dense(1, activation=\"sigmoid\")(h)\n",
    "\n",
    "    score = Lambda(lambda ps: K.prod(ps, axis=1, keepdims=True), name=\"score\")(Prs)\n",
    "\n",
    "    model = Model(inputs=[qn_sel, psi_sel, hit_counter], outputs=score)\n",
    "    optr = Adam()\n",
    "    model.compile(optimizer=optr, loss=loss, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RASCH_model(n_questions, n_students, loss=\"binary_crossentropy\", metrics=None, reg=None):\n",
    "    print(\"Using univariate Rasch model!\")\n",
    "\n",
    "    from keras.initializers import RandomNormal, RandomUniform, Constant\n",
    "    from keras.layers import Add, Reshape, Multiply\n",
    "    from keras.constraints import NonNeg\n",
    "    from keras.regularizers import L2\n",
    "    from keras.optimizers import Adam\n",
    "    psi_sel = Input(shape=(1,), name=\"psi_select\", dtype=\"int32\")\n",
    "    qn_sel = Input(shape=(1,), name=\"q_select\", dtype=\"int32\")\n",
    "    hit_counter = Input(shape=(n_questions, ), name=\"hit_counter\", dtype=\"float32\")\n",
    "\n",
    "#     if reg:\n",
    "#         rw = reg\n",
    "#     else:\n",
    "#         rw = 0.0001\n",
    "    \n",
    "    delta = Flatten()(Embedding(n_questions , 1, name=\"qn_embedding\")(qn_sel))\n",
    "    gamma_row = Flatten()(Embedding(n_students, 1, name=\"gammas\")(psi_sel))\n",
    "    a0 = Flatten()(Embedding(n_students, 1, \n",
    "#                              embeddings_regularizer = L2(rw),\n",
    "                             name=\"alphas\")(psi_sel))\n",
    "    \n",
    "#     log_hits_plus_one = hit_counter\n",
    "#     hcp1 = Lambda(lambda x: x)(hit_counter)\n",
    "#     kc_practice = Dense(10, use_bias=True)(hit_counter)\n",
    "    kc_practice = Dense(1, name=\"qk_loadings\", use_bias=False)(hit_counter)\n",
    "#     kc_practice = Lambda(lambda x: K.log(x+1))(kc_practice)\n",
    "\n",
    "    alpha = add([a0, multiply([kc_practice, gamma_row])])\n",
    "\n",
    "    logit_dif = subtract([alpha, delta])\n",
    "\n",
    "    score = Lambda(lambda z: K.sigmoid(z))(logit_dif)\n",
    "    \n",
    "\n",
    "    model = Model(inputs=[qn_sel, psi_sel, hit_counter], outputs=score)\n",
    "    optr = Adam()\n",
    "    model.compile(optimizer=optr, loss=loss, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_DEEPGAMMA_model(n_questions, n_students, row_w, ptqs=None, loss=f1_loss, non_neg=False,\n",
    "                       metrics=None, deep=False, concat=False, reg=None, reg_w=None):\n",
    "    print(\"MLP model\")\n",
    "    if reg==\"l2\":\n",
    "        print(\"L2 reg'n\", reg)\n",
    "    psi_sel = Input(shape=(1,), name=\"psi_select\", dtype=\"int32\")\n",
    "    qn_sel = Input(shape=(1,), name=\"q_select\", dtype=\"int32\")\n",
    "    hit_counter = Input(shape=(n_questions, ), name=\"hit_counter\", dtype=\"float32\")\n",
    "\n",
    "    sp = pr_to_spread(.5, row_w, as_A_and_D=False)\n",
    "\n",
    "#     q_init = RandomNormal(mean=0) if init50 else \"uniform\"\n",
    "    qn_emb = Embedding(n_questions, row_w, name=\"qn_embedding\", embeddings_initializer=RandomNormal(0))\n",
    "    qn_row = Flatten()(qn_emb(qn_sel))\n",
    "\n",
    "    l2_w = reg_w if (reg_w is not None) else 0.1\n",
    "    \n",
    "    from keras.regularizers import L2\n",
    "    #embeddings_initializer=RandomNormal(mean=1+sp)\n",
    "#     s_init = RandomNormal(mean=sp) #if init50 else \"uniform\"\n",
    "    gamma_row = Flatten()(Embedding(n_students, row_w, name=\"gammas\")(psi_sel))\n",
    "    alpha_row = Flatten()(Embedding(n_students, row_w, name=\"alphas\",\n",
    "                                embeddings_initializer=RandomNormal(sp),\n",
    "#                                 embeddings_regularizer=L2(l2_w/(n_students*row_w)) if reg==\"l2\" else None,\n",
    "#                                 embeddings_regularizer=L2(0.00001/(n_students*row_w)),# if reg==\"l2\" else None, \n",
    "                                )(psi_sel))\n",
    "#     alpha_row = tensorflow.keras.layers.ActivityRegularization(l2=0.01/row_w)(alpha_row)\n",
    "\n",
    "\n",
    "    kc_practice = Dense(row_w, name=\"qk_loadings\")(hit_counter)\n",
    "    learnage = Dense(row_w, activation=\"relu\")(concatenate([gamma_row, kc_practice]))\n",
    "#     learnage = Dense(row_w)(learnage)\n",
    "        \n",
    "#     kc_practice = Dense(row_w, name=\"qk_loadings\", use_bias=False, activation=\"relu\")(hit_counter)\n",
    "#     kc_practice = Lambda(lambda x: K.log(x+1))(kc_practice)\n",
    "#     log_hits_plus_one = Lambda(lambda x: K.log(x+1))(hit_counter)\n",
    "#     kc_practice = Dense(row_w, name=\"qk_loadings\", use_bias=False)(log_hits_plus_one)\n",
    "#     print(\"shape kc-practice\", kc_practice.shape)\n",
    "\n",
    "    psi_row = add( [alpha_row, learnage])\n",
    "#     psi_row = Dense(row_w, activation=\"relu\")(concatenate([alpha_row, learnage]))\n",
    "    \n",
    "    difs = subtract([psi_row, qn_row], name=\"difs\")\n",
    "#     difs = L2(0.001)(difs)\n",
    "    \n",
    "    Prs = difs\n",
    "#     Prs = Lambda(lambda z: K.sigmoid(z))(difs)\n",
    "    \n",
    "#     logs = Lambda(lambda ps: K.log(ps), name=\"log_step\")(Prs)\n",
    "#     summed_logs = Lambda(lambda ps: K.sum(ps, axis=-1, keepdims=True), name=\"sum_step\")(logs)\n",
    "#     score = Lambda(lambda ps: K.exp(ps), name=\"exp_step\")(summed_logs)\n",
    "\n",
    "#     score = Lambda(lambda prs: K.prod(prs, axis=-1, keepdims=True))(Prs)\n",
    "    Prs = Dense(5, activation=\"relu\")(Prs)\n",
    "    score = Dense(1, activation=\"sigmoid\")(Prs)\n",
    "\n",
    "    model = Model(inputs=[qn_sel, psi_sel, hit_counter], outputs=score)\n",
    "    \n",
    "    model.compile(optimizer=\"adam\", loss=loss, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_MLPraw_model(n_questions, n_students, row_w, ptqs=None, loss=f1_loss, non_neg=False,\n",
    "                       metrics=None, deep=False, concat=False, reg=None, reg_w=None, inc_dif=False):\n",
    "    print(\"MLP model\")\n",
    "    if reg==\"l2\":\n",
    "        print(\"L2 reg'n\", reg)\n",
    "    psi_sel = Input(shape=(1,), name=\"psi_select\", dtype=\"int32\")\n",
    "    qn_sel = Input(shape=(1,), name=\"q_select\", dtype=\"int32\")\n",
    "    hit_counter = Input(shape=(n_questions, ), name=\"hit_counter\", dtype=\"float32\")\n",
    "\n",
    "    qn_emb = Embedding(n_questions, row_w, name=\"qn_embedding\",\n",
    "#                        embeddings_initializer=RandomNormal(0)\n",
    "                      )\n",
    "    qn_row = Flatten()(qn_emb(qn_sel))\n",
    "#     qn_row = Dropout(0.5)(qn_row)\n",
    "\n",
    "    l2_w = reg_w if (reg_w is not None) else 0.1\n",
    "    \n",
    "    from keras.regularizers import L2\n",
    "    #embeddings_initializer=RandomNormal(mean=1+sp)\n",
    "#     s_init = RandomNormal(mean=sp) #if init50 else \"uniform\"\n",
    "    gamma_row = Flatten()(Embedding(n_students, row_w, name=\"gammas\")(psi_sel))\n",
    "    alpha_row = Flatten()(Embedding(n_students, row_w, name=\"alphas\",\n",
    "#                                 embeddings_initializer=RandomNormal(sp),\n",
    "#                                 embeddings_regularizer=L2(l2_w/(n_students*row_w)) if reg==\"l2\" else None,\n",
    "#                                 embeddings_regularizer=L2(5e-5/row_w),# if reg==\"l2\" else None, \n",
    "                                )(psi_sel))\n",
    "\n",
    "#     kch = Dropout(0.1)(hit_counter)\n",
    "    kch = hit_counter\n",
    "    kc_practice = Dense(row_w, name=\"qk_loadings\", use_bias=False)(kch)\n",
    "#     kc_practice = Dropout(0.5)(kc_practice)\n",
    "    print(\"shape kc-practice\", kc_practice.shape)\n",
    "    \n",
    "    psi_row = add( [alpha_row, multiply([kc_practice, gamma_row])])\n",
    "#     psi_row = Dropout(0.5)(psi_row)\n",
    "\n",
    "#     if deep:\n",
    "#         h = Dense(max(2,row_w//20), activation=\"relu\")(difs)\n",
    "# #         h = Dense(5, activation=\"relu\")(h)\n",
    "#     else:\n",
    "#     h = difs\n",
    "\n",
    "    hw = max(min(row_w,2),row_w//5)\n",
    "    print(\"hidden w is\", hw)\n",
    "#     h_act = \"sigmoid\"\n",
    "\n",
    "    difs = subtract([psi_row, qn_row], name=\"difs\")\n",
    "    if inc_dif==\"DenP\":\n",
    "#         ha = Dense(row_w)(psi_row)\n",
    "#         hd = Dense(row_w)(qn_row)\n",
    "#         h = concatenate([ha, hd], axis=1, name=\"concat_h\")\n",
    "        h = Dense(row_w)(difs)\n",
    "#         h = Dropout(0.75)(h)\n",
    "        score = Dense(1, activation=\"sigmoid\")(h)\n",
    "    elif inc_dif==\"Dense\":\n",
    "        ha = Dense(row_w)(psi_row)\n",
    "        hd = Dense(row_w)(qn_row)\n",
    "        h = concatenate([ha, hd], axis=1, name=\"concat_h\")\n",
    "#         h = Dropout(0.75)(h)\n",
    "        score = Dense(1, activation=\"sigmoid\")(h)\n",
    "    elif inc_dif==\"dif\":\n",
    "        h = difs\n",
    "#         h = Dense(hw, activation=h_act)(difs)\n",
    "        h = Dropout(0.75)(difs)\n",
    "#         h = Dense(hw, activation=h_act)(h)\n",
    "#         h = Dense(hw, activation=h_act)(h)\n",
    "        score = Dense(1, activation=\"sigmoid\")(h)\n",
    "    elif inc_dif==\"AD\":\n",
    "        del difs\n",
    "        h = concatenate([psi_row, qn_row], axis=1, name=\"concat_h\")\n",
    "        h = Dropout(0.75)(h)\n",
    "#         h = Dense(hw, activation=h_act)(h)\n",
    "#         h = Dense(hw, activation=h_act)(h)\n",
    "#         h = Dense(hw, activation=h_act)(h)\n",
    "        score = Dense(1, activation=\"sigmoid\")(h)\n",
    "    elif inc_dif==\"AD+dif\":\n",
    "#         difs = subtract([psi_row, qn_row], name=\"difs\")\n",
    "        h = concatenate([psi_row, qn_row, difs], axis=1, name=\"concat_h\")\n",
    "        h = Dropout(0.75)(h)\n",
    "#         h = Dense(hw, activation=h_act)(h)\n",
    "#         h = Dense(hw, activation=h_act)(h)\n",
    "#         h = Dense(hw, activation=h_act)(h)\n",
    "        score = Dense(1, activation=\"sigmoid\")(h)\n",
    "    else:\n",
    "        raise Exception(\"invalid concat model in MLPraw model gen:\", inc_dif)\n",
    "\n",
    "    model = Model(inputs=[qn_sel, psi_sel, hit_counter], outputs=score)\n",
    "    \n",
    "    model.compile(optimizer=\"adam\", loss=loss, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_MLP_model(n_questions, n_students, row_w, ptqs=None, loss=f1_loss, non_neg=False,\n",
    "                       metrics=None, deep=False, concat=False, reg=None, reg_w=None):\n",
    "    print(\"MLP model\")\n",
    "    if reg==\"l2\":\n",
    "        print(\"L2 reg'n\", reg)\n",
    "    psi_sel = Input(shape=(1,), name=\"psi_select\", dtype=\"int32\")\n",
    "    qn_sel = Input(shape=(1,), name=\"q_select\", dtype=\"int32\")\n",
    "    hit_counter = Input(shape=(n_questions, ), name=\"hit_counter\", dtype=\"float32\")\n",
    "\n",
    "    sp = pr_to_spread(.5, row_w, as_A_and_D=False)\n",
    "\n",
    "#     q_init = RandomNormal(mean=0) if init50 else \"uniform\"\n",
    "    qn_emb = Embedding(n_questions, row_w, name=\"qn_embedding\", embeddings_initializer=RandomNormal(0))\n",
    "    qn_row = Flatten()(qn_emb(qn_sel))\n",
    "\n",
    "    l2_w = reg_w if (reg_w is not None) else 0.1\n",
    "    \n",
    "    from keras.regularizers import L2\n",
    "    #embeddings_initializer=RandomNormal(mean=1+sp)\n",
    "#     s_init = RandomNormal(mean=sp) #if init50 else \"uniform\"\n",
    "    gamma_row = Flatten()(Embedding(n_students, row_w, name=\"gammas\")(psi_sel))\n",
    "    alpha_row = Flatten()(Embedding(n_students, row_w, name=\"alphas\",\n",
    "                                embeddings_initializer=RandomNormal(sp),\n",
    "#                                 embeddings_regularizer=L2(l2_w/(n_students*row_w)) if reg==\"l2\" else None,\n",
    "#                                 embeddings_regularizer=L2(5e-5/row_w),# if reg==\"l2\" else None, \n",
    "                                )(psi_sel))\n",
    "    alpha_row = tensorflow.keras.layers.ActivityRegularization(l2=0.01/row_w)(alpha_row)\n",
    "\n",
    "\n",
    "    kc_practice = Dense(row_w, name=\"qk_loadings\", use_bias=False)(hit_counter)\n",
    "\n",
    "#     kc_practice = Dense(row_w, name=\"qk_loadings\", use_bias=False, activation=\"relu\")(hit_counter)\n",
    "#     kc_practice = Lambda(lambda x: K.log(x+1))(kc_practice)\n",
    "#     log_hits_plus_one = Lambda(lambda x: K.log(x+1))(hit_counter)\n",
    "#     kc_practice = Dense(row_w, name=\"qk_loadings\", use_bias=False)(log_hits_plus_one)\n",
    "#     print(\"shape kc-practice\", kc_practice.shape)\n",
    "    \n",
    "    psi_row = add( [alpha_row, multiply([kc_practice, gamma_row])])\n",
    "\n",
    "    difs = subtract([psi_row, qn_row], name=\"difs\")\n",
    "#     difs = L2(0.001)(difs)\n",
    "    \n",
    "    score = Dense(1, activation=\"sigmoid\")(difs)\n",
    "\n",
    "    model = Model(inputs=[qn_sel, psi_sel, hit_counter], outputs=score)\n",
    "    \n",
    "    model.compile(optimizer=\"adam\", loss=loss, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_MLPd_model(n_questions, n_students, row_w, ptqs=None, loss=f1_loss, non_neg=False,\n",
    "                       metrics=None, deep=False, concat=False, reg=None):\n",
    "    print(\"MLP DEEP model\")\n",
    "    if reg==\"l2\":\n",
    "        print(\"L2 reg'n\")\n",
    "    psi_sel = Input(shape=(1,), name=\"psi_select\", dtype=\"int32\")\n",
    "    qn_sel = Input(shape=(1,), name=\"q_select\", dtype=\"int32\")\n",
    "    hit_counter = Input(shape=(n_questions, ), name=\"hit_counter\", dtype=\"float32\")\n",
    "\n",
    "#     sp = pr_to_spread(.5, row_w, as_A_and_D=False)\n",
    "\n",
    "#     long_clip = WeightClip(-100,100)\n",
    "\n",
    "#     q_init = RandomNormal(mean=0) if init50 else \"uniform\"\n",
    "    qn_emb = Embedding(n_questions, row_w, name=\"qn_embedding\")\n",
    "    qn_row = Flatten()(qn_emb(qn_sel))\n",
    "\n",
    "    from keras.regularizers import L2\n",
    "    from keras.layers import Dropout\n",
    "    gamma_row = Flatten()(Embedding(n_students, row_w, name=\"gammas\")(psi_sel))\n",
    "    alpha_row = Flatten()(Embedding(n_students, row_w, name=\"alphas\",\n",
    "                                   embeddings_initializer=RandomNormal(1),\n",
    "                                   embeddings_regularizer=L2(1.0/(n_students*row_w)) if reg==\"l2\" else None )(psi_sel))\n",
    "\n",
    "    kc_practice = Dense(row_w, name=\"qk_loadings\")(hit_counter)\n",
    "    print(\"shape kc-practice\", kc_practice.shape)\n",
    "    \n",
    "    psi_row = add( [alpha_row, multiply([kc_practice, gamma_row])])\n",
    "\n",
    "    difs = subtract([psi_row, qn_row], name=\"difs\")\n",
    "    h = Dense(row_w/2, activation=\"relu\")(difs)\n",
    "#     h = Dropout(0.2)(h)\n",
    "    h = Dense(4, activation=\"linear\")(h)\n",
    "#     h = Dropout(0.2)(h)\n",
    "    score = Dense(1, activation=\"sigmoid\")(h)\n",
    "\n",
    "    model = Model(inputs=[qn_sel, psi_sel, hit_counter], outputs=score)\n",
    "    \n",
    "    model.compile(optimizer=\"adam\", loss=loss, metrics=metrics)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_CONC_model(n_questions, n_students, row_w, ptqs=None, loss=f1_loss, non_neg=False,\n",
    "                       metrics=None, deep=False, concat=False, reg=None):\n",
    "    print(\"MLP CONC model\")\n",
    "    if reg==\"l2\":\n",
    "        print(\"L2 reg'n\")\n",
    "    psi_sel = Input(shape=(1,), name=\"psi_select\", dtype=\"int32\")\n",
    "    qn_sel = Input(shape=(1,), name=\"q_select\", dtype=\"int32\")\n",
    "    hit_counter = Input(shape=(n_questions, ), name=\"hit_counter\", dtype=\"float32\")\n",
    "\n",
    "#     sp = pr_to_spread(.5, row_w, as_A_and_D=False)\n",
    "\n",
    "    long_clip = WeightClip(-100,100)\n",
    "\n",
    "#     q_init = RandomNormal(mean=0) if init50 else \"uniform\"\n",
    "    qn_emb = Embedding(n_questions, row_w, name=\"qn_embedding\")\n",
    "    qn_row = Flatten()(long_clip(qn_emb(qn_sel)))\n",
    "\n",
    "    from keras.regularizers import L2\n",
    "    from keras.layers import Dropout, concatenate\n",
    "    gamma_row = Flatten()(Embedding(n_students, row_w, name=\"gammas\")(psi_sel))\n",
    "    alpha_row = Flatten()(Embedding(n_students, row_w, name=\"alphas\",\n",
    "                                   embeddings_initializer=RandomNormal(1),\n",
    "                                   embeddings_regularizer=L2(1/(n_students*row_w)) if reg==\"l2\" else None )(psi_sel))\n",
    "\n",
    "    kc_practice = Dense(row_w, name=\"qk_loadings\")(hit_counter)\n",
    "    print(\"shape kc-practice\", kc_practice.shape)\n",
    "    \n",
    "    psi_row = add( [alpha_row, multiply([kc_practice, gamma_row])])\n",
    "\n",
    "    conc = concatenate([psi_row, qn_row], name=\"conc\")\n",
    "#     h = Dense(10, activation=\"relu\")(conc)\n",
    "#     h = Dropout(0.2)(h)\n",
    "#     h = Dense(4, activation=\"relu\")(h)\n",
    "#     h = Dropout(0.2)(h)\n",
    "    score = Dense(1, activation=\"sigmoid\")(conc)\n",
    "\n",
    "    model = Model(inputs=[qn_sel, psi_sel, hit_counter], outputs=score)\n",
    "    \n",
    "    model.compile(optimizer=\"adam\", loss=loss, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from types import ModuleType, FunctionType\n",
    "from gc import get_referents\n",
    "\n",
    "# Custom objects know their class.\n",
    "# Function objects seem to know way too much, including modules.\n",
    "# Exclude modules as well.\n",
    "BLACKLIST = type, ModuleType, FunctionType\n",
    "\n",
    "\n",
    "def getsize(obj):\n",
    "    \"\"\"sum size of object & members.\"\"\"\n",
    "    if isinstance(obj, BLACKLIST):\n",
    "        raise TypeError('getsize() does not take argument of type: '+ str(type(obj)))\n",
    "    seen_ids = set()\n",
    "    size = 0\n",
    "    objects = [obj]\n",
    "    while objects:\n",
    "        need_referents = []\n",
    "        for obj in objects:\n",
    "            if not isinstance(obj, BLACKLIST) and id(obj) not in seen_ids:\n",
    "                seen_ids.add(id(obj))\n",
    "                size += sys.getsizeof(obj)\n",
    "                need_referents.append(obj)\n",
    "        objects = get_referents(*need_referents)\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "FdR1WiKecC8g",
    "outputId": "447bf9bb-72bb-4e31-e3f2-2667c0c0ae88",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset 5 : [[1.4645131]] 3.1089039466440367 1.4084517039946103\n",
      "[[ 1.37173388 -1.02864352  7.55703414 ... -1.49886535  1.53442336\n",
      "   1.55596703]\n",
      " [ 2.4635658   0.53089028 -5.45221188 ... -0.74608016  2.82944632\n",
      "   1.92081998]\n",
      " [ 1.55326451 -3.58004045 -1.89803685 ... -0.54779934  1.07594026\n",
      "  -1.57406956]\n",
      " ...\n",
      " [ 2.1023064   2.8166152  -0.4222581  ...  2.81755714 -6.81089099\n",
      "  -5.47685088]\n",
      " [-3.94892705 -1.47331223  1.72161864 ...  5.54785844  0.01453553\n",
      "  -1.07487148]\n",
      " [-2.68058906  0.55106707  0.27516104 ... -0.01492254  0.32854899\n",
      "  -0.09848295]]\n",
      "[[-10.         -10.         -10.         ... -10.         -10.\n",
      "  -10.        ]\n",
      " [-10.         -10.          -2.23522303 ... -10.         -10.\n",
      "  -10.        ]\n",
      " [-10.         -10.         -10.         ... -10.         -10.\n",
      "  -10.        ]\n",
      " ...\n",
      " [-10.         -10.         -10.         ... -10.         -10.\n",
      "  -10.        ]\n",
      " [-10.         -10.         -10.         ... -10.         -10.\n",
      "  -10.        ]\n",
      " [-10.         -10.         -10.         ... -10.         -10.\n",
      "  -10.        ]]\n"
     ]
    }
   ],
   "source": [
    "emb_w = 2\n",
    "import gc, zlib\n",
    "\n",
    "from collections import Counter\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping, LambdaCallback\n",
    "data_to_run = [5,]\n",
    "c= Counter()\n",
    "for a in data_to_run:\n",
    "    (tw,a0,a1, students_temp, qz_temp) = pickle.load(open(home+\"/synth_data/MLTM_10000_1000_(100_1_5)_{}.p\".format(a), \"rb\"))\n",
    "    print(\"loaded dataset\",a,\":\", a0,a1,tw)             \n",
    "    \n",
    "    n_students = 1000 # len(students2)\n",
    "    n_questions = 1000 #len(questions)\n",
    "    spars = 0.1\n",
    "\n",
    "    # from keras.models import load_model\n",
    "    fname = \"MLTM_1000_1000_(100_1_5)_sp100_5_0\"\n",
    "    #   m = load_model(home+\"/models/\" + fname, custom_objects={'WeightClip': WeightClip})\n",
    "    \n",
    "    print(students_temp)\n",
    "    print(qz_temp)\n",
    "    \n",
    "    \n",
    "#     c[a] += 1\n",
    "\n",
    "#     n_students = 500 # len(students2)\n",
    "#     n_questions = 500 #len(questions)\n",
    "#     students2 = students_temp[0:n_students]\n",
    "#     questions = qz_temp[0:n_questions]\n",
    "\n",
    "#     curr_min = numpy.min(students2)\n",
    "#     start = curr_min - .1\n",
    "\n",
    "#     #   pre_trained_qns = m.get_weights()[1]\n",
    "#     #   print(pre_trained_qns.shape)\n",
    "#     m = None\n",
    "\n",
    "#     print(0.01**(1/3))\n",
    "#     # p = 1/(1+e-z)\n",
    "#     print(pr_to_spread(0.215, as_A_and_D=False))#  -1.295\n",
    "#     # print(a0)\n",
    "\n",
    "#     longitude = 12 # number of steps we assume students have been in play\n",
    "#     # for s in students2:\n",
    "#     #   print((s-start)/longitude)\n",
    "\n",
    "#     students_start = numpy.zeros_like(students2) + start\n",
    "#     # gammas = (students2 - start)/longitude\n",
    "#     gammas = numpy.random.uniform(low=0.01, high=2.4, size=(n_students, emb_w))\n",
    "#     print(\"Gammas:\", numpy.min(gammas), numpy.max(gammas), numpy.mean(gammas), numpy.median(gammas))\n",
    "\n",
    "#     fname = \"Longitudinal_{}_{}_(100_1_5)_run={}\".format(n_students, n_questions, a)\n",
    "#     # try:\n",
    "\n",
    "#     # try:\n",
    "#     #   o_hits\n",
    "#     # except NameError:\n",
    "# #     if not o_hits:\n",
    "# #     (sixs, qixs, hits, hout) = pickle.load(open(home+\"/synth_data/\" + fname + \".p\", \"rb\"))\n",
    "# #     (sixs, qixs, hout) = pickle.load(open(home+\"/real_data/XL1041.p\", \"rb\"))\n",
    "#     # render_student_histories(sixs, qixs, hits, hout)\n",
    "#     questions = None\n",
    "#     students2 = None\n",
    "#     students_temp = None\n",
    "#     qz_temp = None\n",
    "\n",
    "#     # pre_trained_qns = None\n",
    "#     gc.collect()\n",
    "#     o_hits=[]\n",
    "#     t_hits=[]\n",
    "\n",
    "#     odata, vdata, tdata, sid_six_lookup, qid_qix_lookup = split_next_step(sixs, qixs, hout, \n",
    "#                                                                           max_students = 100000,\n",
    "#                                                                           min_hist = 40,\n",
    "#                                                                           max_hist = None)#, alternate=True, balance_training=False)\n",
    "#     (o_sixs, o_qixs, o_hits, o_out), (v_sixs, v_qixs, v_hits, v_out), (t_sixs, t_qixs, t_hits, t_out) = (odata, vdata, tdata)\n",
    "#     print(\"len o_sixes\", len(o_sixs))\n",
    "\n",
    "#     n_questions = len(set(qixs))\n",
    "#     n_students = len(set(sixs))\n",
    "\n",
    "#     # raise Exception(\"DELIBERATE EXCEPTION CALLED\")\n",
    "\n",
    "# #     def uncomp(chits):\n",
    "# #         for hix, chrow in enumerate(chits):\n",
    "# #             chits[hix] = pickle.loads(zlib.decompress(chrow))\n",
    "# #         chits = numpy.array(chits, dtype=\"uint8\")\n",
    "# #         return chits\n",
    "\n",
    "# #     print(\"uncomping\")\n",
    "# #     o_hits = uncomp(o_chits)\n",
    "# #     v_hits = uncomp(v_chits)\n",
    "# #     t_hits = uncomp(t_chits)\n",
    "# #     print(\"straight outta comp-ton\")\n",
    "\n",
    "#   # o_chits, v_chits, t_chits = None, None, None\n",
    "\n",
    "#   # except:\n",
    "#   # # if True:\n",
    "#   #   av_sc, sixs, qixs, hits, hout = run_data(students_start, questions, gammas, model_to_train=None)\n",
    "#   #   #   if s > 100:\n",
    "#   #   #     break\n",
    "#   #     # print(h)\n",
    "#   #     # print(r)\n",
    "#   #     # print(\"***\")\n",
    "\n",
    "#   #   gc.collect()\n",
    "#   #   sixs = numpy.array(sixs, dtype=\"uint16\")\n",
    "#   #   qixs = numpy.array(qixs, dtype=\"uint16\")\n",
    "#   #   hits = numpy.array(hits, dtype=\"uint8\")\n",
    "#   #   for hix, hrow in enumerate(hits):\n",
    "#   #     compd = zlib.compress(pickle.dumps(hrow))\n",
    "#   #     chits[hix] = compd\n",
    "#   #   hout = numpy.array(hout, dtype=\"uint8\")\n",
    "#   #   pickle.dump((sixs, qixs, chits, hout), open(home+\"/synth_data/\" + fname + \".p\", \"wb\"))\n",
    "#   #   chits = None\n",
    "\n",
    "#   # for h in hits[0:10]:\n",
    "#   #     print(h)\n",
    "\n",
    "#   # raise Exception(\"GAR\")\n",
    "\n",
    "#   # for s,q,h,r in zip(sixs, qixs, hits, hout):\n",
    "#   #   print(s,q, r)\n",
    "#   # print(hout)\n",
    "#   # print(int(sum(hout)))\n",
    "#   # print(len(hout))\n",
    "#   # raise Exception(\"DELIBERATE EXCEPTION CALLED\")\n",
    "\n",
    "#   # qlayer = m.get_layer(\"qn_embedding\")\n",
    "#   # print(qlayer.shape)\n",
    "#   # m.get_layer(\"qn_embedding\").set_weights(pre_trained_qns)\n",
    "#   # m.get_layer(\"qn_embedding\").trainable=False\n",
    "\n",
    "\n",
    "#   # hin2 = hin #numpy.array(hin).reshape(-1,(1,1,n_questions))\n",
    "#   # hout2 = numpy.array(hout).reshape(-1,1)\n",
    "\n",
    "#     n_to_keep = 10000\n",
    "\n",
    "#   # qixs = qixs[0:n_to_keep]\n",
    "#   # sixs = sixs[0:n_to_keep]  \n",
    "#   # hits = hits[0:n_to_keep]\n",
    "#   # hout = hout[0:n_to_keep]\n",
    "\n",
    "#   # n_questions = len(numpy.unique(o_qixs))\n",
    "#   # n_students  = len(numpy.unique(o_sixs))\n",
    "\n",
    "\n",
    "\n",
    "#   # n_questions = max(max(o_qixs),max(v_qixs),max(t_qixs))+1\n",
    "#   # n_students = max(max(o_sixs),max(v_sixs),max(t_sixs))+1\n",
    "\n",
    "\n",
    "\n",
    "#   # m = generate_longitudinal_model(n_questions, n_students, emb_w, None) #pre_trained_qns[0:n_questions])\n",
    "\n",
    "#     n_to_test = 1000 #len(hout)//10\n",
    "#     test_choices = numpy.random.choice(range(len(hout)), size=n_to_test)\n",
    "#   # t_in = hin2[test_choices, :]\n",
    "\n",
    "#   # t_hits = hits[test_choices]\n",
    "#   # o_hits = numpy.delete(hits, test_choices, axis=0)\n",
    "\n",
    "#   # qs_in_trimmed_data =  numpy.unique(qixs)\n",
    "#   # t_hits = numpy.array([hits[ix] for ix in test_choices])\n",
    "#   # o_hits = numpy.delete(hits, test_choices, axis=0)\n",
    "#   # hits=None\n",
    "#   # t_hits = t_hits[:, qs_in_trimmed_data]\n",
    "#   # t_hits = t_hits.reshape(-1, len(qs_in_trimmed_data))\n",
    "#   # o_hits = o_hits[:, qs_in_trimmed_data]\n",
    "#   # o_hits = o_hits.reshape(-1, len(qs_in_trimmed_data))\n",
    "\n",
    "#     gc.collect()\n",
    "\n",
    "#   # t_out = hout[test_choices, :]\n",
    "#   # t_out = numpy.array([hout[ix] for ix in test_choices]).reshape(-1,1)\n",
    "#   # o_out = numpy.delete(hout, test_choices, axis=0).reshape(-1,1)\n",
    "#   # hout=None\n",
    "\n",
    "\n",
    "#   # t_sixs = numpy.array([sixs[ix] for ix in test_choices]).reshape(-1,1)\n",
    "#   # # t_sixs = sixs[test_choices, :]\n",
    "#   # o_sixs = numpy.delete(sixs, test_choices, axis=0).reshape(-1,1)\n",
    "#   # sixs=None\n",
    "\n",
    "#   # t_qixs = numpy.array([qixs[ix] for ix in test_choices]).reshape(-1,1)\n",
    "#   # # t_qixs = qixs[test_choices, :]\n",
    "#   # o_qixs = numpy.delete(qixs, test_choices, axis=0).reshape(-1,1)\n",
    "\n",
    "#     print(\"MM\", numpy.min(o_qixs), numpy.max(o_qixs))\n",
    "#     qixs=None\n",
    "  \n",
    "#   # o_in  = numpy.delete(hin2, test_choices, axis=0)\n",
    "  \n",
    "#   # hazard_model = Model(inputs=m.input,\n",
    "#   #                         outputs=m.get_layer(\"alphas\").output)\n",
    "# #         intermediate_output = intermediate_layer_model.predict([qz,sz])\n",
    "       \n",
    "    \n",
    "    \n",
    "# #         print_prs = LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "# # #                                        print(numpy.min(intermediate_layer_model.predict([qz,sz])),\n",
    "# # #                                              numpy.max(intermediate_layer_model.predict([qz,sz]))))\n",
    "# #                                        print(prs_model.predict([qz[0:10],sz[0:10]])))\n",
    "\n",
    "#   # ixs = o_out > 0.5\n",
    "#     ixs = [True if o_out[n]>=0.5 else False for n in range(len(o_out))]\n",
    "#   # print(ixs)\n",
    "#   # print(o_out[ixs])\n",
    "#   # print(o_qixs[ixs])\n",
    "#   # print(o_sixs[ixs])\n",
    "#   # print(o_hits[ixs])\n",
    "#     print_hazard = LambdaCallback(on_epoch_end=lambda batch, logs:\n",
    "#                                   # print(hazard_model.predict([o_qixs[ixs], o_sixs[ixs], o_hits[ixs]])))\n",
    "#                                   print(m.predict([o_qixs[ixs], o_sixs[ixs], o_hits[ixs]]), o_out[ixs]))\n",
    "\n",
    "#         # print_zmask = LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "#         #                                print(z_model.predict([qz[0:10],sz[0:10]])))\n",
    "\n",
    "#   # n_ones  = int(numpy.sum(o_out, axis=0))\n",
    "#   # n_zeros = len(o_out) - n_ones\n",
    "\n",
    "#   # n_ones =  sum([1 if o_out[n]>=0.5 else 0 for n in range(len(o_out))])\n",
    "#   # n_zeros = sum([1 if o_out[n]<0.5 else 0 for n in range(len(o_out))])\n",
    "\n",
    "# #   geschichte = m.fit([o_qixs, o_sixs, o_hits], o_out, epochs=10000, validation_split=0.01, callbacks=[es], shuffle=True, class_weight=class_weightz)\n",
    "\n",
    "# print(type(o_sixs))\n",
    "# print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "iOhg-f9pbesM",
    "outputId": "10322f14-0681-4aad-e50f-cce06b8e3c60",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.metrics import binary_accuracy\n",
    "def gen_and_train(odata, vdata, t_data, emb_w=10, draw=False, cog_model=\"MLP\", q_weight=None, \n",
    "                  monitor=None, balance_classes=True, \n",
    "                  loss=None, metrics=None, reg_w=None):\n",
    "    from keras.callbacks import EarlyStopping, LambdaCallback\n",
    "\n",
    "    reg=\"\"\n",
    "    \n",
    "    (o_sixs, o_qixs, o_hits, o_out), (v_sixs, v_qixs, v_hits, v_out) = (odata, vdata)\n",
    "    \n",
    "#     emb_w = 4\n",
    "    m = None\n",
    "    \n",
    "    config_dict = {}    \n",
    "    config_dict[\"cog_model\"] = cog_model\n",
    "    config_dict[\"emb_w\"]     = emb_w\n",
    "    \n",
    "#     n_ones = int(sum(numpy.ravel(numpy.round(o_out))))\n",
    "#     n_zeros = len(o_out) - n_ones\n",
    "#     print(len(o_out))\n",
    "#     print(\"1s:\", n_ones)\n",
    "#     print(\"0s:\", n_zeros)\n",
    "\n",
    "# #     # minority_w = .75\n",
    "#     if n_zeros > n_ones:\n",
    "#         zero_w = 1.0\n",
    "#         one_w = n_zeros/n_ones\n",
    "#     else:\n",
    "#         zero_w = n_ones/n_zeros\n",
    "#         one_w = 1.0\n",
    "\n",
    "#     class_weightz0 = {\n",
    "#         0: zero_w,\n",
    "#         1: one_w,\n",
    "#     }\n",
    "    \n",
    "#     print(\"cw0\")\n",
    "#     print(class_weightz0)\n",
    "\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    class_weightz = compute_class_weight(\"balanced\", [0,1], o_out)\n",
    "    class_weightz = class_weightz / min(class_weightz)\n",
    "    print(\"class weights:\", class_weightz)\n",
    "    cw0 = {}\n",
    "    for cwix,cw in enumerate(class_weightz):\n",
    "        cw0[cwix] = cw\n",
    "    class_weightz = cw0\n",
    "    print(\"class weights (dict):\", class_weightz)\n",
    "    \n",
    "    # o_out = o_out.reshape(-1,1)\n",
    "    o_out = o_out.astype(float)\n",
    "\n",
    "    n_questions = max(max(o_qixs),max(v_qixs),max(t_qixs))+1\n",
    "    n_students = max(max(o_sixs),max(v_sixs),max(t_sixs))+1\n",
    "\n",
    "#     n_questions = int(o_hits.shape[1])\n",
    "#     n_students = int(max(o_sixs)+1)\n",
    "    print(\"nq, ns\")\n",
    "    print(n_questions, n_students)\n",
    "    \n",
    "    if metrics is None:\n",
    "        metrics = [binary_crossentropy, binary_accuracy, mean_absolute_error, mean_squared_error, f1_loss]\n",
    "#     lozz = \"binary_crossentropy\"\n",
    "    if loss==\"f1_loss\":\n",
    "        loss = f1_loss \n",
    "#     elif loss ==\"f1_loss_micro\":\n",
    "#         loss = f1_loss_micro\n",
    "    if cog_model==\"MLPraw\":\n",
    "        m = generate_MLPraw_model(n_questions, n_students, emb_w, ptqs=None, loss=loss, non_neg=False, \n",
    "                                   metrics=metrics, deep=False, reg=reg, reg_w=reg_w, inc_dif=\"dif\")\n",
    "    elif cog_model==\"MLPrawAD\":\n",
    "        m = generate_MLPraw_model(n_questions, n_students, emb_w, ptqs=None, loss=loss, non_neg=False, \n",
    "                                   metrics=metrics, deep=False, reg=reg, reg_w=reg_w, inc_dif=\"AD\")\n",
    "    elif cog_model==\"MLPrawADD\":\n",
    "        m = generate_MLPraw_model(n_questions, n_students, emb_w, ptqs=None, loss=loss, non_neg=False, \n",
    "                                   metrics=metrics, deep=False, reg=reg, reg_w=reg_w, inc_dif=\"AD+dif\")\n",
    "    elif cog_model==\"MLPrawDen\":\n",
    "        m = generate_MLPraw_model(n_questions, n_students, emb_w, ptqs=None, loss=loss, non_neg=False, \n",
    "                               metrics=metrics, deep=False, reg=reg, reg_w=reg_w, inc_dif=\"Dense\")\n",
    "    elif cog_model==\"MLPrawDP\":\n",
    "        m = generate_MLPraw_model(n_questions, n_students, emb_w, ptqs=None, loss=loss, non_neg=False, \n",
    "                               metrics=metrics, deep=False, reg=reg, reg_w=reg_w, inc_dif=\"DenP\")\n",
    "    elif cog_model==\"DEEPGAMMA\":\n",
    "        m = generate_DEEPGAMMA_model(n_questions, n_students, emb_w, ptqs=None, loss=loss, non_neg=False, \n",
    "                               metrics=metrics, deep=False, reg=reg, reg_w=reg_w)\n",
    "    elif cog_model[0:3]==\"CFM\":\n",
    "        sfoc = False\n",
    "        reg = \"\"\n",
    "        rw  = None\n",
    "        if len(cog_model)==4:\n",
    "            if cog_model[3] == \"z\":\n",
    "                reg = \"l2\"\n",
    "                rw = reg_w\n",
    "            elif cog_model[3] == \"l\":\n",
    "                reg = \"l1\"\n",
    "                rw = reg_w\n",
    "            else:\n",
    "                reg = \"\"\n",
    "                rw  = None\n",
    "        m = create_CFM_model(n_questions, n_students, emb_w, loss=loss, sfocus=sfoc, metrics=metrics, reg=reg, reg_w=rw)\n",
    "#     elif cog_model[0:5]==\"AFMsg\":\n",
    "#         m = create_AFMsg_model(n_questions, n_students, emb_w, loss=loss, metrics=metrics)\n",
    "    elif cog_model==\"RASCHz\":\n",
    "        rw = 5e-8\n",
    "        m = create_RASCH_model(n_questions, n_students, loss=loss, metrics=metrics, reg=rw)\n",
    "    elif cog_model==\"RASCH\":\n",
    "        m = create_RASCH_model(n_questions, n_students, loss=loss, metrics=metrics)\n",
    "    elif cog_model.startswith(\"AFMx\"):\n",
    "        if len(cog_model)==5:\n",
    "            if cog_model[4]==\"z\":\n",
    "                reg=\"l2\"\n",
    "            elif cog_model[4]==\"l\":\n",
    "                reg=\"l1\"\n",
    "            else:\n",
    "                reg=None\n",
    "        m = create_AFMx_model(n_questions, n_students, emb_w, loss=loss, metrics=metrics, reg=reg)\n",
    "    elif cog_model.startswith(\"AFMg\"):\n",
    "        if len(cog_model)==5:\n",
    "            if cog_model[4]==\"z\":\n",
    "                reg=\"l2\"\n",
    "            elif cog_model[4]==\"l\":\n",
    "                reg=\"l1\"\n",
    "            else:\n",
    "                reg=None\n",
    "        m = create_AFMg_model(n_questions, n_students, emb_w, loss=loss, metrics=metrics, reg=reg)\n",
    "    elif cog_model.startswith(\"AFM\"):\n",
    "        if len(cog_model)==4:\n",
    "            if cog_model[3]==\"z\":\n",
    "                reg=\"l2\"\n",
    "            elif cog_model[3]==\"l\":\n",
    "                reg=\"l1\"\n",
    "            else:\n",
    "                reg=None\n",
    "        m = create_AFM_model(n_questions, n_students, emb_w, loss=loss, sfocus=False, metrics=metrics, reg=reg)\n",
    "\n",
    "    elif cog_model==\"MLTM0z\":\n",
    "        m = generate_MLTM0_model(n_questions, n_students, emb_w, loss=loss, metrics=metrics, l2=True)\n",
    "    elif cog_model==\"MLTM0\":\n",
    "        m = generate_MLTM0_model(n_questions, n_students, emb_w, loss=loss, metrics=metrics, l2=False)\n",
    "    elif cog_model==\"MLTMz\":\n",
    "        m = generate_MLTM_raw_model(n_questions, n_students, emb_w, loss=loss, metrics=metrics, reg=\"l2\", reg_w=reg_w)\n",
    "    elif cog_model==\"MLTMl\":\n",
    "        m = generate_MLTM_raw_model(n_questions, n_students, emb_w, loss=loss, metrics=metrics, reg=\"l1\")\n",
    "    elif cog_model==\"MLTMp\":\n",
    "        print(\"TARTOLA\")\n",
    "        m = generate_MLTMp_model(n_questions, n_students, emb_w, loss=loss, metrics=metrics)    \n",
    "    elif cog_model==\"MLTMa\":\n",
    "        m = generate_MLTMa_model(n_questions, n_students, emb_w, loss=loss, metrics=metrics)\n",
    "    elif cog_model.startswith(\"MLTMb\"):\n",
    "        print(\"KOG MOD\", cog_model)\n",
    "        print(cog_model[-1])\n",
    "        reg = \"l2\" if cog_model[-1]=='z' else (\"l1\" if cog_model[-1]=='l' else None)\n",
    "        print(\"REGG:\", reg)\n",
    "        m = generate_MLTMb_model(n_questions, n_students, emb_w, loss=loss, metrics=metrics, \n",
    "                                 reg=reg, reg_w=reg_w)\n",
    "    elif cog_model==\"MLTM\":\n",
    "        print(\"WAnkOPHONE\")\n",
    "        print(\"RAW w init50\")\n",
    "        m = generate_MLTM_raw_model(n_questions, n_students, emb_w, ptqs=None, loss=loss, metrics=metrics, \n",
    "                                    init50=True, reg=None)\n",
    "    elif cog_model.startswith(\"CONC\"):\n",
    "        if cog_model[-1]==\"z\":\n",
    "            reg=\"l2\"\n",
    "        else:\n",
    "            reg=None\n",
    "        m = generate_CONC_model(n_questions, n_students, emb_w, ptqs=None, loss=loss, non_neg=False, metrics=metrics, reg=reg)\n",
    "    elif cog_model.startswith(\"MLP\"):\n",
    "        if cog_model[-1]==\"z\":\n",
    "            reg=\"l2\"\n",
    "        else:\n",
    "            reg=None\n",
    "        if cog_model.startswith(\"MLPd\"):\n",
    "            m = generate_MLPd_model(n_questions, n_students, emb_w, ptqs=None, loss=loss, non_neg=False, metrics=metrics, reg=reg)\n",
    "#         elif cog_model.startswith(\"MLPs\"):\n",
    "#             print(\"MLPS MODELSSQQQ\")\n",
    "#             m = generate_MLP2_model(n_questions, n_students, emb_w, ptqs=None, loss=loss, non_neg=False, metrics=metrics, reg=reg)\n",
    "        else:\n",
    "            m = generate_MLP_model(n_questions, n_students, emb_w, ptqs=None, loss=loss, non_neg=False, \n",
    "                                   metrics=metrics, deep=False, reg=reg, reg_w=reg_w)\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"Unknown cognitive model {}\".format(cog_model))\n",
    "\n",
    "    # t_hits = t_hits[:, 0:n_questions]\n",
    "\n",
    "    print(\"TRAINING:\")\n",
    "    print(\"Unique students:\", n_students)\n",
    "    print(\"Unique questions:\", n_questions)\n",
    "    print(\"Total activity:\", len(o_out), \"(\",numpy.sum(o_out),\")\")\n",
    "\n",
    "    print(\"ov shape\", o_hits.shape, v_hits.shape)\n",
    "\n",
    "    if monitor==\"val_accuracy\":\n",
    "        mon_mode = \"max\"\n",
    "        bl = -math.inf\n",
    "    elif monitor ==\"val_f1_loss\":\n",
    "        mon_mode = \"min\"\n",
    "        bl = math.inf\n",
    "    elif monitor[-5:] == \"error\":\n",
    "        mon_mode = \"min\"\n",
    "        bl = math.inf\n",
    "    elif monitor[-4:] ==\"loss\":\n",
    "        mon_mode = \"min\"\n",
    "        bl = math.inf\n",
    "    elif monitor == \"binary_crossentropy\":\n",
    "        mon_mode = \"min\"\n",
    "        bl = math.inf\n",
    "        \n",
    "    print(\"monitoring info\", monitor, mon_mode)\n",
    "#     es = EarlyStopping(monitor=\"val_f1_loss\", patience=10, restore_best_weights=True, baseline=math.inf)    \n",
    "    es = EarlyStopping(monitor=monitor, patience=15, restore_best_weights=True, mode=mon_mode, baseline=bl)\n",
    "    \n",
    "#     es = EarlyStopping(monitor=\"val_accuracy\", patience=10, restore_best_weights=True, baseline=-math.inf)\n",
    "#     es = EarlyStopping(monitor=\"val_f1_m\", patience=10, restore_best_weights=True, mode=\"max\")\n",
    "\n",
    "#     print(\"check assertions\")\n",
    "#     assert not numpy.any(numpy.isnan(o_qixs))\n",
    "#     assert not numpy.any(numpy.isnan(o_sixs))\n",
    "#     assert not numpy.any(numpy.isnan(o_hits))\n",
    "#     assert not numpy.any(numpy.isnan(o_out))\n",
    "\n",
    "    bs = len(o_out)\n",
    "    \n",
    "#     from keras.utils import plot_model\n",
    "#     if draw:\n",
    "#         print(\"plotting model\")\n",
    "#         plot_model(m, to_file=cog_model+str(emb_w)+\".png\", show_shapes=True, show_layer_names=True)\n",
    "#         return m, None, config_dict\n",
    "    \n",
    "\n",
    "    o_qixs = o_qixs.reshape(-1,1)\n",
    "    o_sixs = o_sixs.reshape(-1,1)\n",
    "    o_hits = o_hits.reshape(-1, n_questions)\n",
    "    o_out = o_out.reshape(-1,1)\n",
    "\n",
    "    v_qixs = v_qixs.reshape(-1,1)\n",
    "    v_sixs = v_sixs.reshape(-1,1)\n",
    "    v_hits = v_hits.reshape(-1, n_questions)\n",
    "    v_out = v_out.reshape(-1,1)\n",
    "\n",
    "    \n",
    "#     lcb = keras.callbacks.LambdaCallback(\n",
    "# #         on_epoch_begin= lambda e,l: print(\"Begin\", m.get_layer(\"qn_embedding\").get_weights()[0]),\n",
    "#         on_epoch_end= lambda e,l: print(\"Begin\", m.get_layer(\"qn_embedding\").get_weights()[0]),\n",
    "#     )\n",
    "    \n",
    "    print(m.summary())\n",
    "    print(\"fitting\")\n",
    "\n",
    "    assert class_weightz is not None\n",
    "    assert o_qixs is not None\n",
    "    assert o_sixs is not None\n",
    "    assert o_hits is not None\n",
    "    assert o_out is not None\n",
    "    assert v_qixs is not None\n",
    "    assert v_sixs is not None\n",
    "    assert v_hits is not None\n",
    "    assert v_out is not None\n",
    "    assert es is not None\n",
    "    \n",
    "    geschichte = m.fit([o_qixs, o_sixs, o_hits], o_out, \n",
    "                       verbose=1,\n",
    "#                        batch_size=320, epochs=10000, \n",
    "                       batch_size=320, epochs=1000,\n",
    "#                        validation_split=0.1,\n",
    "                       validation_data=((v_qixs, v_sixs, v_hits), v_out), \n",
    "                       callbacks=[es], \n",
    "                       shuffle=True,\n",
    "                       class_weight=(class_weightz if balance_classes else None))\n",
    "    print(\"fertig\", cog_model, emb_w, loss, monitor)\n",
    "    return m, geschichte, config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.layer_utils import count_params\n",
    "from sklearn.metrics import log_loss\n",
    "def AIC(y, y_hat, n_params, n_obs=1):\n",
    "#     y_hat = model.predict(X)\n",
    "#     resid = y - y_hat\n",
    "#     sse = numpy.sum(numpy.power(resid,2)) / n_obs\n",
    "# y_true = np.array([0, 1, 1])\n",
    "# y_pred = np.array([0.1, 0.2, 0.9])\n",
    "\n",
    "#     print(y)\n",
    "#     print(y_hat)\n",
    "\n",
    "    ll = -log_loss(y, y_hat)\n",
    "    # 0.60671964791658428\n",
    "#     print(\"LL\", ll)\n",
    "    k   = n_params\n",
    "    aic = 2*k - 2* math.log(ll)\n",
    "    return aic\n",
    "\n",
    "def run_acc_mae_test(m, o_data, t_data, config_dict, print_clfn_report=False):\n",
    "    from sklearn.metrics import accuracy_score, mean_absolute_error, f1_score\n",
    "    print(config_dict[\"cog_model\"], \"Â£MB_W\", config_dict[\"emb_w\"])\n",
    "    t_sixs, t_qixs, t_hits, _ = t_data\n",
    "    o_sixs, o_qixs, o_hits, _ = o_data\n",
    "    p_hats = numpy.round( m.predict( [t_qixs, t_sixs, t_hits] ) )\n",
    "    p_trues = t_out\n",
    "    t_f1 = f1_score(p_trues, p_hats, average=\"macro\")\n",
    "    f1_micro = f1_score(p_trues, p_hats, average=\"micro\")\n",
    "    t_acc = accuracy_score(p_trues, p_hats)\n",
    "    t_mae = mean_absolute_error(p_trues, p_hats)\n",
    "    n_params = count_params(m.trainable_weights)\n",
    "    aic = 0 #AIC(p_trues, p_hats, n_params)\n",
    "    print(\"macro\", t_f1  )\n",
    "    print(\"micro\", f1_micro)\n",
    "    print( t_acc )\n",
    "    print( t_mae )\n",
    "    if print_clfn_report:\n",
    "        from sklearn.metrics import classification_report\n",
    "        print(classification_report(p_trues, p_hats))\n",
    "    return t_f1, t_acc, t_mae, aic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# def distance(a,b_list, cosine=False):\n",
    "# #     return numpy.sqrt(numpy.sum(numpy.power(a-b,2)))\n",
    "#     a = numpy.array(a).reshape(1,-1)\n",
    "#     if not cosine:\n",
    "#         #return scipy.spatial.distance.euclidean(a,b)\n",
    "#         return scipy.spatial.distance.cdist(a,b_list, metric=\"euclidean\")\n",
    "#     else:\n",
    "#         dasErgebnis = scipy.spatial.distance.cdist(a,b_list, metric=\"cosine\")\n",
    "# #         for x in reswlt.flatten():\n",
    "# #             if math.isnan(x):\n",
    "# #                 print(\"Nan is cosine distance result\")\n",
    "# #                 print(a)\n",
    "# #                 print(list(b_list))\n",
    "# #                 raise Exception(\"NAN in cos distance\")\n",
    "#         if numpy.isnan(numpy.sum(dasErgebnis)):\n",
    "#             dasErgebnis = numpy.nan_to_num(dasErgebnis, copy=False, nan=1.0, posinf=None, neginf=None)\n",
    "#         return dasErgebnis\n",
    "\n",
    "    \n",
    "def distance(a, b_list):\n",
    "    a = numpy.array(a).reshape(1,-1)\n",
    "    b_list = numpy.array(b_list).reshape(-1,a.shape[1]) # len(b_list) x width(a)\n",
    "    return scipy.spatial.distance.cdist(a,b_list, metric=\"euclidean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def build_adjacency_matrix(points_list, cosine=False):\n",
    "    dim = len(points_list) # num of points is the dim of the adj_mx\n",
    "#     print(\"dim is\", dim)\n",
    "    flat_dmx = [] # numpy.zeros((dim,dim)) # empty mx\n",
    "    \n",
    "    for a_ix in range(dim-1):\n",
    "        a = points_list[a_ix]\n",
    "#         print(a_ix, \"a\\n\",a)\n",
    "        to_comp = points_list[a_ix+1:]\n",
    "#         print(\"# to comp\")\n",
    "#         print(len(to_comp))\n",
    "#         print(to_comp)\n",
    "        ds = distance(a, to_comp).ravel()\n",
    "#         print(\"# little ds\", len(ds))\n",
    "#         print(ds)\n",
    "        flat_dmx.extend(list(ds))\n",
    "#     flat_dmx = list(numpy.array(flat_dmx).ravel())\n",
    "    \n",
    "    \n",
    "#     for ax,a in enumerate(points_list):\n",
    "#         for bx,b in enumerate(points_list):\n",
    "#             if ax<bx: #only fill in the top right triangle of the matrix\n",
    "#                 flat_dmx.append(distance(a,b, cosine=cosine))\n",
    "# #                 adj_mx[ax,bx] = distance(a,b)\n",
    "    print(\"flat dmx length is \", len(flat_dmx))\n",
    "    return flat_dmx\n",
    "\n",
    "\n",
    "def find_pairwise_rbo_in_adj_mx_list(adj_mx_list):\n",
    "    ranking_list = []\n",
    "    for amx in adj_mx_list:\n",
    "#         rankings = list(numpy.argsort(amx))\n",
    "        rankings = amx\n",
    "        ranking_list.append(rankings)\n",
    "    \n",
    "    rbos = []\n",
    "    seen = set()\n",
    "    for ix,r1 in enumerate(ranking_list):\n",
    "        for jx,r2 in enumerate(ranking_list):\n",
    "            if ix!=jx:\n",
    "                if ((ix,jx) not in seen) and ((jx,ix) not in seen):\n",
    "#                     print(\"r1\", r1)\n",
    "#                     print(\"r2\", r2)\n",
    "                    this_rbo = spearmanr(r1, r2)[0]\n",
    "#                     this_rbo = rbo_score(r1, r2, p=0.98)\n",
    "                    rbos.append(this_rbo)\n",
    "                    seen.add((ix,jx))\n",
    "                    seen.add((jx,ix))\n",
    "\n",
    "    print(\"correlations\", rbos)\n",
    "    mean_rbo = numpy.mean(rbos)\n",
    "#     median_rbo = numpy.median(rbos)\n",
    "    sd_rbo = numpy.std(rbos)\n",
    "    return mean_rbo, sd_rbo\n",
    "\n",
    "\n",
    "def find_pairwise_std_in_adj_mx_list(adj_mx_list):\n",
    "#     for a in adj_mx_list:\n",
    "#         plt.hist(a)\n",
    "    plt.show()\n",
    "    \n",
    "    mean_dist = numpy.mean(adj_mx_list)\n",
    "    \n",
    "    adj_mx_list = numpy.array(adj_mx_list)\n",
    "#     mean_distances = numpy.mean(adj_mx_list, axis=1).reshape(-1,1)\n",
    "#     min_distances = numpy.min(adj_mx_list, axis=1).reshape(-1,1)\n",
    "    n_adj_mx_list = adj_mx_list / mean_dist\n",
    "    vr = numpy.std(adj_mx_list, axis=0)\n",
    "    n_vr = numpy.std(n_adj_mx_list, axis=0) \n",
    "#     plt.hist(vr)\n",
    "#     plt.show()\n",
    "#     plt.hist(n_vr)\n",
    "#     plt.show()\n",
    "#     print(vr)\n",
    "#     print(n_vr)\n",
    "#     print(mean_distances)\n",
    "    vr   = numpy.mean(vr)\n",
    "    n_vr = numpy.mean(n_vr)\n",
    "    return vr, n_vr\n",
    "    \n",
    "flax = [\n",
    "    [1,2,3],\n",
    "    [1,2,3],\n",
    "    [1,3,10]\n",
    "]\n",
    "find_pairwise_std_in_adj_mx_list(flax)\n",
    "\n",
    "\n",
    "flax2 = [\n",
    "    [1,2,5],\n",
    "    [1,5,3],\n",
    "    [5,3,10]\n",
    "]\n",
    "find_pairwise_std_in_adj_mx_list(flax2)\n",
    "\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from random import sample\n",
    "from numpy.random import uniform\n",
    "import numpy as np\n",
    "from math import isnan\n",
    " \n",
    "# def hopkins(X):\n",
    "#     d = X.shape[1]\n",
    "#     #d = len(vars) # columns\n",
    "#     n = len(X) # rows\n",
    "#     m = int(0.1 * n) # heuristic from article [1]\n",
    "#     nbrs = NearestNeighbors(n_neighbors=1).fit(X)\n",
    " \n",
    "#     rand_X = sample(range(0, n, 1), m)\n",
    " \n",
    "#     ujd = []\n",
    "#     wjd = []\n",
    "#     for j in range(0, m):\n",
    "#         u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)\n",
    "#         ujd.append(u_dist[0][1])\n",
    "#         w_dist, _ = nbrs.kneighbors(X[rand_X[j]].reshape(1, -1), 2, return_distance=True)\n",
    "#         wjd.append(w_dist[0][1])\n",
    " \n",
    "#     H = sum(ujd) / (sum(ujd) + sum(wjd))\n",
    "#     if isnan(H):\n",
    "#         print (ujd, wjd)\n",
    "#         H = 0\n",
    " \n",
    "#     return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def compare_adj_matrices(adj_mcs, handle=None):\n",
    "    \n",
    "#     vr, n_vr = find_pairwise_std_in_adj_mx_list(adj_mcs)\n",
    "    vr, sd = find_pairwise_rbo_in_adj_mx_list(adj_mcs)\n",
    "    \n",
    "    max_val=0\n",
    "    for m in adj_mcs:\n",
    "        this_max = numpy.amax(m)\n",
    "        if this_max>max_val:\n",
    "            max_val = this_max\n",
    "    if max_val == 0:\n",
    "        raise Exception(\"Distance matrix is all zeros\")\n",
    "            \n",
    "    flattened_mxs = adj_mcs\n",
    "#     mean_dist = numpy.mean(adj_mcs)\n",
    "    \n",
    "    if handle:\n",
    "        print(\"For\", handle)\n",
    "    print(\"number of flattened adj mxs = \", len(flattened_mxs))\n",
    "    print(\"number of elements per flattened mx = \", len(flattened_mxs[0]))\n",
    "    print(\"distances min/max/mu/med:\", numpy.min(adj_mcs), numpy.max(adj_mcs), numpy.mean(adj_mcs), numpy.median(adj_mcs))\n",
    "#     print(\"stdev for technique\", numpy.std(adj_mcs))\n",
    "#     print(\"stdev/mu\", numpy.std(adj_mcs)/ mean_dist)\n",
    "    \n",
    "#     if normalise:\n",
    "    \n",
    "#     sc = MinMaxScaler()\n",
    "#     norm_flattened_mxs = sc.fit_transform(flattened_mxs)\n",
    "\n",
    "#     norm_flattened_mxs = []\n",
    "#     for mx in flattened_mxs:\n",
    "#         mxn = numpy.array(mx) / mean_dist\n",
    "#         norm_flattened_mxs.append(mxn)\n",
    "\n",
    "#     corr =0\n",
    "#     cnt  =0\n",
    "#     seen=set()\n",
    "#     for ix,src_mx in enumerate(flattened_mxs):\n",
    "#         for jx,des_mx in enumerate(flattened_mxs):\n",
    "#             print(\"ixjx\",ix,jx)\n",
    "#             if ix!=jx and (ix,jx) not in seen and (jx,ix) not in seen:\n",
    "#                 seen.add((ix,jx))\n",
    "#                 seen.add((jx,ix))\n",
    "#                 cnt  += 1\n",
    "#                 this_corr = spearmanr(src_mx, des_mx, axis=0)[0]\n",
    "#                 print(\"raw corr=\", this_corr)\n",
    "#                 corr += this_corr\n",
    "#             else:\n",
    "#                 print(\"seen\", ix,jx)\n",
    "\n",
    "#     print(\"corr=\",corr,\"cnt=\",cnt)\n",
    "#     corr = corr/cnt\n",
    "    \n",
    "#     n_corr =0\n",
    "#     cnt    =0\n",
    "#     seen=set()\n",
    "#     for ix,src_mx in enumerate(norm_flattened_mxs):\n",
    "#         for jx,des_mx in enumerate(norm_flattened_mxs):\n",
    "#             print(\"n ixjx\",ix,jx)\n",
    "#             if ix!=jx and (ix,jx) not in seen and (jx,ix) not in seen:\n",
    "#                 seen.add((ix,jx))\n",
    "#                 seen.add((jx,ix))\n",
    "#                 cnt  += 1\n",
    "#                 this_corr = spearmanr(src_mx, des_mx, axis=0)[0]\n",
    "#                 print(\"normed corr=\", this_corr)\n",
    "#                 n_corr += this_corr\n",
    "#             else:\n",
    "#                 print(\"seen\", ix,jx)\n",
    "#     print(\"n_corr=\",n_corr,\"ncnt=\",cnt)\n",
    "#     n_corr = n_corr/cnt\n",
    "    \n",
    "#     mu = numpy.mean(flattened_mxs, axis=0)\n",
    "#     s2 = numpy.var(flattened_mxs, axis=0)\n",
    "\n",
    "#     n_mu = numpy.mean(norm_flattened_mxs, axis=0)\n",
    "#     n_s2 = numpy.var(norm_flattened_mxs, axis=0)\n",
    "    \n",
    "#     print(\"Correlation=\", corr)\n",
    "#     print(\"Norm Corr/n=\", n_corr)\n",
    "    print(\"Average var       =\", vr, \"({})\".format(sd))\n",
    "#     print(\"Norm Av var       =\", n_vr)\n",
    "#             print(\"Norm/d corr/n Med =\", correltn_md)\n",
    "#             print(\"Norm/d corr/n Mean=\", correltn_mn)\n",
    "    return vr, sd, 0, 0 #TODO deprecate these zeroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flat dmx length is  6\n",
      "flat dmx length is  6\n",
      "ad1 [2.8284271247461903, 2.23606797749979, 2.23606797749979, 5.0, 5.0, 0.0]\n",
      "ad2 [2.8284271247461903, 2.23606797749979, 2.23606797749979, 5.0, 5.0, 0.0]\n",
      "correlations [0.9999999999999999]\n",
      "number of flattened adj mxs =  2\n",
      "number of elements per flattened mx =  6\n",
      "distances min/max/mu/med: 0.0 5.0 2.8834271799576285 2.53224755112299\n",
      "Average var       = 0.9999999999999999 (0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9999999999999999, 0.0, 0, 0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mx1 = [ [3, 3], [3, 5], [3,3], [5,8] ]\n",
    "mx1 = [ [2, 3], [4, 5], [1,1], [1,1] ]\n",
    "mx2 = [ [2, 3], [4, 5], [1,1], [1,1] ]\n",
    "\n",
    "# mx1 = [[4],[3],[2],[1],[0]]\n",
    "# mx2 = [[0],[1],[1],[1],[4]]\n",
    "\n",
    "ad1 = build_adjacency_matrix(mx1)\n",
    "ad2 = build_adjacency_matrix(mx2)\n",
    "\n",
    "# ad1 = [[0],[2],[1]]\n",
    "# ad2 = [[2],[1],[0]]\n",
    "      \n",
    "print(\"ad1\", ad1)\n",
    "print(\"ad2\", ad2)\n",
    "\n",
    "compare_adj_matrices( [ad1, ad2] )\n",
    "\n",
    "# list1 = ['a','b','c','d','e']\n",
    "# list2 = ['b','a','c','d','e']\n",
    "# list3 = ['a','b','c','e','d']\n",
    "    \n",
    "# score1 = rbo_score(list1, list2)\n",
    "# print(score1)\n",
    "# # assert score1 == 0.8\n",
    "# score2 = rbo_score(list1, list3)\n",
    "# print(score2)\n",
    "# # assert score2 == 0.95\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig\n",
      "[[ 2 45]\n",
      " [30 62]]\n",
      "samez:\n",
      " SpearmanrResult(correlation=array([[nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan]]), pvalue=array([[nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan]]))\n",
      "(4, 4)\n",
      "randz:\n",
      " SpearmanrResult(correlation=array([[ 1.        ,  0.00834949, -0.05825257,  0.03508272],\n",
      "       [ 0.00834949,  1.        , -0.02431038, -0.01282159],\n",
      "       [-0.05825257, -0.02431038,  1.        ,  0.13143306],\n",
      "       [ 0.03508272, -0.01282159,  0.13143306,  1.        ]]), pvalue=array([[0.        , 0.93429159, 0.56482397, 0.72894962],\n",
      "       [0.93429159, 0.        , 0.81026602, 0.89924979],\n",
      "       [0.56482397, 0.81026602, 0.        , 0.19241477],\n",
      "       [0.72894962, 0.89924979, 0.19241477, 0.        ]]))\n",
      "(4, 4)\n",
      "scalz:\n",
      " SpearmanrResult(correlation=array([[1., 1., 1., 1.],\n",
      "       [1., 1., 1., 1.],\n",
      "       [1., 1., 1., 1.],\n",
      "       [1., 1., 1., 1.]]), pvalue=array([[0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjm49/.venvs/isaac/lib/python3.6/site-packages/numpy/lib/function_base.py:2559: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(666)\n",
    "n_to_comp = 100\n",
    "mx_side = 2\n",
    "orig  = numpy.random.randint(0,100, size=(mx_side, mx_side))\n",
    "samez = [ orig ] * n_to_comp\n",
    "randz = [ orig ] + [ numpy.random.randint(0,100, size=(mx_side, mx_side)) for _ in range(n_to_comp-1) ]\n",
    "scalz = [ orig*(i+1) for i in range(n_to_comp) ]\n",
    "\n",
    "print(\"orig\")\n",
    "print(orig)\n",
    "\n",
    "# print(\"random\")\n",
    "# print(randz)\n",
    "\n",
    "# print(\"scaled\")\n",
    "# print(scalz)\n",
    "\n",
    "samez_flat = [numpy.ravel(s) for s in samez]\n",
    "# print(samez_flat)\n",
    "sp_r = spearmanr( samez_flat, axis=0 )\n",
    "print(\"samez:\\n\",sp_r)\n",
    "print(sp_r[0].shape)\n",
    "\n",
    "randz_flat = [numpy.ravel(r) for r in randz]\n",
    "# print(randz_flat)\n",
    "sp_r = spearmanr(randz_flat, axis=0)\n",
    "print(\"randz:\\n\", sp_r)\n",
    "print(sp_r[0].shape)\n",
    "\n",
    "scalz_flat = [numpy.ravel(c) for c in scalz]\n",
    "# print(scalz_flat)\n",
    "sp_r = spearmanr( scalz_flat, axis=0)\n",
    "print(\"scalz:\\n\",sp_r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_history_results(history_results, o_data, t_data):\n",
    "    import copy \n",
    "    rankin = []\n",
    "    max_acc = -math.inf\n",
    "    \n",
    "    for item in history_results:\n",
    "#     config_dict, m, h = item\n",
    "        config_dict, m, h = item\n",
    "        config_dict = copy.copy(config_dict)\n",
    "        cog_model = config_dict[\"cog_model\"]\n",
    "        emb_w = config_dict[\"emb_w\"]\n",
    "        try:\n",
    "            q_weight = config_dict[\"q_weight\"]\n",
    "            config_dict[\"cog_model\"] = config_dict[\"cog_model\"] + \" \" + str(emb_w) + \" \" + str(q_weight)\n",
    "        except:\n",
    "            pass\n",
    "#             raise Exception(\"No such key: q_weight\")\n",
    "\n",
    "        v_loss = h.history[\"val_loss\"]\n",
    "        v_acc  = h.history[\"val_accuracy\"]\n",
    "        v_mse   = h.history[\"val_mean_absolute_error\"]\n",
    "        o_loss   = h.history[\"loss\"]\n",
    "        o_acc    = h.history[\"accuracy\"]\n",
    "        o_mse    = h.history[\"mean_absolute_error\"]\n",
    "        plot_acc = v_acc#[0:-10]\n",
    "        plot_loss = v_loss#[0:-10]\n",
    "        plot_mse = v_mse#[0:-10]\n",
    "\n",
    "        t_acc, t_mae = run_acc_mae_test(m, o_data, t_data, config_dict)\n",
    "\n",
    "        mod_name = cog_model +\" \"+ str(emb_w)\n",
    "        try:\n",
    "            mod_name = mod_name + \" \"+ str(q_weight)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        tup = (t_mae, t_acc, min(v_mse), max(v_acc), mod_name)\n",
    "        rankin.append( tup ) \n",
    "        if max_acc < max(v_acc):\n",
    "            max_acc = max(v_acc)\n",
    "            max_mod = cog_model +\" \"+ str(emb_w)\n",
    "            min_mse = min(v_mse)\n",
    "            min_loss = min(v_loss)\n",
    "            best_m = m\n",
    "        \n",
    "    print(\"**\", max_mod, max_acc, min_loss, min_mse)\n",
    "    m = best_m\n",
    "    \n",
    "    rankin = sorted(rankin)\n",
    "    for r in rankin:\n",
    "               print(r[-1],\"&\", numpy.round(r[1],2),\"&\", numpy.round(r[0],2),\"\\\\\\\\\")\n",
    "#         print(r[-1],\"&\", numpy.round(r[1],2),\"({})\".format(numpy.round(r[3],2)),\"&\", numpy.round(r[0],2), \"({})\".format(numpy.round(r[2],2)),\"\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####\n",
    "# ## NEXT FEW CELLS: NeurIPS Compo 2020 dataset\n",
    "# #####\n",
    "\n",
    "# test_df = pandas.DataFrame.from_csv(\"./starter_kit/submission_templates/submission_task_1_2.csv\", index_col=\"QuestionId\")\n",
    "# sids = test_df[\"UserId\"]\n",
    "# qids = test_df.index\n",
    "\n",
    "# test_sids = set(sids)\n",
    "# test_qids = set(qids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_sixs = numpy.array([uniq_sids.index(s) for s in sids]).reshape(-1,1)\n",
    "# t_qixs = numpy.array([uniq_qids.index(q) for q in qids]).reshape(-1,1)\n",
    "\n",
    "# t_hits = []\n",
    "# for six,qix in zip(t_sixs,t_qixs):\n",
    "#     print(six,qix)\n",
    "#     zs = last_h[six]\n",
    "#     chits.append(zs)\n",
    "#     zs[qix] += 1\n",
    "#     print(sum(zs))\n",
    "# t_hits = numpy.array(t_hits)\n",
    "# t_hits = pca.transform(t_hits)\n",
    "\n",
    "# fnm = home+\"/models/\" + handle.replace(\"/\",\"~\") + \"_\" + str(rep)\n",
    "# m = keras.models.load_model(fnm, custom_objects={'WeightClip': WeightClip}, compile=False)\n",
    "# results = m.predict([t_qixs, t_sixs, t_hits])\n",
    "\n",
    "# print(\"----\")\n",
    "# for r in results:\n",
    "#     print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_df = pandas.DataFrame.from_csv(\"./starter_kit/data/train_data/train_task_1_2.csv\", index_col=\"QuestionId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_sids)\n",
    "# print(raw_df[\"UserId\"].isin(test_sids))\n",
    "# print(raw_df.index.isin(test_qids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_relevant = raw_df.loc[ (raw_df[\"UserId\"] in test_sids & raw_df.index in test_qids) ]\n",
    "# print(len(test_relevant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_sids = sids\n",
    "# test_qids = qids\n",
    "\n",
    "# indices = numpy.random.choice(len(raw_df), size=50000, replace=False)\n",
    "\n",
    "\n",
    "# sids = raw_df[\"UserId\"]#.iloc[indices]\n",
    "# qids = raw_df.index#[indices]\n",
    "# hout = raw_df[\"IsCorrect\"]#.iloc[indices]\n",
    "# print(len(sids))\n",
    "# del raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniq_qids = list(numpy.unique(qids))\n",
    "# print(len(uniq_qids))\n",
    "# qixlookup = {}\n",
    "# for ix,uq in enumerate(uniq_qids):\n",
    "#     qixlookup[uq] = ix\n",
    "    \n",
    "# chits = []\n",
    "# last_h = {}\n",
    "\n",
    "# # six_lookup = {}\n",
    "# uniq_sids = list(numpy.unique(sids))\n",
    "# sixlookup={}\n",
    "# for ix,us in enumerate(uniq_sids):\n",
    "#     sixlookup[us] = ix\n",
    "\n",
    "# print(\"built lookups\")\n",
    "    \n",
    "# for sid in uniq_sids:\n",
    "#     six = sixlookup[sid]\n",
    "#     last_h[six] = [0]*len(uniq_qids)\n",
    "\n",
    "# print(\"init'd last_h lookup\")\n",
    "    \n",
    "# v_indices = numpy.random.choice(len(sids), size=1000, replace=False)\n",
    "\n",
    "# sixs = numpy.array([sixlookup[s] for s in sids])\n",
    "# print(\"sixs remap done\")\n",
    "# qixs = numpy.array([qixlookup[q] for q in qids])\n",
    "# print(\"qixs remap done\")\n",
    "# hout = numpy.array(hout)\n",
    "# print(\"EOND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heu = generate_heu_autoencoder(len(uniq_qids), 100)\n",
    "# es = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "        \n",
    "# for six,qix in zip(sixs,qixs):\n",
    "#     print(six,qix)\n",
    "#     zs = last_h[six]\n",
    "#     chits.append(zs)\n",
    "#     zs[qix] += 1\n",
    "#     print(sum(zs))\n",
    "    \n",
    "#     if len(chits > 1000):\n",
    "#         heu.fit(o_hits, o_hits, callbacks=[es], validation_split=0.05, epochs=10000, batch_size=len(chits), shuffle=True)\n",
    "#         chits = []\n",
    "# heu.fit(o_hits, o_hits, callbacks=[es], validation_split=0.05, epochs=10000, batch_size=len(chits), shuffle=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./real_data/examliftb1_LFA_strat_1000000_m100\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(658050, 1130) None\n",
      "(1254, 1130)\n",
      "(1258, 1130)\n"
     ]
    }
   ],
   "source": [
    "seen = None\n",
    "max_s = 1000000\n",
    "# if dataset_handle == \"cmu_geom_steplevel\":\n",
    "#     (sixs, qixs, hout) = pickle.load(open(home+\"/real_data/cmu_geom_steplevel.p\", \"rb\"))\n",
    "#     min_hist = 0\n",
    "# elif dataset_handle == \"examliftb1\":\n",
    "#     (sixs, qixs, hout) = pickle.load(open(home+\"/real_data/XL1041.p\", \"rb\"))\n",
    "#     min_hist = 40\n",
    "# else:\n",
    "#     raise Exception(\"Invalid dataset handle \" + str(dataset_handle))\n",
    "\n",
    "# dataset_handle = \"isaac\" \n",
    "dataset_handle = \"examliftb1\" \n",
    "# dataset_handle = \"cmu_geom_steplevel\" \n",
    "\n",
    "min_hist = 100\n",
    "\n",
    "# dataset_name = \"examliftb1\"+str(max_s)\n",
    "dataset_name = dataset_handle+\"_LFA_strat_\"+str(max_s)+\"_m\"+str(min_hist)\n",
    "fnm = home+\"/real_data/\" + dataset_name\n",
    "# fnm = home+\"/real_data/\" + dataset_name\n",
    "print(fnm)\n",
    "with open(fnm, 'rb') as f:\n",
    "     data_bundle = pickle.load(f)\n",
    "        \n",
    "# ./real_data/examliftb1100000\n",
    "\n",
    "odata, vdata, tdata, sid_six_lookup, qid_qix_lookup = data_bundle\n",
    "(o_sixs, o_qixs, o_hits, o_out), (v_sixs, v_qixs, v_hits, v_out), (t_sixs, t_qixs, t_hits, t_out) = (odata, vdata, tdata)\n",
    "print(o_hits.shape, print(type(o_hits)))\n",
    "print(v_hits.shape)\n",
    "print(t_hits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RESET\n",
    "model_lookup = {} #defaultdict(list)\n",
    "max_acc = 0\n",
    "handles = []\n",
    "\n",
    "from keras.metrics import binary_accuracy, binary_crossentropy, mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import f1_score\n",
    "import os\n",
    "mon_lookup = {\n",
    "    \"binary_crossentropy\" : \"xe\",\n",
    "    \"mean_squared_error\"  : \"mse\",\n",
    "    \"mean_absolute_error\" : \"mae\",\n",
    "    \"f1_loss\" : \"f1\",\n",
    "    \"binary_accuracy\" : \"acc\",\n",
    "    \"log_likelihood\" : \"ll\",\n",
    "    \"loss\" : \"px\"\n",
    "}\n",
    "\n",
    "# max_s = 250000\n",
    "print(\"max_s\", max_s)\n",
    "\n",
    "\n",
    "def step_through_cog_models(data_bundle=None, cog_models=None):\n",
    "# def step_through_cog_models(sixs, qixs, hout, cog_models=None, data_bundle=None):\n",
    "#     cog_models = [\"MLP\", \"MLP+\",\"MLTM\",\"LFA\",\"SLFA\"]\n",
    "#     cog_models = [\"MLP\", \"MLP+\",\"MLTM\", \"MLTM+\", \"MLTM_no_init\"]\n",
    "#     cog_models = [\"MLP\", \"MLP+\",\"MLTM\", \"MLTM+\", \"LFA\",\"SLFA\"]\n",
    "    #(\"MLTM+\",32,10,\"val_f1_m\",False)\n",
    "    metrics = [binary_crossentropy, binary_accuracy, mean_absolute_error, mean_squared_error, f1_loss]\n",
    "\n",
    "    if cog_models is None:\n",
    "        core_models = [\"CFM\", \"RASCH\", \"AFM\", \"MLTM\"]\n",
    "        variant_models = [\"AFMg\", \"MLTMb\", \"MLTM0\"]\n",
    "#         ffnn_models = [\"MLPs\", \"MLPsz\"] #\"MLP\", \"MLPd\"]#, \"CONC\"]\n",
    "        ffnn_models = [\"MLPs\", \"MLP\", \"MLPd\"]#, \"CONC\"]\n",
    "        regd_models = [s+\"z\" for s in [\"MLTM\",\"MLP\",\"MLPd\",\"AFM\",\"AFMg\",\"CFM\",\"MLTMb\",\"MLTM0\"]]\n",
    "#         cog_models = variant_models\n",
    "        cog_models = regd_models\n",
    "#         cog_models = [\"AFM\", \"AFMg\", \"AFMx\", \"MLTM\", \"MLTMa\",\"MLTMb\", \"CFM\", \"MLP\", \"MLPd\"]\n",
    "#         cog_models = [\"MLTMz\"] #\"MLTMb\", \"MLP\", \"CFM\", \"RASCH\", \"AFM\", \"AFMg\", \"AFMx\", \"MLTM\", \"MLTMa\",\"MLTMb\",\"MLTM0\", \"MLP\", \"MLPd\"]\n",
    "#         cog_models = [\"AFMg\"]\n",
    "        cog_models = [\"MLTMz\", \"MLPz\"]#,\"AFMg\"]\n",
    "#         cog_models = [\"RASCH\"]\n",
    "#         cog_models = [\"MLTMb\",\"MLTMbz\",\"MLTM0\",\"MLTM0z\"]\n",
    "#         cog_models = [\"MLPrawDen\", \"MLPrawDP\"]#, \"MLPrawAD\", \"MLPrawADD\"]\n",
    "#         cog_models = ffnn_models\n",
    "#         emb_ws = [400,300,256,128,64,32,16, 8]\n",
    "        emb_ws = [ 64, ]#, 32, 64]\n",
    "#         emb_ws = [24, 32]\n",
    "#         emb_ws = [32, 64, 128]\n",
    "#         emb_ws = [96]# 32, 64, 96]\n",
    "#         emb_ws = [8,16,32, 64,128]#, 256, 300, 400]\n",
    "#         emb_ws = [32,64,128,256,300,400,500]\n",
    "\n",
    "#         emb_ws = [8,]#, 16, 32, 64, 128, 256, 300, 400,]\n",
    "\n",
    "#         q_ws = [None]\n",
    "#         losses = [\"binary_crossentropy\",\"mean_squared_error\",\"f1_loss\"]#, \"binary_crossentropy\"]\n",
    "#         losses = [(\"binary_crossentropy\", \"val_loss\")]\n",
    "        losses = [\n",
    "                    (\"binary_crossentropy\", \"val_loss\"), \n",
    "#                   (\"binary_crossentropy\", \"val_f1_loss\"), \n",
    "#                   (\"mean_squared_error\", \"val_loss\"),\n",
    "#                   (\"f1_loss\", \"val_f1_loss\")\n",
    "                 ]\n",
    "#         losses = [(\"f1_loss\", \"val_f1_loss\")]\n",
    "        q_ws = [None]\n",
    "\n",
    "#         reg_ws = numpy.random.normal(loc=1e-5, scale=1e-5, size=20) # [1e-8, 1e-7, 1e-6, 0.00001]#, 0.0001, 0.001]\n",
    "        reg_ws = [None]\n",
    "#         reg_ws = [None, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "#         monitor_settings = [\"val_mean_absolute_error\", \"val_mean_squared_error\"]\n",
    "#         monitor_settings = [\"val_loss\", \"val_f1_m\", \"val_mean_squared_error\"]\n",
    "        balance_settings = [ False, ]\n",
    "#         q_ws = [None, 1, 5, 10, 50, 100, 500]\n",
    "#         q_ws = [None,1,10,100,1000]\n",
    "#     emb_ws = [14, ]# [1,2,4,6,8,10,12,14,16]\n",
    "#     emb_ws = [1,2,4,6,8,10,12,14,16]\n",
    "    n_reps = 5\n",
    "\n",
    "    payload = []\n",
    "        \n",
    "\n",
    "    seen=[]\n",
    "    \n",
    "    \n",
    "    odata, vdata, tdata, sid_six_lookup, qid_qix_lookup = data_bundle\n",
    "    (o_sixs, o_qixs, o_hits, o_out), (v_sixs, v_qixs, v_hits, v_out), (t_sixs, t_qixs, t_hits, t_out) = (odata, vdata, tdata)\n",
    "    o_hits = o_hits.astype(\"int8\")\n",
    "    \n",
    "    print(len(v_out), sum(v_out))\n",
    "    print(len(t_out), sum(t_out))\n",
    "#     raise Exception(\"le what\")\n",
    "    \n",
    "    f1s = []\n",
    "    overwrite_disc=True\n",
    "    for rep in range(n_reps):\n",
    "        for cog_model in cog_models:# zip(cog_models, q_ws):\n",
    "            for w in emb_ws:\n",
    "                if cog_model==\"RASCH\":\n",
    "                    w=1\n",
    "                for qw in q_ws:\n",
    "                    for rw in reg_ws:\n",
    "                        w0 = rw\n",
    "                        for bal in balance_settings:\n",
    "                            for lozz,mon in losses:\n",
    "                                best_f1_regw = (0,0)\n",
    "#                                 rw = None #rw / (w*n_students)\n",
    "                                #w1 = rw if (rw is not None) else None\n",
    "#                                 w1 = 5.5e-8\n",
    "                                w1 = rw\n",
    "    \n",
    "#                                 if max_w = None:\n",
    "#                                     max_w = (rw, 0)                \n",
    "#                                 w1s = numpy.random.normal(max_w[0], scale=w1, size=10)\n",
    "                    \n",
    "                                qcode = \"\" if (qw is None) else \"q\"+str(qw)\n",
    "                                moncode = mon_lookup[mon[mon.index(\"_\")+1:]]\n",
    "                                losscode = mon_lookup[lozz]\n",
    "                                balstr = \"bal\" if bal else \"\"\n",
    "                                \n",
    "                                handle = \"{}{}/{}/{}({}/{})\".format(cog_model, w, qcode, balstr, losscode, moncode)\n",
    "                                memkey = handle+str(w1)\n",
    "                                if max_s != 100000:\n",
    "                                    handle += \"#\"+str(max_s)\n",
    "                                print(\"GO FOR\", handle)\n",
    "                                \n",
    "                                fn = home+\"/lfa_models/\" + handle.replace(\"/\",\"~\") + \"_\" + str(rep)\n",
    "                                \n",
    "                                print(\"checking for cached file\", fn)\n",
    "                                if os.path.isfile(fn):\n",
    "                                    print(fn, \"found\")\n",
    "                                    continue         \n",
    "                                \n",
    "                                m, h, config_dict = gen_and_train(odata, vdata, tdata, draw=True, cog_model=cog_model, emb_w=w, q_weight=qw, monitor=mon, balance_classes=bal, loss=lozz, metrics=metrics, reg_w=w1)\n",
    "#                                 m.summary()\n",
    "#                                 t_sixs, t_qixs, t_hits, _ = t_data\n",
    "#                                 o_sixs, o_qixs, o_hits, _ = o_data\n",
    "\n",
    "                                op_hats = numpy.round( m.predict( [o_qixs, o_sixs, o_hits] ) )\n",
    "                                op_trues = numpy.round(o_out)    \n",
    "                                o_f1 = f1_score(op_trues, op_hats, average=\"macro\")\n",
    "\n",
    "                                vp_hats = numpy.round( m.predict( [v_qixs, v_sixs, v_hits] ) )\n",
    "                                vp_trues = numpy.round(v_out)    \n",
    "                                v_f1 = f1_score(vp_trues, vp_hats, average=\"macro\")\n",
    "    \n",
    "                                p_hats = numpy.round( m.predict( [t_qixs, t_sixs, t_hits] ) )\n",
    "                                p_trues = numpy.round(t_out)    \n",
    "                                t_f1 = f1_score(p_trues, p_hats, average=\"macro\")\n",
    "                \n",
    "#                                 if t_f1 > max_w[1]:\n",
    "#                                     max_w = (w1,t_f1)\n",
    "                \n",
    "                                print(w1, \":F1s v/t \", o_f1, v_f1, t_f1)\n",
    "                                f1s.append((handle, w0, w1, v_f1, t_f1))\n",
    "                \n",
    "                                config_dict[\"q_weight\"]=qw\n",
    "                                config_dict[\"monitor_value\"]=mon\n",
    "                                config_dict[\"balance_classes\"]=bal\n",
    "                                config_dict[\"loss\"]=lozz\n",
    "                                \n",
    "                                config_dict[\"handle\"] = handle\n",
    "                                \n",
    "                                payload.append( (config_dict, _, _) )\n",
    "                                \n",
    "                                plt.plot(h.history['loss'], label='xH (trn)')\n",
    "                                plt.plot(h.history['val_loss'], label='xH (val)')\n",
    "                                plt.plot(h.history['f1_loss'], label='F1L (trn)')\n",
    "                                plt.plot(h.history['val_f1_loss'], label='F1L (val)')\n",
    "                                plt.title(handle)\n",
    "#                                 plt.ylabel('MAE value')\n",
    "#                                 plt.xlabel('No. epoch')\n",
    "                                plt.legend(loc=\"upper right\")\n",
    "                                plt.show()\n",
    "                                \n",
    "                                if overwrite_disc:\n",
    "                                    try:\n",
    "                                        m.save(home+\"/lfa_models/\" + handle.replace(\"/\",\"~\") + \"_\" + str(rep), save_format=\"h5\")\n",
    "                                    except OSError as ose:\n",
    "                                        print(ose)\n",
    "                                        import shutil\n",
    "                                        shutil.rmtree(fn)\n",
    "                                        m.save(home+\"/lfa_models/\" + handle.replace(\"/\",\"~\") + \"_\" + str(rep), save_format=\"h5\")\n",
    "                                                                \n",
    "        for h, ww,w,v,t in f1s:\n",
    "    #         print(h, ww,w,\"\\t\", v,\"\\t\", t)\n",
    "            print(h, ww,w,\"\\t\\t\", t)\n",
    "\n",
    "    mn = numpy.median([tup[4] for tup in f1s])\n",
    "    mae = numpy.mean([numpy.abs(tup[4]-mn) for tup in f1s])\n",
    "    print(mn, mae)\n",
    "    \n",
    "    return payload, data_bundle\n",
    "\n",
    "#########################################\n",
    "\n",
    "# try:\n",
    "#     del data_bundle\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "# try:\n",
    "#     data_bundle\n",
    "#     print(\"DATA FOUJND\")\n",
    "#     (sixs, qixs, hout) = (None, None, None)\n",
    "# except:\n",
    "#     data_bundle = None\n",
    "# (sixs, qixs, hout) = pickle.load(open(home+\"/real_data/XL1041.p\", \"rb\"))\n",
    "    \n",
    "\n",
    "history_results, data_bundle = step_through_cog_models(data_bundle=data_bundle)\n",
    "(o_data, v_data, t_data, sid_six_lookup, qid_qix_lookup) = data_bundle\n",
    "\n",
    "(o_sixs, o_qixs, o_hits, o_out), (v_sixs, v_qixs, v_hits, v_out), (t_sixs, t_qixs, t_hits, t_out) = (o_data, v_data, t_data)\n",
    "print(numpy.array(o_hits).shape)\n",
    "print(numpy.array(v_hits).shape)\n",
    "print(numpy.array(t_hits).shape)\n",
    "\n",
    "print(\"DATAGEN DONE\")\n",
    "print(max(o_qixs), max(v_qixs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prep done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.38      0.48    180201\n",
      "          1       0.80      0.92      0.86    480361\n",
      "\n",
      "avg / total       0.76      0.77      0.75    660562\n",
      "\n",
      "***\n",
      "2512\n",
      "2512\n"
     ]
    }
   ],
   "source": [
    "def make_CRF_model(sixs,qixs,obs, v_sixs, v_qixs, v_out, t_sixs, t_qixs, t_out, o_hits, v_hits, t_hits):\n",
    "    from sklearn_crfsuite import CRF\n",
    "    from sklearn.cross_validation import cross_val_predict\n",
    "    from sklearn_crfsuite.metrics import flat_classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "    crf = CRF(algorithm='lbfgs',\n",
    "              c1=0.1,\n",
    "              c2=0.1,\n",
    "              max_iterations=100,\n",
    "              all_possible_transitions=False)\n",
    "    \n",
    "    def make_interaction_histories(sixs, qixs, obs, v_sixs,v_qixs,v_out, t_sixs, t_qixs, t_out, o_hits, v_hits, t_hits):\n",
    "        '''separate out the s,q,o data into interaction histories, one for each student'''\n",
    "        \n",
    "        pca = TruncatedSVD(n_components=2)\n",
    "        pca.fit(o_hits)\n",
    "        \n",
    "        histories = []\n",
    "        mappage = defaultdict(list)\n",
    "        for s,q,o,h in zip(sixs,qixs,obs, o_hits):\n",
    "#             print(pca.transform(h).shape)\n",
    "            mappage[s].append((str(s),str(q), \"1\" if o else \"0\", pca.transform(h)))\n",
    "\n",
    "        for s,q,o,h in zip(v_sixs,v_qixs,v_out, v_hits):\n",
    "            mappage[s].append((str(s),str(q), \"1\" if o else \"0\", pca.transform(h)))\n",
    "    \n",
    "        for s,q,o,h in zip(t_sixs,t_qixs,t_out, t_hits):\n",
    "            mappage[s].append((str(s),str(q), \"1\" if o else \"0\", pca.transform(h)))\n",
    "\n",
    "        for s in mappage:\n",
    "            histories.append(mappage[s])\n",
    "\n",
    "#         for this_six in numpy.unique(sixs):\n",
    "#             interaction_hist = [(s,q,o) for s,q,o in zip(sixs,qixs,obs) if s==this_six]\n",
    "#         histories.append(interaction_hist)\n",
    "        return histories\n",
    "    \n",
    "    def tup2features(interaction_hist, ix):\n",
    "        features = {\n",
    "        'bias': 1.0,\n",
    "#         'history': hist,\n",
    "        's_ix': interaction_hist[ix][0],\n",
    "        'q_ix': interaction_hist[ix][1],\n",
    "#         'ob': interaction_hist[ix][2],\n",
    "        'hist_x' : interaction_hist[ix][3][0][0],\n",
    "        'hist_y' : interaction_hist[ix][3][0][1],\n",
    "        'begin_hist' : False,\n",
    "        'end_hist' : False,\n",
    "        }\n",
    "        \n",
    "        if ix>0:\n",
    "            features.update({\n",
    "#             's_ix--': interaction_hist[ix-1][0],\n",
    "            'q_ix--': interaction_hist[ix-1][1],\n",
    "            'ob--': interaction_hist[ix-1][2],\n",
    "            'hist_x--' : interaction_hist[ix-1][3][0][0],\n",
    "            'hist_y--' : interaction_hist[ix-1][3][0][1],\n",
    "            })\n",
    "        else:\n",
    "            features[\"begin_hist\"] = True\n",
    "\n",
    "        if ix < len(interaction_hist)-1:\n",
    "            features.update({\n",
    "#             's_ix++': interaction_hist[ix+1][0],\n",
    "            'q_ix++': interaction_hist[ix+1][1],\n",
    "            'ob++': interaction_hist[ix+1][2],\n",
    "            'hist_x++' : interaction_hist[ix+1][3][0][0],\n",
    "            'hist_y++' : interaction_hist[ix+1][3][0][1],\n",
    "            })\n",
    "        else:\n",
    "            features[\"end_hist\"] = True\n",
    "        return features\n",
    "\n",
    "    def hist2features(interaction_hist):\n",
    "        return [tup2features(interaction_hist, ix) for ix in range(len(interaction_hist))]\n",
    "        \n",
    "    def hist2labels(interaction_hist):\n",
    "        return [interaction_hist[ix][2] for ix in range(len(interaction_hist))]\n",
    "        \n",
    "    hist_list = make_interaction_histories(sixs,qixs,obs, v_sixs,v_qixs,v_out, t_sixs, t_qixs, t_out, o_hits, v_hits, t_hits)\n",
    "    X,y = [],[]\n",
    "    for hist in hist_list:\n",
    "#         print(hist)\n",
    "#         print(hist.shape)\n",
    "        X.append(hist2features(hist))\n",
    "        y.append(hist2labels(hist))\n",
    "        \n",
    "#     hist_list = make_interaction_histories(t_sixs,t_qixs,t_out)\n",
    "#     tX,ty = [],[]\n",
    "#     for hist in hist_list:\n",
    "# # #         print(hist)\n",
    "# # #         print(hist.shape)\n",
    "#         tX.append(hist2features(hist))\n",
    "#         ty.append(hist2labels(hist))\n",
    "#     print(len(tX))\n",
    "#     print(tX[0])\n",
    "    \n",
    "        \n",
    "    print(\"prep done\")\n",
    "    pred = cross_val_predict(estimator=crf, X=X, y=y, cv=5)\n",
    "    report = flat_classification_report(y_pred=pred, y_true=y)\n",
    "    print(report)\n",
    "    \n",
    "#     crf.fit(X,y)\n",
    "#     pred = crf.predict(tX)\n",
    "#     print(pred[0:10])\n",
    "    \n",
    "#     pred = numpy.array(pred).ravel()\n",
    "    print(\"***\")\n",
    "    print(len(X))\n",
    "    print(len(y))\n",
    "    \n",
    "#     pred2 = []\n",
    "#     y2 = []\n",
    "#     for plist,ylist in zip(pred,y):\n",
    "#         for item,yy in zip(plist,ylist):\n",
    "#             pred2.append(int(item))\n",
    "#             y2.append(int(yy))\n",
    "#     y=y2\n",
    "#     pred = pred2\n",
    "#     print(pred[0:100])\n",
    "#     print(y[0:100])\n",
    "            \n",
    "#     print(f1_score(y_pred=pred, y_true=y, average=\"macro\"))\n",
    "\n",
    "# print(o_out.astype(bool))\n",
    "make_CRF_model(o_sixs, o_qixs, o_out, v_sixs, v_qixs, v_out, t_sixs, t_qixs, t_out, o_hits, v_hits, t_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# f1_kde={}\n",
    "# acc_kde={}\n",
    "# upto=10\n",
    "# for h in seen:\n",
    "#     print(h)\n",
    "#     pears = numpy.array(agg[h])\n",
    "#     f1s = [p[0] for p in pears]\n",
    "#     mu_f1 = numpy.mean(f1s)\n",
    "#     accs = [p[1] for p in pears]\n",
    "#     mu_acc = numpy.mean(accs)\n",
    "#     print(h, len(pears))\n",
    "#     print(pears)\n",
    "# #     print(av_f1, av_acc)\n",
    "#     print(\"\\t\", numpy.round(mu_f1,4), numpy.round(mu_acc,4))\n",
    "#     f1_kde[h] = f1s[0:upto]\n",
    "#     acc_kde[h] = accs[0:upto]\n",
    "\n",
    "# print(\"Plot of resultant F1 score\")\n",
    "# # print(f1_kde)\n",
    "# for row in f1_kde:\n",
    "#     print(row)\n",
    "# ax1 = pandas.DataFrame(f1_kde).plot.kde(bw_method=.8, figsize=(8,8))\n",
    "# ax1.set_xlabel(\"$F_{1}$\")\n",
    "# ax1.set_title(\"Distribution of prediction $F_{1}$ across models\")\n",
    "\n",
    "# print(\"Plot of resultant F1 score\")\n",
    "# print(acc_kde)\n",
    "# ax2 = pandas.DataFrame(acc_kde).plot.kde(bw_method=.8, figsize=(8,8))\n",
    "# ax2.set_xlabel(\"Accuracy\")\n",
    "# ax2.set_title(\"Distribution of prediction accuracy across models\")\n",
    "\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Longitudinal_Datagen.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
