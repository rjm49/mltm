{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "from math import exp, sqrt, log\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras import Input, Model\n",
    "from keras.layers import Lambda, subtract, concatenate\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from importlib import reload\n",
    "from matplotlib import pyplot as plt\n",
    "from random import random, shuffle, choice, randint, uniform\n",
    "\n",
    "from classes import Student, Question\n",
    "from utils import generate_student_name, create_qs, create_students, generate_attempts, calculate_pass_probability, attempt_q\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "from NN_utils import BigTable, WeightClip\n",
    "\n",
    "print(\"started\")\n",
    "\n",
    "use_saved = True\n",
    "do_train = True\n",
    "do_testing = True\n",
    "create_scorecards = True\n",
    "\n",
    "base = \"../../../isaac_data_files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def fro_norm(w):\n",
    "# # #     return K.sqrt(K.sum(K.square(K.abs(w))))\n",
    "# #     return K.sqrt(K.sum(K.square(w)))\n",
    "\n",
    "# # def cust_reg(w):\n",
    "# #     m = K.dot(K.transpose(w), w) - np.eye(w.shape)\n",
    "# #     return fro_norm(m)\n",
    "\n",
    "# from keras.layers import add, GaussianNoise, Subtract\n",
    "import tensorflow as tf\n",
    "# # import tensorflow_probability as tfp\n",
    "# def tf_cov(x):\n",
    "#     mean_x = tf.reduce_mean(x, axis=0, keep_dims=True)\n",
    "#     _, var = tf.nn.moments(x, axes=[0])\n",
    "#     mx = tf.matmul(tf.transpose(mean_x), mean_x)\n",
    "#     vx = tf.matmul(tf.transpose(x), x)/tf.cast(tf.shape(x)[0], tf.float32)\n",
    "#     cov_xx = vx - mx\n",
    "#     return (cov_xx/var, var, cov_xx)\n",
    "\n",
    "def tf_corrcoef(x):\n",
    "    def t(x): return tf.transpose(x)\n",
    "#     x_t = tf.constant(x)\n",
    "#     y_t = tf.constant(y)\n",
    "    xy_t =  x#tf.concat([x, x], axis=0)\n",
    "    mean_t = tf.reduce_mean(xy_t, axis=0, keep_dims=True)\n",
    "    cov_t =  tf.matmul(t(xy_t-mean_t),xy_t-mean_t)/(tf.cast(tf.shape(x)[0], tf.float32))\n",
    "    cov2_t = tf.diag(1/tf.sqrt(tf.diag_part(cov_t)))\n",
    "    cor = cov2_t @ cov_t @ cov2_t\n",
    "    return cor\n",
    "\n",
    "# # def K_corrmx(x):\n",
    "# #     var = K.var(x, axis=1)\n",
    "\n",
    "# def tf_corrcoef(x):\n",
    "#     return tfp.stats.correlation(\n",
    "#     x,\n",
    "#     y=None,\n",
    "#     sample_axis=0,\n",
    "#     event_axis=-1,\n",
    "#     keepdims=False,\n",
    "#     name=None\n",
    "#     )\n",
    "\n",
    "# data = numpy.array([[1., 4, 2], \n",
    "#                     [5, 6, 24], \n",
    "#                     [15, 1, 5], \n",
    "#                     [7,3,8], \n",
    "#                     [9,4,7]])\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     print(sess.run(tf_cov(tf.constant(data, dtype=tf.float32))))\n",
    "#     print(sess.run(tf_corrcoef(tf.constant(data, dtype=tf.float32))))\n",
    "\n",
    "# ## validating with numpy solution\n",
    "# pc = numpy.corrcoef(data.T, bias=True)\n",
    "# print(pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l1\n",
    "from keras.layers import Dropout, multiply, Subtract, Concatenate, Dense\n",
    "from keras import backend as K\n",
    "def generate_qs_model(qn_table, psi_table, optimiser):\n",
    "    psi_sel = Input(shape=(1,), name=\"psi_select\", dtype=\"int32\")\n",
    "    qn_sel = Input(shape=(1,), name=\"q_select\", dtype=\"int32\")\n",
    "    print(qn_table, psi_table, psi_sel, qn_sel)\n",
    "    print(\"psi_sel shape\", psi_sel.shape)\n",
    "\n",
    "    psi_table.trainable=True\n",
    "    qn_table.trainable=True\n",
    "    \n",
    "    qn_row = qn_table(qn_sel)\n",
    "    psi_row = psi_table(psi_sel)\n",
    "    \n",
    "    qn_ws = qn_table.weights[0]\n",
    "    psi_ws = psi_table.weights[0]\n",
    "\n",
    "    klip = Lambda(lambda w: K.clip(w,0,1))\n",
    "    \n",
    "    q_masque = klip(qn_row)\n",
    "#     q_masque = Dropout(0.2)(q_masque)\n",
    "    s_masque = Lambda(lambda s: K.clip(s,0,1))(psi_row)\n",
    "\n",
    "#     qn_row = GaussianNoise(.1)(qn_row)\n",
    "#     psi_row = GaussianNoise(.1)(psi_row)\n",
    "    \n",
    "#     L2norm = Lambda(lambda t: K.l2_normalize(t, axis=0))\n",
    "#     nqn_ws = L2norm(qn_ws)\n",
    "#     npsi_ws = L2norm(psi_ws)\n",
    "    \n",
    "#     L_ortho = Lambda(lambda w : K.sqrt(K.sum(K.square(K.dot(K.transpose(w), w) - numpy.eye(w.shape[1])))))\n",
    "#     L_ortho = Lambda(lambda w : K.sqrt(K.sum(K.square(K.dot(w, K.transpose(w)) - numpy.eye(w.shape[0])))))\n",
    "\n",
    "#     L_2 = Lambda(lambda w: K.mean(K.abs(K.sum(K.clip(w,0,1), axis=1)-1))\n",
    "    L_2 = Lambda(lambda w: K.sqrt(K.mean(K.square(w))))\n",
    "#     L_2 = Lambda(lambda w: K.mean(K.mean(w, axis=1)))\n",
    "\n",
    "    svm = Lambda(lambda w: K.sum(w))\n",
    "    avg = Lambda(lambda w: K.mean(w))\n",
    "    root = Lambda(lambda w: K.sqrt(w))\n",
    "    than = Lambda(lambda w: K.tanh(w))\n",
    "#     ortho_loss = L_ortho(qn_row)\n",
    "    #L_d1 = Lambda(lambda w: K.mean(100*K.abs(K.clip(K.sum(K.clip(w,0,1), axis=1)-1, -1,1)) ))\n",
    "#     L_d2 = Lambda(lambda w: K.mean(K.abs(K.sum(K.tanh(w), axis=1)-numpy.tanh(1)) ))\n",
    "\n",
    "    \n",
    "    \n",
    "# ortho_loss = L_ortho(npsi_ws) + L_ortho(nqn_ws)\n",
    "\n",
    "#      L_2_loss = L_2(qn_ws) + L_2(psi_ws)\n",
    "#     print(\"ortho_loss=\",ortho_loss)\n",
    "\n",
    "\n",
    "#     q_w = Lambda(lambda qm: K.sum(qm), name=\"q_sum\")(q_masque)\n",
    "#     s_w = Lambda(lambda sm: K.sum(sm), name=\"s_sum\")(s_masque)\n",
    "\n",
    "    qn_row = Lambda(lambda q: K.clip(q,1,math.inf))(qn_row)\n",
    "    psi_row = Lambda(lambda s: K.clip(s,1,math.inf))(psi_row)\n",
    "\n",
    "\n",
    "    difs = Concatenate()([psi_row, qn_row])\n",
    "    h = Dense(100, activation=\"selu\")(difs)\n",
    "#     h = Dense(30, activation=\"relu\")(h)\n",
    "#     h = Dense(30, activation=\"relu\")(h)\n",
    "    Pr = Dense(1, activation=\"sigmoid\")(h)\n",
    "    \n",
    "    print(\"Pr\",Pr.shape)\n",
    "#     model = Model(inputs=[qn_sel, psi_sel], outputs=[Pr,W])\n",
    "#     model.compile(optimizer=optimiser, loss=[\"binary_crossentropy\",\"mae\"], metrics=[\"mse\",\"accuracy\"], loss_weights=[1,0.1])\n",
    "    model = Model(inputs=[qn_sel, psi_sel], outputs=Pr)\n",
    "    model.compile(optimizer=optimiser, loss=\"binary_crossentropy\", metrics=[\"mse\",\"accuracy\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p999 9.20979013808\n",
      "p50 2.63424047381\n",
      "max inf\n",
      "av nan\n",
      "Qinit 0\n",
      "Q choices are range(0, 10)\n",
      "Qinit 1\n",
      "Q choices are range(0, 10)\n",
      "Qinit 2\n",
      "Q choices are range(0, 10)\n",
      "Qinit 3\n",
      "Q choices are range(0, 10)\n",
      "Qinit 4\n",
      "Q choices are range(0, 10)\n",
      "Qinit 5\n",
      "Q choices are range(0, 10)\n",
      "Qinit 6\n",
      "Q choices are range(0, 10)\n",
      "Qinit 7\n",
      "Q choices are range(0, 10)\n",
      "Qinit 8\n",
      "Q choices are range(0, 10)\n",
      "Qinit 9\n",
      "Q choices are range(0, 10)\n",
      "Qinit 10\n",
      "Q choices are range(0, 10)\n",
      "Qinit 11\n",
      "Q choices are range(0, 10)\n",
      "Qinit 12\n",
      "Q choices are range(0, 10)\n",
      "Qinit 13\n",
      "Q choices are range(0, 10)\n",
      "Qinit 14\n",
      "Q choices are range(0, 10)\n",
      "Qinit 15\n",
      "Q choices are range(0, 10)\n",
      "Qinit 16\n",
      "Q choices are range(0, 10)\n",
      "Qinit 17\n",
      "Q choices are range(0, 10)\n",
      "Qinit 18\n",
      "Q choices are range(0, 10)\n",
      "Qinit 19\n",
      "Q choices are range(0, 10)\n",
      "Qinit 20\n",
      "Q choices are range(0, 10)\n",
      "Qinit 21\n",
      "Q choices are range(0, 10)\n",
      "Qinit 22\n",
      "Q choices are range(0, 10)\n",
      "Qinit 23\n",
      "Q choices are range(0, 10)\n",
      "Qinit 24\n",
      "Q choices are range(0, 10)\n",
      "Qinit 25\n",
      "Q choices are range(0, 10)\n",
      "Qinit 26\n",
      "Q choices are range(0, 10)\n",
      "Qinit 27\n",
      "Q choices are range(0, 10)\n",
      "Qinit 28\n",
      "Q choices are range(0, 10)\n",
      "Qinit 29\n",
      "Q choices are range(0, 10)\n",
      "Qinit 30\n",
      "Q choices are range(0, 10)\n",
      "Qinit 31\n",
      "Q choices are range(0, 10)\n",
      "Qinit 32\n",
      "Q choices are range(0, 10)\n",
      "Qinit 33\n",
      "Q choices are range(0, 10)\n",
      "Qinit 34\n",
      "Q choices are range(0, 10)\n",
      "Qinit 35\n",
      "Q choices are range(0, 10)\n",
      "Qinit 36\n",
      "Q choices are range(0, 10)\n",
      "Qinit 37\n",
      "Q choices are range(0, 10)\n",
      "Qinit 38\n",
      "Q choices are range(0, 10)\n",
      "Qinit 39\n",
      "Q choices are range(0, 10)\n",
      "Qinit 40\n",
      "Q choices are range(0, 10)\n",
      "Qinit 41\n",
      "Q choices are range(0, 10)\n",
      "Qinit 42\n",
      "Q choices are range(0, 10)\n",
      "Qinit 43\n",
      "Q choices are range(0, 10)\n",
      "Qinit 44\n",
      "Q choices are range(0, 10)\n",
      "Qinit 45\n",
      "Q choices are range(0, 10)\n",
      "Qinit 46\n",
      "Q choices are range(0, 10)\n",
      "Qinit 47\n",
      "Q choices are range(0, 10)\n",
      "Qinit 48\n",
      "Q choices are range(0, 10)\n",
      "Qinit 49\n",
      "Q choices are range(0, 10)\n",
      "Qinit 50\n",
      "Q choices are range(0, 10)\n",
      "Qinit 51\n",
      "Q choices are range(0, 10)\n",
      "Qinit 52\n",
      "Q choices are range(0, 10)\n",
      "Qinit 53\n",
      "Q choices are range(0, 10)\n",
      "Qinit 54\n",
      "Q choices are range(0, 10)\n",
      "Qinit 55\n",
      "Q choices are range(0, 10)\n",
      "Qinit 56\n",
      "Q choices are range(0, 10)\n",
      "Qinit 57\n",
      "Q choices are range(0, 10)\n",
      "Qinit 58\n",
      "Q choices are range(0, 10)\n",
      "Qinit 59\n",
      "Q choices are range(0, 10)\n",
      "Qinit 60\n",
      "Q choices are range(0, 10)\n",
      "Qinit 61\n",
      "Q choices are range(0, 10)\n",
      "Qinit 62\n",
      "Q choices are range(0, 10)\n",
      "Qinit 63\n",
      "Q choices are range(0, 10)\n",
      "Qinit 64\n",
      "Q choices are range(0, 10)\n",
      "Qinit 65\n",
      "Q choices are range(0, 10)\n",
      "Qinit 66\n",
      "Q choices are range(0, 10)\n",
      "Qinit 67\n",
      "Q choices are range(0, 10)\n",
      "Qinit 68\n",
      "Q choices are range(0, 10)\n",
      "Qinit 69\n",
      "Q choices are range(0, 10)\n",
      "Qinit 70\n",
      "Q choices are range(0, 10)\n",
      "Qinit 71\n",
      "Q choices are range(0, 10)\n",
      "Qinit 72\n",
      "Q choices are range(0, 10)\n",
      "Qinit 73\n",
      "Q choices are range(0, 10)\n",
      "Qinit 74\n",
      "Q choices are range(0, 10)\n",
      "Qinit 75\n",
      "Q choices are range(0, 10)\n",
      "Qinit 76\n",
      "Q choices are range(0, 10)\n",
      "Qinit 77\n",
      "Q choices are range(0, 10)\n",
      "Qinit 78\n",
      "Q choices are range(0, 10)\n",
      "Qinit 79\n",
      "Q choices are range(0, 10)\n",
      "Qinit 80\n",
      "Q choices are range(0, 10)\n",
      "Qinit 81\n",
      "Q choices are range(0, 10)\n",
      "Qinit 82\n",
      "Q choices are range(0, 10)\n",
      "Qinit 83\n",
      "Q choices are range(0, 10)\n",
      "Qinit 84\n",
      "Q choices are range(0, 10)\n",
      "Qinit 85\n",
      "Q choices are range(0, 10)\n",
      "Qinit 86\n",
      "Q choices are range(0, 10)\n",
      "Qinit 87\n",
      "Q choices are range(0, 10)\n",
      "Qinit 88\n",
      "Q choices are range(0, 10)\n",
      "Qinit 89\n",
      "Q choices are range(0, 10)\n",
      "Qinit 90\n",
      "Q choices are range(0, 10)\n",
      "Qinit 91\n",
      "Q choices are range(0, 10)\n",
      "Qinit 92\n",
      "Q choices are range(0, 10)\n",
      "Qinit 93\n",
      "Q choices are range(0, 10)\n",
      "Qinit 94\n",
      "Q choices are range(0, 10)\n",
      "Qinit 95\n",
      "Q choices are range(0, 10)\n",
      "Qinit 96\n",
      "Q choices are range(0, 10)\n",
      "Qinit 97\n",
      "Q choices are range(0, 10)\n",
      "Qinit 98\n",
      "Q choices are range(0, 10)\n",
      "Qinit 99\n",
      "Q choices are range(0, 10)\n",
      "Q:0, difficulty=8.78 across 10 components\n",
      "Q:1, difficulty=7.91 across 10 components\n",
      "Q:2, difficulty=5.86 across 10 components\n",
      "Q:3, difficulty=7.44 across 10 components\n",
      "Q:4, difficulty=7.78 across 10 components\n",
      "Q:5, difficulty=7.99 across 10 components\n",
      "Q:6, difficulty=8.54 across 10 components\n",
      "Q:7, difficulty=8.08 across 10 components\n",
      "Q:8, difficulty=8.57 across 10 components\n",
      "Q:9, difficulty=7.14 across 10 components\n",
      "Q:10, difficulty=7.38 across 10 components\n",
      "Q:11, difficulty=7.89 across 10 components\n",
      "Q:12, difficulty=6.84 across 10 components\n",
      "Q:13, difficulty=9.06 across 10 components\n",
      "Q:14, difficulty=8.16 across 10 components\n",
      "Q:15, difficulty=7.93 across 10 components\n",
      "Q:16, difficulty=6.72 across 10 components\n",
      "Q:17, difficulty=8.02 across 10 components\n",
      "Q:18, difficulty=7.31 across 10 components\n",
      "Q:19, difficulty=8.15 across 10 components\n",
      "Q:20, difficulty=8.33 across 10 components\n",
      "Q:21, difficulty=8.13 across 10 components\n",
      "Q:22, difficulty=7.25 across 10 components\n",
      "Q:23, difficulty=7.97 across 10 components\n",
      "Q:24, difficulty=7.45 across 10 components\n",
      "Q:25, difficulty=8.91 across 10 components\n",
      "Q:26, difficulty=7.54 across 10 components\n",
      "Q:27, difficulty=9.23 across 10 components\n",
      "Q:28, difficulty=7.07 across 10 components\n",
      "Q:29, difficulty=7.90 across 10 components\n",
      "Q:30, difficulty=7.63 across 10 components\n",
      "Q:31, difficulty=8.07 across 10 components\n",
      "Q:32, difficulty=8.75 across 10 components\n",
      "Q:33, difficulty=8.23 across 10 components\n",
      "Q:34, difficulty=8.09 across 10 components\n",
      "Q:35, difficulty=7.93 across 10 components\n",
      "Q:36, difficulty=8.79 across 10 components\n",
      "Q:37, difficulty=8.44 across 10 components\n",
      "Q:38, difficulty=7.85 across 10 components\n",
      "Q:39, difficulty=7.71 across 10 components\n",
      "Q:40, difficulty=7.03 across 10 components\n",
      "Q:41, difficulty=7.69 across 10 components\n",
      "Q:42, difficulty=8.92 across 10 components\n",
      "Q:43, difficulty=7.72 across 10 components\n",
      "Q:44, difficulty=7.69 across 10 components\n",
      "Q:45, difficulty=7.94 across 10 components\n",
      "Q:46, difficulty=7.08 across 10 components\n",
      "Q:47, difficulty=7.44 across 10 components\n",
      "Q:48, difficulty=6.92 across 10 components\n",
      "Q:49, difficulty=8.30 across 10 components\n",
      "Q:50, difficulty=8.44 across 10 components\n",
      "Q:51, difficulty=7.60 across 10 components\n",
      "Q:52, difficulty=8.30 across 10 components\n",
      "Q:53, difficulty=7.57 across 10 components\n",
      "Q:54, difficulty=7.53 across 10 components\n",
      "Q:55, difficulty=7.78 across 10 components\n",
      "Q:56, difficulty=8.27 across 10 components\n",
      "Q:57, difficulty=8.60 across 10 components\n",
      "Q:58, difficulty=8.76 across 10 components\n",
      "Q:59, difficulty=7.99 across 10 components\n",
      "Q:60, difficulty=7.20 across 10 components\n",
      "Q:61, difficulty=6.94 across 10 components\n",
      "Q:62, difficulty=8.41 across 10 components\n",
      "Q:63, difficulty=8.61 across 10 components\n",
      "Q:64, difficulty=7.73 across 10 components\n",
      "Q:65, difficulty=7.96 across 10 components\n",
      "Q:66, difficulty=6.56 across 10 components\n",
      "Q:67, difficulty=7.35 across 10 components\n",
      "Q:68, difficulty=8.22 across 10 components\n",
      "Q:69, difficulty=9.35 across 10 components\n",
      "Q:70, difficulty=6.61 across 10 components\n",
      "Q:71, difficulty=7.76 across 10 components\n",
      "Q:72, difficulty=7.59 across 10 components\n",
      "Q:73, difficulty=8.54 across 10 components\n",
      "Q:74, difficulty=8.37 across 10 components\n",
      "Q:75, difficulty=8.00 across 10 components\n",
      "Q:76, difficulty=6.68 across 10 components\n",
      "Q:77, difficulty=7.33 across 10 components\n",
      "Q:78, difficulty=6.54 across 10 components\n",
      "Q:79, difficulty=9.21 across 10 components\n",
      "Q:80, difficulty=8.50 across 10 components\n",
      "Q:81, difficulty=7.52 across 10 components\n",
      "Q:82, difficulty=8.39 across 10 components\n",
      "Q:83, difficulty=7.88 across 10 components\n",
      "Q:84, difficulty=8.46 across 10 components\n",
      "Q:85, difficulty=8.52 across 10 components\n",
      "Q:86, difficulty=7.95 across 10 components\n",
      "Q:87, difficulty=8.58 across 10 components\n",
      "Q:88, difficulty=6.59 across 10 components\n",
      "Q:89, difficulty=8.46 across 10 components\n",
      "Q:90, difficulty=8.92 across 10 components\n",
      "Q:91, difficulty=7.90 across 10 components\n",
      "Q:92, difficulty=8.42 across 10 components\n",
      "Q:93, difficulty=8.76 across 10 components\n",
      "Q:94, difficulty=8.95 across 10 components\n",
      "Q:95, difficulty=5.57 across 10 components\n",
      "Q:96, difficulty=7.82 across 10 components\n",
      "Q:97, difficulty=7.96 across 10 components\n",
      "Q:98, difficulty=6.59 across 10 components\n",
      "Q:99, difficulty=7.18 across 10 components\n",
      "qid 0 [2.306042159684443, 3.6821784427167996, 2.9155579142958628, 1.2915926724128273, 1.8255566884070848, 1.560193823397913, 3.479055370279383, 3.588619204035515, 3.1164587945400863, 2.766190959564599]\n",
      "qid 1 [2.8556133197957827, 1.345100072190827, 2.7651011258016176, 3.6799781282107817, 1.8006263235889768, 2.2791157221547573, 1.7116394890673965, 1.7325957159293908, 3.5280329941237696, 2.1404918118390612]\n",
      "qid 2 [2.602646862161081, 1.3629155023340198, 2.4398213280497068, 2.0916264463091023, 1.6272072624152436, 2.151231412248226, 1.2796944563711607, 1.09998151720347, 1.0738324032405977, 2.012420850170412]\n",
      "qid 3 [2.8039655981382037, 2.754828253512952, 3.040748319832234, 1.5951712757584384, 2.4537369204279957, 2.6285389273936093, 1.8472211089666188, 1.3242746437392012, 2.030588783493644, 2.4208658625852095]\n",
      "qid 4 [2.801093413099766, 3.300898769614694, 1.2013178093094592, 1.8829751257938985, 1.5471153341188046, 2.5163665937813064, 2.57654567382417, 1.5476947735237898, 3.537907493568438, 2.555067895184564]\n",
      "qid 5 [2.7319631025105395, 3.112013398483436, 2.1527353032771646, 1.2819926528958316, 3.2584490477399837, 2.3440657280523487, 1.786893067658287, 1.6110710576080454, 2.658662397636304, 3.384262633159555]\n",
      "qid 6 [2.87389741755265, 3.6265856804986285, 3.4568349396872695, 2.747621318154415, 1.905136245682686, 2.03959885224915, 2.427127958487063, 1.890009707363194, 1.6344699781864485, 3.4720821637789285]\n",
      "qid 7 [3.565185498903194, 1.7481630812592526, 2.6200336936561515, 2.163598530373659, 1.558062264460208, 3.4834763164621347, 2.6294602317611915, 1.41359876388095, 2.9219356603870885, 2.4300799705640874]\n",
      "qid 8 [1.3983184603383092, 1.006693661713737, 1.3478530322367974, 2.770397005024563, 3.6640018510985564, 3.6806309798694126, 3.6157433562718486, 2.6695206459576646, 2.5635757335150924, 2.7047039209884844]\n",
      "qid 9 [2.8012855095983262, 2.705055785913294, 1.096159282880176, 2.2139090205790684, 1.479394267485704, 3.0319979133995507, 1.5452039326231184, 2.1782799218053865, 3.1749010600008623, 1.0632895579472603]\n",
      "qid 10 [3.483460984846791, 2.6652665587939843, 2.1255528975176556, 3.2978571103142946, 1.353355918670859, 1.072196652011705, 1.0477677628393685, 2.100866995532538, 1.7850916566005532, 2.8583963327760786]\n",
      "qid 11 [2.0812388089880898, 2.2676117470352626, 1.2506211612586964, 2.2828767437102577, 2.742882951775451, 3.3665583999785835, 2.384132131387328, 2.787825649223322, 3.4739925779654524, 1.302056227699495]\n",
      "qid 12 [2.015868039526401, 1.9967056486196442, 2.067525168294084, 1.5562031195100516, 2.812843970487079, 1.7526269779675516, 2.7015061320152265, 1.3973179494081125, 2.0752692329978295, 2.746894096705267]\n",
      "qid 13 [3.489961529496786, 1.1496516684154066, 3.6780134915805567, 3.136168536715264, 2.3099537741349474, 3.4741424270698453, 3.759093689433712, 2.4177182769172223, 1.9641478311146447, 2.0087055132621785]\n",
      "qid 14 [2.059295041987582, 1.4145687041889832, 3.270249264084404, 1.4435785753129882, 2.9294024951483744, 3.393931088718302, 3.610611113008536, 1.003166901559339, 1.9943129001060598, 3.080483014380156]\n",
      "qid 15 [1.5603908368342279, 2.7340355881939002, 1.7484348821224427, 2.7068969357917885, 3.3500536429297814, 2.518231181799391, 2.09985022045111, 1.7620931877387846, 2.815332398245406, 3.0858584101068125]\n",
      "qid 16 [1.5496072699026153, 2.6858869126705147, 1.4586548370384869, 1.8788064124603134, 1.4543920942424016, 1.3107561141787536, 3.3076450680254186, 2.410661580281606, 2.261949550065542, 2.0578805615830786]\n",
      "qid 17 [3.545275308111685, 1.1097597390773914, 3.0252076145820275, 1.5812944576576844, 1.6116871412331044, 1.3675395726508852, 3.4218423738691226, 3.009307100477474, 3.1785286816826233, 1.887286727562688]\n",
      "qid 18 [1.2478364264861723, 1.6744498128934562, 1.8335552552352814, 2.8670586769146778, 2.0420136888212586, 1.8127415185056779, 3.0256978226034392, 3.6028393061024415, 1.4064715409214694, 2.434308620868346]\n",
      "qid 19 [2.1196432009319404, 1.0782411584867877, 1.2195256680299642, 1.0896474316041473, 3.688845702018799, 3.683850294391846, 1.7203904323499817, 3.1717168359992516, 3.1315986896740786, 2.8580552137065887]\n",
      "qid 20 [3.487897985793506, 2.2939439201407117, 3.286162168045935, 2.993670672947437, 1.2609278574621565, 1.3880099997944653, 2.2140947704375384, 3.5110359565730773, 1.7945057422821027, 2.8550481162339203]\n",
      "qid 21 [3.199687151383965, 2.501452871399111, 3.301917318637831, 2.7375527091063496, 2.1973296005070058, 3.176963502075402, 3.042583673863927, 1.6936096111937817, 1.4681097148435702, 1.4344752826290268]\n",
      "qid 22 [2.6412833025678735, 2.0895643355446865, 1.2110067744261008, 3.7060052663018284, 2.8965262048717157, 2.0664361123630406, 2.0632699104254595, 1.8474840256900413, 1.0738865999253135, 2.1456526614070586]\n",
      "qid 23 [1.7020812351939785, 3.6619148314120356, 1.406030214959677, 3.487883392909868, 1.4279024984485174, 1.8645341895819108, 2.1403884359771066, 2.431637459192706, 2.878730565406225, 2.969348510367532]\n",
      "qid 24 [2.8568257589163943, 1.34023542013491, 2.499975938919154, 2.7832448621491617, 1.7321988076243309, 1.4933361027638274, 1.1562741462780128, 3.6013082177207556, 1.149431647412636, 3.263118819928334]\n",
      "qid 25 [3.02121849806065, 3.2208219840760144, 2.6506388462633153, 3.6944629158177023, 2.279211571660219, 2.2929316521695564, 1.2596026699784397, 2.443068799074272, 2.8084960293081433, 3.6492162085192774]\n",
      "qid 26 [2.0639845863272406, 2.3301695961160673, 2.220053308760188, 2.8028259103291226, 1.5513197879316036, 3.1646742763970552, 2.150653398063577, 2.3145514902695625, 2.816958647457679, 2.004363845410268]\n",
      "qid 27 [3.2104952720993376, 2.4763515139518057, 3.488259307819792, 1.8480512084026697, 2.7522194722139584, 2.842308153395776, 2.3134676058330856, 3.7556647606447404, 3.022127809765827, 2.998276548030929]\n",
      "qid 28 [1.6153559057902858, 3.7070072413465684, 1.9825858545791026, 2.3523142913184074, 1.4853265004816676, 1.7970640154107973, 1.323206866696899, 2.919284429348752, 2.2947528784126296, 1.7729790907534526]\n",
      "qid 29 [3.6761277861973687, 3.000447665007063, 2.6803929565733804, 2.2937416168441853, 1.8720363872233396, 1.900294629205546, 2.5848827372425562, 2.932498700688263, 1.6064697498658886, 1.584929989048747]\n",
      "qid 30 [1.8543217653980206, 1.3732605880541706, 2.0629873805921384, 3.2012076732083563, 3.2135555191871066, 2.9910100102563577, 1.7485555867569913, 2.38063066864683, 3.0024144484510105, 1.1818656561236984]\n",
      "qid 31 [2.335650904909733, 2.1066571573547557, 2.044386140770797, 1.861583598840193, 3.2520136837278333, 2.9610751248261176, 3.05684881711522, 2.4907331989115726, 3.083249367486999, 1.79434922949645]\n",
      "qid 32 [2.51947159842637, 3.523419331002799, 1.4471534446723373, 2.841018322611225, 3.600155171003233, 3.3924574961658327, 1.5956887410282885, 3.4813546289080137, 2.66466952407087, 1.2043813399445065]\n",
      "qid 33 [2.7093375106997577, 3.048177154710094, 3.2155936784767, 2.7886410145558673, 2.5989830572591384, 1.6282580542475427, 1.9418041615580874, 2.9148336225820897, 2.599443631580491, 2.149848854436623]\n",
      "qid 34 [1.548997758127419, 2.5420885296415516, 2.973124549942192, 3.4534536713761446, 3.46429626086494, 2.1970643909111107, 1.4357477046641827, 2.7907657127901038, 2.5010173309792556, 1.6830179502426803]\n",
      "qid 35 [3.722937868958365, 1.4078593661548875, 2.72787771742634, 1.1117049922119544, 1.8688396219269674, 3.3410260169785007, 2.374690471306334, 1.313089762229432, 3.203554348723471, 2.455805054640341]\n",
      "qid 36 [1.5804673596638437, 3.746347071505167, 2.874156304296089, 2.0339903516472413, 3.1792642121837202, 1.8172204738079554, 3.6410535689455816, 2.2365028551673323, 3.523479436566666, 2.0452577497708275]\n",
      "qid 37 [2.7449495271250406, 3.1040682490777467, 1.9598055244321486, 3.2917603367236237, 1.1010881108747723, 2.121992960591836, 3.1777867124737433, 1.1335642950623859, 3.1766530782893083, 3.493030906040279]\n",
      "qid 38 [1.4012286397641933, 3.617942486976853, 1.1862416021395181, 2.783600668165159, 1.6390534012728184, 2.6323821328347616, 3.495527026977063, 2.0976565587559715, 3.1054259774370134, 1.2366047193805763]\n",
      "qid 39 [1.887334893134621, 3.6043191687271956, 1.6576994456089842, 2.0607479502745427, 3.6225454405644335, 2.966931539237093, 2.0039240094257424, 1.6818401999673867, 1.1202137274414288, 2.4251437207365463]\n",
      "qid 40 [2.7546361238454544, 1.5332587034498713, 1.5713793373591374, 2.4190433739895054, 1.3056457192528403, 2.354550775320084, 2.4675054899160584, 1.5055880703026925, 1.7053175582821913, 3.550918831692335]\n",
      "qid 41 [2.9205985663996277, 2.2835602888446997, 3.463268271913478, 1.1679913576218532, 1.1325378034443372, 1.3161391569058567, 2.399670646651296, 1.9020182269787247, 3.2377299145738885, 3.0304957029119497]\n",
      "qid 42 [1.4157353698456503, 3.0938873478500275, 2.5297299913599973, 3.4672918707937663, 2.224344136658317, 3.1419907087723953, 3.61715298146818, 3.5344126934216393, 1.3349782708287075, 2.7329440868253694]\n",
      "qid 43 [2.3882572993237607, 3.4482899092764563, 2.2196830294406, 2.317019383968157, 1.4942175689625357, 1.5951534900048787, 3.062687912794813, 3.490555425306559, 1.101210501161015, 2.0204645894865862]\n",
      "qid 44 [1.0238477413715643, 2.003480375914056, 3.4094890059294016, 3.084783620319397, 1.4476678755701693, 2.810006178662915, 1.9839236602140649, 1.3489083248789964, 3.6341711090195705, 1.9840759444585956]\n",
      "qid 45 [1.6825704484190975, 1.9555521985268152, 2.135760861699156, 2.925165543711298, 2.297756132159755, 2.374662113957199, 3.6826913800238184, 2.9718213308358665, 2.770280974690685, 1.487277593259712]\n",
      "qid 46 [3.7222005808436247, 2.904247513671148, 1.3872524101297303, 1.0137040815007834, 1.951586256112719, 1.121482822084162, 2.996218852348913, 1.535419326453989, 2.5252807231980388, 1.4426854338689477]\n",
      "qid 47 [2.8598520451600318, 1.4676045380496, 2.6593358105757323, 1.9312729839815939, 1.6843744493885096, 2.6330625979067492, 1.6606663731750237, 3.078947039917229, 1.8515490742300433, 2.970425850377171]\n",
      "qid 48 [2.9758846854121, 1.767432203449191, 1.8231081543730987, 2.7058808245169406, 1.86034540290355, 2.3235496323503293, 2.95696334117687, 2.2629153415775, 1.0175692702977732, 1.20216367386251]\n",
      "qid 49 [3.664289535555449, 3.483660142451908, 1.2705878214691055, 3.095253896676263, 2.3277507948153495, 3.2587842279196364, 1.0299985035642134, 2.2636948943488813, 2.9133371330050517, 1.1870774059050926]\n",
      "qid 50 [3.618670932517682, 2.1638284804238865, 1.8663290245745208, 2.6233981266759265, 2.1949366709082114, 3.360014738631979, 1.3707365496695285, 3.0202670854482725, 2.6668637575297995, 2.9924138104838933]\n",
      "qid 51 [1.481412182288955, 2.193893110557247, 2.23843108716191, 1.083110647980374, 3.0556092744471948, 3.227811915839277, 1.3901494436224864, 3.1995773328433703, 1.7360272724998296, 3.109100491887066]\n",
      "qid 52 [2.6091030731962825, 1.8773891942052425, 2.538905653618042, 3.6506402651839065, 2.6905607970262952, 2.285775814233104, 1.013601935625505, 3.735134436753094, 3.1849727694507766, 1.1052769283190638]\n",
      "qid 53 [3.293607229073322, 1.6756816241713088, 1.7933000313861192, 1.0979948118795835, 1.2876296937119998, 3.2264070329027845, 1.4454926351135193, 3.2685088229326005, 2.7262904842384774, 2.6247793687082877]\n",
      "qid 54 [2.1765180465388996, 2.809787387346134, 1.1139108938724085, 1.1296496347598861, 2.179727047352017, 3.2798342869268993, 1.2188150203650996, 3.3424041282280115, 3.254897935353422, 1.6566493774437951]\n",
      "qid 55 [3.461918014816816, 3.1037409565765977, 1.2872487536438957, 3.287240407716039, 1.6676485128827268, 2.983038896283454, 2.4025651781681745, 1.1518262241141866, 1.777546943065414, 2.1366552595267523]\n",
      "qid 56 [3.666434251865439, 1.4981632402740097, 3.162394937220342, 2.289780463326334, 2.493343691510467, 3.6711975799052476, 2.297277407673768, 2.7816373237756777, 1.7135308306921448, 1.33092896988972]\n",
      "qid 57 [2.6972053866846433, 1.9714885884482265, 1.7819386981085286, 2.712161808286491, 2.2736849234114187, 3.5156405492150835, 2.9575440969689897, 3.300210106442887, 1.8840840013585765, 3.3881944808804727]\n",
      "qid 58 [3.7088496473976194, 1.943663543913881, 2.9401982642525075, 3.3772346835352427, 2.4641115442504082, 2.020630640731838, 3.1904524592780286, 1.8822972056935874, 2.9816679499713095, 2.5297301565781734]\n",
      "qid 59 [2.6503596532704226, 2.059684402984911, 3.6228873157487906, 1.2961300151788475, 1.81988384282134, 2.192065174253706, 3.2729470845665536, 3.1113532010036447, 2.603341199248567, 1.5510610524631858]\n",
      "qid 60 [3.6066789062765223, 2.315906917734787, 2.75583921284678, 1.0177423348317274, 1.619592503777863, 3.464670355915357, 1.6612733830266402, 1.4856503861591428, 1.4789342978818816, 1.758649920749785]\n",
      "qid 61 [3.2727929072547286, 1.1329226782397002, 2.5623615361684555, 1.1571419932530058, 3.193432608381274, 1.8524084754294896, 1.3183357535103262, 1.5415551805647918, 1.505052656386329, 2.8840720094514873]\n",
      "qid 62 [2.6142244252222424, 3.7459568131064467, 2.7364928834601896, 2.9354749156380864, 2.2237393608781866, 1.5820812299161284, 1.8062549867793571, 3.559372005140243, 2.9736608975385526, 1.2057261049957335]\n",
      "qid 63 [2.2029252232883216, 2.175120038037231, 1.0371387979183635, 3.1557078514071923, 2.930629268012308, 1.6562052709220485, 3.6984198007638023, 3.289860584724278, 3.776477746200234, 1.8409225472496324]\n",
      "qid 64 [3.6005612923366543, 3.111345722680832, 1.0021347998435068, 2.378868138999694, 2.440266774219733, 1.7915026528106481, 1.6679392252521954, 2.9757995576071234, 2.1944873472735296, 2.20927914908661]\n",
      "qid 65 [3.5276034993276393, 2.685623755979844, 2.212557293142991, 3.072591829179041, 2.8066212312050913, 2.020282945111272, 2.338978005232705, 1.8476321602826689, 2.1941960594939536, 1.919823435303568]\n",
      "qid 66 [1.0478308393800146, 1.8376923315952585, 1.859028212060906, 2.028511694090761, 3.0497988837629078, 1.8968214838674213, 2.2669030872974605, 2.2632414858608927, 1.0291546848536257, 2.600622689499013]\n",
      "qid 67 [1.8237140455780803, 1.6538864689431603, 1.5538805825017827, 3.350539447973664, 2.983896866399542, 1.716184695643412, 2.5167894742333035, 1.6950076012002224, 2.3289497378992525, 2.812483539855532]\n",
      "qid 68 [3.4208704532215126, 2.5008750119322, 2.8284749330678247, 3.2165328481122066, 2.4034634178222856, 1.365832737701151, 3.0294160107497947, 1.3648793408232416, 2.594017529642568, 2.41569270411048]\n",
      "qid 69 [1.5965529524655362, 3.523961618551683, 2.6475334603039586, 3.1500657355341555, 2.915057593160524, 3.0912484657256587, 3.4630025486301883, 2.9210106146502115, 2.260373021130322, 3.4342702919973873]\n",
      "qid 70 [1.3159338183741196, 3.7628389155205664, 1.8338996191913772, 2.0774896365861153, 1.2051164126730356, 1.9636262275498795, 2.546430509304094, 1.5745448061516627, 1.7806843960300371, 1.6474007756955458]\n",
      "qid 71 [2.8557636363115027, 1.7162067851006544, 2.144434269666821, 3.065638381377171, 2.3695072340451073, 2.2315097331795077, 2.9367837255367086, 2.43730437932627, 1.4801899397219596, 2.7983743519662223]\n",
      "qid 72 [1.1205584039317016, 2.7605646531569157, 3.5174814084718014, 2.499614818851302, 3.0035764250244177, 1.5588092812041412, 1.2695422885639047, 1.5345667360078592, 3.2750183404442454, 1.98856598312764]\n",
      "qid 73 [2.9609117809072623, 2.279542372977791, 3.638365242832251, 3.058138472524328, 3.363518604353265, 1.8962312798826497, 1.0402733496740741, 1.569147584808606, 3.568643940991454, 2.266077700677945]\n",
      "qid 74 [2.9266945603282064, 1.6142425951633177, 1.199270574829914, 1.2391407705715296, 3.6997804377148014, 2.14287048201475, 3.6792360782233713, 2.773798927588077, 2.9534779493964116, 2.773376304441482]\n",
      "qid 75 [1.4229553850469525, 3.2222468493661345, 2.8518660349803104, 1.324800690406232, 2.102505149873483, 1.4603955415040617, 2.7861329609969125, 1.4093752421541836, 3.74372783252072, 3.373407041952786]\n",
      "qid 76 [1.7614934083583065, 2.497752914354387, 1.9204061167230275, 1.4777325912795445, 2.479156070559525, 2.2547693909050373, 1.7962885376855071, 1.9859291506254952, 2.6897301133243396, 1.9488972327567664]\n",
      "qid 77 [1.2703940892340013, 1.409257532684196, 2.4170544754105228, 1.7533407680778157, 1.6659214154035897, 3.130744581641892, 2.628785049897169, 1.2181682499242996, 2.594992790742525, 3.6668987572351486]\n",
      "qid 78 [1.5708602912264036, 2.812704525397457, 1.477124459467333, 1.6300797841648, 1.6323840891936072, 2.992633163891165, 1.7228602731077702, 2.885091315341657, 1.1165412361989455, 1.8346838249529867]\n",
      "qid 79 [2.972603711983788, 1.355433782327761, 2.1552353924345002, 3.7719472395859195, 2.569419332596124, 3.441463402938125, 3.5036626274966363, 3.627820465054886, 3.1820656325853203, 1.1129051035476563]\n",
      "qid 80 [2.762127579709123, 1.2910838241275704, 1.9249469668556234, 2.582669781149453, 3.671932506031164, 2.994707103129759, 3.0686400302522245, 2.377934665074553, 1.449836064969031, 3.5951807184969398]\n",
      "qid 81 [2.147203608798576, 2.4638954569911133, 1.7065995584614022, 2.150787092362747, 2.982398462361048, 1.499567038999768, 2.602332145934279, 1.1843331048410302, 3.130280740684807, 3.043747969492432]\n",
      "qid 82 [2.5334627263486387, 2.3793046430418157, 2.7840017381066837, 1.4160469155033066, 2.642083514104342, 2.945778877614459, 3.4199350525314696, 1.37710352635914, 3.545310205483333, 2.5916493930294355]\n",
      "qid 83 [3.6974049295597946, 3.4161281302194344, 1.5831921934073092, 2.1673424903502987, 2.3240458937859105, 1.607286800102278, 2.328373958446628, 2.334921515110362, 1.9265438716386674, 2.64492921514159]\n",
      "qid 84 [3.224891440204857, 2.0516823198153, 2.4900098814675458, 1.8053735273297662, 2.61901181581033, 2.3388927109322, 3.7698208558150417, 1.5248724954328188, 3.4148236314868994, 2.655965689524689]\n",
      "qid 85 [1.6444742984608784, 3.78083115216316, 2.322537369668529, 2.1935230192171966, 3.4810073900980307, 3.1235469473739186, 2.158865481401235, 1.7170162650032896, 2.4028329631881036, 3.192008397784014]\n",
      "qid 86 [1.1798075809889301, 2.086784381819432, 3.117672727327453, 1.1554217009445, 3.1272442603936454, 2.757250780909544, 3.175050044552636, 1.9573944238382832, 1.2105424222593626, 3.698507051326094]\n",
      "qid 87 [1.4680324789869297, 3.6696306767747653, 1.802366666224002, 3.7190581066195167, 2.9864073105192963, 3.268171470667216, 1.435678001443097, 3.4923052648888855, 2.0183221551709627, 1.7389868980993246]\n",
      "qid 88 [1.724926259278364, 1.2441134379292178, 2.601502944904098, 1.597233307330209, 1.6595009806128918, 2.3716004926446566, 1.7632018149302682, 2.6255032746913898, 1.9913062384969802, 2.6953931127400357]\n",
      "qid 89 [2.508283280315078, 2.973566263798914, 1.787284414887551, 2.9703380395476624, 3.471900019823148, 2.71395680165661, 3.0142806669373057, 1.3265434847330149, 1.3945069811577255, 3.4911363612105464]\n",
      "qid 90 [2.4691728877235395, 1.2408830908715665, 3.457860059175621, 1.610078464366398, 3.719390667028667, 3.4114518303089456, 3.466687444029946, 2.2033443093418996, 3.120845693621777, 2.292768088671247]\n",
      "qid 91 [1.907185657806417, 2.651970608536465, 1.5255696870613678, 3.6956993300395395, 1.4493713612728971, 3.6300293819775495, 1.4180433821942322, 1.7731268464673313, 2.6069825818096106, 2.932666057034428]\n",
      "qid 92 [3.281101212106065, 2.539181616197936, 3.011049727421603, 1.1079198251366071, 2.002086886812463, 2.254251840063124, 2.7818123162347503, 3.3844609179117895, 1.2672723402860495, 3.677427549570704]\n",
      "qid 93 [1.8559670451433528, 3.008783819910935, 3.566329467365898, 1.1568118392043494, 2.658730960485751, 3.3765265944846306, 2.4370223048963116, 3.776367338920893, 3.071161172336107, 1.4351130349703431]\n",
      "qid 94 [1.4809173431588325, 3.4978197420595776, 1.8651023476611042, 2.8509480481931035, 3.364408026368013, 2.9005799519931377, 2.9851884554927044, 1.3565334204183193, 3.1554872344041867, 3.687916079023048]\n",
      "qid 95 [2.179908264579674, 1.4302535524697582, 1.1491505055614915, 1.5538442578327365, 2.436821142068367, 1.1180252861561197, 1.0418815343571675, 1.43489054362023, 2.8813290609020643, 1.3812099265147322]\n",
      "qid 96 [2.68409664901403, 1.909884137464965, 1.1690819129979895, 1.5288928777631403, 3.0124071897928877, 1.2324090916994237, 3.5135349456473928, 1.1311392307818526, 3.307773984849608, 3.385612686891021]\n",
      "qid 97 [2.235514480972512, 1.425101116188871, 3.2691014225321338, 1.133840334603003, 3.2990949570240193, 3.564699027383668, 2.795220522390979, 2.197718388997588, 1.4429729737016752, 2.466578171493265]\n",
      "qid 98 [2.7141747405977465, 1.5240152444890174, 2.999391903189559, 1.0325481930908789, 2.431104956289603, 2.4345733733995347, 2.000816068118306, 2.0390885118023743, 1.0277636849954774, 1.6109471241139461]\n",
      "qid 99 [1.3410857481912775, 2.5177858206548063, 1.0466500880614118, 2.861349838635805, 2.6261682644727182, 3.2783708874404693, 2.2733786096396553, 1.5524280267410964, 2.8209515854823, 1.0056055807209778]\n",
      "CYR RORU , skill=21.67 across 10 comps\n",
      "THIMOH BAV , skill=24.00 across 10 comps\n",
      "FEMO WIV , skill=23.34 across 10 comps\n",
      "DYJUN SOWU , skill=26.58 across 10 comps\n",
      "FEK CHYHAB , skill=21.89 across 10 comps\n",
      "RUBUG NEG , skill=24.28 across 10 comps\n",
      "FEPHI PHEPH , skill=23.77 across 10 comps\n",
      "DYGU CEFU , skill=25.57 across 10 comps\n",
      "MUW VUN , skill=25.14 across 10 comps\n",
      "SAVO CACH , skill=27.07 across 10 comps\n",
      "FOS VIP , skill=29.71 across 10 comps\n",
      "FYKA COGOG , skill=27.05 across 10 comps\n",
      "POTYS GAD , skill=25.53 across 10 comps\n",
      "TOG YAR , skill=28.78 across 10 comps\n",
      "FOKA DOGA , skill=23.83 across 10 comps\n",
      "VETH YECHO , skill=26.75 across 10 comps\n",
      "CHIC DEDIH , skill=24.58 across 10 comps\n",
      "KOV CHULU , skill=24.08 across 10 comps\n",
      "CHIMY PHOBE , skill=23.63 across 10 comps\n",
      "WYYO PUNI , skill=20.33 across 10 comps\n",
      "MARIF TIGUV , skill=23.63 across 10 comps\n",
      "THUY SIDA , skill=29.36 across 10 comps\n",
      "BOLED VYJIW , skill=25.06 across 10 comps\n",
      "PHIKU TYTHA , skill=26.76 across 10 comps\n",
      "VOWAR YUTU , skill=26.97 across 10 comps\n",
      "PECI COP , skill=22.95 across 10 comps\n",
      "DAG DOM , skill=28.29 across 10 comps\n",
      "TYHY MULA , skill=19.54 across 10 comps\n",
      "YUBEF NATHOC , skill=22.71 across 10 comps\n",
      "THIHOL HYJUD , skill=20.69 across 10 comps\n",
      "----0\n",
      "\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "passed 50841/100000\n",
      "98000\n"
     ]
    }
   ],
   "source": [
    "    from keras.optimizers import Adam, SGD\n",
    "    numpy.random.seed(666)\n",
    "    import sklearn\n",
    "    import math\n",
    "\n",
    "\n",
    "#     minval= 0\n",
    "    offset = 1\n",
    "#     maxval = 100\n",
    "    minval = -math.inf\n",
    "    maxval = math.inf\n",
    "    \n",
    "    beta_min = 0\n",
    "    beta_max = 100\n",
    "    theta_min = 0\n",
    "    theta_max = 100\n",
    "    \n",
    "#     minb,maxb,mina,maxa =(1.0, 5.327592330763304, 6.568865477642566, 15.754823559394731)\n",
    "#     minb,maxb,mina,maxa =(1.0, 4.893499423485679, 6.060544209353401, 16.009594538893413) #100\n",
    "#     minb,maxb,mina,maxa =(1.0, 3.96, 4.86, 13.36) #50?\n",
    "#     minb,maxb,mina,maxa =(1.0, 6.744937439353762, 6.560468126136105, 13.903555808574247) #30\n",
    "    minb,maxb,mina,maxa =(1.0, 3.7812870150169404, 2.691968403956818, 12.165240419546407) # 10?\n",
    "\n",
    "    dim=10\n",
    "    inv_sigmoid = lambda p: -numpy.log(1/p - 1)\n",
    "    p50 = inv_sigmoid( 0.5**(1/dim) )\n",
    "    p99 = inv_sigmoid( 0.999**(1/dim) )\n",
    "\n",
    "#     maxval= offset+p99\n",
    "    av = (minval+maxval)/2\n",
    "    print(\"p999\", p99)\n",
    "    print(\"p50\", p50)\n",
    "    print(\"max\", maxval)\n",
    "    print(\"av\",av)\n",
    "    \n",
    "\n",
    "    n_blobs = 0\n",
    "    \n",
    "    n_qs = 100\n",
    "    n_students = 1000\n",
    "\n",
    "    n_traits = dim\n",
    "    nnw = n_traits\n",
    "    min_active_traits = n_traits\n",
    "    max_active_traits = n_traits\n",
    "    \n",
    "    qs= create_qs(n_qs, minb,maxb, n_traits, min_active_traits, max_active_traits)\n",
    "    ss= create_students(n_students, mina,maxa, n_traits)\n",
    "    \n",
    "    attempts, attempts_by_q, attempts_by_psi, attempts_n_map = generate_attempts(qs,ss)\n",
    "        \n",
    "    attempts = numpy.array(attempts)\n",
    "#     filter_ixs = numpy.random.choice(all_ixs, size=len(attempts), replace=False) #len(attempts)//100, replace=False)\n",
    "    \n",
    "    all_ixs = range(len(attempts))\n",
    "    val_ixs = numpy.random.choice(all_ixs, size=2*int(min(len(attempts)/10,1000)), replace=False) #len(attempts)//100, replace=False)\n",
    "    trn_ixs = list(set(all_ixs) - set(val_ixs))\n",
    "    test_ixs = val_ixs[0:len(val_ixs)//2]\n",
    "    val_ixs = val_ixs[len(val_ixs)//2:]\n",
    "    \n",
    "    sz = attempts[trn_ixs, 0]\n",
    "    qz = attempts[trn_ixs, 1]\n",
    "    orig_pfz = attempts[trn_ixs, 2]\n",
    "    pfz = orig_pfz\n",
    "    print(len(pfz))\n",
    "#     pfz = numpy.where(orig_pfz < 0.5, 2000, 0).astype(\"float128\")\n",
    "\n",
    "    vsz = attempts[val_ixs, 0]\n",
    "    vqz = attempts[val_ixs, 1]\n",
    "    orig_vpfz = attempts[val_ixs, 2]\n",
    "    vpfz = orig_vpfz\n",
    "#     vpfz = numpy.where(orig_vpfz < 0.5, 2000, 0).astype(\"float128\")\n",
    "\n",
    "    tsz = attempts[test_ixs, 0]\n",
    "    tqz = attempts[test_ixs, 1]\n",
    "    tpfz = attempts[test_ixs, 2]\n",
    "\n",
    "\n",
    "#     s_table =  BigTable((len(uids_across_days), dim), 0, 20, init_hilo= 5)\n",
    "#     qn_table = BigTable((len(qids_seen),dim), 0, 10, init_hilo= 5-p50)\n",
    "#     s_table =  BigTable((len(uids_across_days), dim), 0, 24, init_hilo= 9)\n",
    "#     qn_table = BigTable((len(qids_seen),dim), 0, 14, init_hilo= 9-p50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NN_utils.BigTable object at 0x7faddab0fbe0> <NN_utils.BigTable object at 0x7fadda57f3c8> Tensor(\"psi_select_2:0\", shape=(?, 1), dtype=int32) Tensor(\"q_select_2:0\", shape=(?, 1), dtype=int32)\n",
      "psi_sel shape (?, 1)\n",
      "<keras.initializers.RandomUniform object at 0x7fadd07e24a8>\n",
      "kk (100, 10)\n",
      "selector shape (?, 1)\n",
      "flat selector shape (?,)\n",
      "call kk (100, 10)\n",
      "'rows' shape, (?, 10)\n",
      "<keras.initializers.RandomUniform object at 0x7fadd01c4ac8>\n",
      "kk (1000, 10)\n",
      "selector shape (?, 1)\n",
      "flat selector shape (?,)\n",
      "call kk (1000, 10)\n",
      "'rows' shape, (?, 10)\n",
      "Pr (?, 1)\n",
      "PSI WS:\n",
      "[[ 0.77348971  1.99678969  1.7150476  ...,  0.31257534  1.11737061\n",
      "   1.88809371]\n",
      " [ 0.14557195  1.36638618  1.22847533 ...,  1.41593051  0.07025862\n",
      "   0.18451071]\n",
      " [ 0.04691148  1.54577875  0.86273575 ...,  1.53148317  0.81927752\n",
      "   0.8550005 ]\n",
      " ..., \n",
      " [ 1.59669495  0.96086669  1.8748858  ...,  1.95325828  1.84723353\n",
      "   1.90262079]\n",
      " [ 0.42949629  1.42697906  0.70398355 ...,  0.36282992  0.10429454\n",
      "   1.16826415]\n",
      " [ 1.69580889  0.6001482   1.32481909 ...,  1.03118443  1.6791842\n",
      "   1.80375695]]\n",
      "QN WS:\n",
      "[[  2.07602262e-01   6.17479086e-01   1.67932296e+00   2.27689743e-01\n",
      "    1.84829402e+00   1.07615709e+00   1.98900056e+00   1.20937777e+00\n",
      "    6.37907028e-01   1.67729616e+00]\n",
      " [  2.92593479e-01   2.14300156e-02   1.09808111e+00   8.54597330e-01\n",
      "    1.51992774e+00   1.11387467e+00   1.87345266e+00   8.89180899e-01\n",
      "    1.73459244e+00   8.11977148e-01]\n",
      " [  1.69202018e+00   1.60830307e+00   1.63696337e+00   5.10998726e-01\n",
      "    8.51368904e-01   1.74930930e+00   9.26494598e-04   1.72320652e+00\n",
      "    7.45758533e-01   8.85384083e-02]\n",
      " [  2.17828751e-02   1.26615787e+00   1.77808285e-01   1.74988174e+00\n",
      "    1.41999841e+00   7.53329039e-01   1.47723198e+00   1.34318137e+00\n",
      "    6.19366646e-01   1.52680945e+00]\n",
      " [  3.86931181e-01   1.11501908e+00   1.66468477e+00   1.88370228e+00\n",
      "    1.98398852e+00   1.76539874e+00   1.26939726e+00   4.33323145e-01\n",
      "    2.19743967e-01   2.65372753e-01]\n",
      " [  1.25014329e+00   1.89599848e+00   8.16607475e-03   1.53249049e+00\n",
      "    5.76805353e-01   1.77282596e+00   8.52179527e-02   7.46913195e-01\n",
      "    5.09212017e-02   1.77018118e+00]\n",
      " [  1.33216524e+00   1.63147545e+00   7.40625858e-02   9.73169327e-01\n",
      "    1.52169037e+00   1.40696239e+00   4.50752258e-01   1.50739074e+00\n",
      "    1.98963332e+00   4.12688255e-02]\n",
      " [  3.56273413e-01   4.15876627e-01   1.41580582e+00   4.78314877e-01\n",
      "    2.14924812e-01   1.23994398e+00   1.01974869e+00   1.41082287e+00\n",
      "    7.84780979e-01   1.22339654e+00]\n",
      " [  5.23251057e-01   8.01458120e-01   4.18145180e-01   1.86725068e+00\n",
      "    3.03960800e-01   3.10484648e-01   1.92369080e+00   4.51566696e-01\n",
      "    1.67916155e+00   1.74445152e-01]\n",
      " [  1.74196005e+00   1.78090930e+00   1.42478967e+00   8.62062216e-01\n",
      "    5.74901581e-01   1.91581511e+00   7.58270979e-01   1.68775582e+00\n",
      "    1.09951520e+00   9.12127018e-01]\n",
      " [  1.26481962e+00   7.55219221e-01   1.48675323e+00   1.91228390e-02\n",
      "    1.92811990e+00   1.31277514e+00   1.48657823e+00   6.47480965e-01\n",
      "    1.20248771e+00   4.62496758e-01]\n",
      " [  4.14082527e-01   6.05346203e-01   1.26895928e+00   5.58077097e-01\n",
      "    1.69227910e+00   1.42341161e+00   2.42607594e-02   7.03691959e-01\n",
      "    1.11136270e+00   1.06077981e+00]\n",
      " [  1.09340549e+00   2.74431467e-01   1.90207767e+00   1.71701765e+00\n",
      "    3.24406385e-01   4.06521320e-01   1.48816538e+00   1.84527349e+00\n",
      "    1.68214321e+00   6.25900269e-01]\n",
      " [  1.28772831e+00   1.74085736e+00   7.95744658e-01   1.52464938e+00\n",
      "    1.66884351e+00   1.69180775e+00   6.74548149e-01   1.08601403e+00\n",
      "    8.48752499e-01   1.97012711e+00]\n",
      " [  1.66639805e-01   1.49675035e+00   6.91082001e-01   1.61230803e+00\n",
      "    1.31777525e+00   8.50468636e-01   9.63677883e-01   8.01420927e-01\n",
      "    1.29227734e+00   1.28333426e+00]\n",
      " [  5.15258312e-01   9.92130518e-01   3.70965242e-01   1.93577385e+00\n",
      "    9.57907677e-01   1.31676626e+00   1.83549166e+00   5.91974258e-02\n",
      "    4.05183792e-01   1.09454346e+00]\n",
      " [  1.68595338e+00   2.22971201e-01   1.34973025e+00   1.46949911e+00\n",
      "    6.06985807e-01   1.62791944e+00   9.23969984e-01   1.27744222e+00\n",
      "    8.16929817e-01   2.75239468e-01]\n",
      " [  1.61439943e+00   1.07246757e+00   1.06276274e+00   1.82728839e+00\n",
      "    6.61455870e-01   3.70924234e-01   1.78709149e+00   1.59601760e+00\n",
      "    8.09350967e-01   1.19473267e+00]\n",
      " [  3.05985451e-01   1.65187120e-01   6.52320623e-01   6.42545223e-02\n",
      "    1.50517344e+00   1.37058091e+00   1.58178306e+00   1.91996837e+00\n",
      "    1.35119653e+00   1.50951695e+00]\n",
      " [  1.54351473e-01   1.00257277e+00   3.61070633e-02   1.68577671e-01\n",
      "    7.83050060e-01   1.80276322e+00   7.98178196e-01   1.78972173e+00\n",
      "    1.61850929e-01   1.14547944e+00]\n",
      " [  1.03729510e+00   1.77043891e+00   1.96602058e+00   1.81085396e+00\n",
      "    9.28575277e-01   1.55782127e+00   1.62492847e+00   1.84990907e+00\n",
      "    5.61687946e-01   7.77713776e-01]\n",
      " [  1.25393200e+00   1.45952272e+00   1.82448673e+00   1.22344542e+00\n",
      "    1.06263328e+00   1.30799317e+00   2.81127453e-01   1.23906040e+00\n",
      "    4.15072441e-01   1.40526938e+00]\n",
      " [  1.44782686e+00   9.05446768e-01   1.48750257e+00   1.81436777e+00\n",
      "    2.44668722e-01   1.12697124e-01   6.68571711e-01   1.32079172e+00\n",
      "    3.20061207e-01   1.57627559e+00]\n",
      " [  1.61532950e+00   1.82619572e-01   9.65659142e-01   9.65975523e-01\n",
      "    1.47172928e-01   1.35636282e+00   9.94215012e-01   1.86917067e+00\n",
      "    2.98011780e-01   9.26283121e-01]\n",
      " [  1.95514631e+00   1.30715299e+00   1.87850451e+00   1.32936859e+00\n",
      "    1.66512346e+00   4.03670549e-01   5.45128345e-01   4.35798168e-02\n",
      "    1.72503710e+00   9.42129135e-01]\n",
      " [  8.84834528e-01   1.62775922e+00   5.57282209e-01   8.71696472e-01\n",
      "    1.98130608e+00   1.35363674e+00   1.14918876e+00   1.08758521e+00\n",
      "    1.58716846e+00   1.36575198e+00]\n",
      " [  1.59432292e+00   3.98947239e-01   4.69624758e-01   8.00691128e-01\n",
      "    1.97984242e+00   9.48186636e-01   1.23968220e+00   1.84451771e+00\n",
      "    8.30035210e-01   1.68135405e-01]\n",
      " [  5.22065163e-01   1.58378577e+00   1.22468901e+00   2.94196606e-02\n",
      "    2.25969076e-01   1.81622195e+00   4.13972378e-01   1.18415236e+00\n",
      "    1.81380987e-01   7.20362425e-01]\n",
      " [  4.81516361e-01   1.73417425e+00   4.15904999e-01   1.35142970e+00\n",
      "    5.12858391e-01   7.72133589e-01   1.70623112e+00   7.71858692e-01\n",
      "    1.90017533e+00   6.26375914e-01]\n",
      " [  1.14866114e+00   1.03183579e+00   1.38027740e+00   3.30422640e-01\n",
      "    3.90070438e-01   2.33679295e-01   8.64721775e-01   6.80223465e-01\n",
      "    2.39933729e-01   5.05743027e-01]\n",
      " [  7.74388313e-01   1.35255361e+00   5.80172777e-01   1.34218001e+00\n",
      "    1.83591843e-02   1.07455873e+00   1.63090682e+00   1.37353706e+00\n",
      "    1.54486060e+00   1.57532454e+00]\n",
      " [  1.08906960e+00   1.85736108e+00   1.61390328e+00   1.02278709e-01\n",
      "    1.96437073e+00   1.06402016e+00   5.02432823e-01   1.77954149e+00\n",
      "    1.63842320e+00   1.84058595e+00]\n",
      " [  1.44244766e+00   1.56344175e+00   1.96013188e+00   1.28488040e+00\n",
      "    9.14452076e-01   6.92864656e-01   1.54924870e-01   3.82700443e-01\n",
      "    5.80296993e-01   1.62861896e+00]\n",
      " [  2.78943062e-01   7.62270927e-01   1.48383021e+00   1.84032488e+00\n",
      "    1.79836750e-02   8.21370363e-01   1.63432288e+00   1.95920062e+00\n",
      "    1.73512602e+00   6.04876280e-01]\n",
      " [  3.50883484e-01   1.67578959e+00   1.64230657e+00   1.96337867e+00\n",
      "    1.09382033e+00   1.32819414e+00   8.86598110e-01   5.39219856e-01\n",
      "    1.53992820e+00   5.45835257e-01]\n",
      " [  1.18548679e+00   1.06742430e+00   9.02245283e-01   1.87463832e+00\n",
      "    1.54327464e+00   7.19070196e-01   1.10648394e-01   1.28854561e+00\n",
      "    1.74711728e+00   7.47956753e-01]\n",
      " [  1.60097361e+00   1.87008739e+00   9.20936584e-01   1.04804039e-01\n",
      "    2.44323015e-01   3.73308659e-02   1.79371214e+00   3.24773550e-01\n",
      "    6.32951498e-01   1.74660134e+00]\n",
      " [  1.71692872e+00   9.61643457e-01   2.46600628e-01   2.96857119e-01\n",
      "    1.54838991e+00   1.05189729e+00   2.86189556e-01   1.32202888e+00\n",
      "    1.08819199e+00   1.21615791e+00]\n",
      " [  6.67711496e-01   8.61705065e-01   1.53852940e-01   1.63406086e+00\n",
      "    9.03587818e-01   1.85321259e+00   1.23444605e+00   9.15515184e-01\n",
      "    3.14857483e-01   1.51598215e+00]\n",
      " [  5.05502462e-01   1.30836940e+00   1.89544988e+00   3.08818579e-01\n",
      "    7.82017231e-01   1.87022424e+00   9.93883133e-01   1.52043700e+00\n",
      "    5.21684408e-01   1.64785624e+00]\n",
      " [  9.75038767e-01   2.31145144e-01   1.92716384e+00   1.28771973e+00\n",
      "    1.29970980e+00   3.50295067e-01   1.31747317e+00   1.88253307e+00\n",
      "    2.66847610e-02   6.92158461e-01]\n",
      " [  1.53188467e-01   1.24621391e-01   1.58623004e+00   4.84501123e-01\n",
      "    1.73260307e+00   1.26946068e+00   1.97504807e+00   3.53123188e-01\n",
      "    8.35081577e-01   1.01815462e+00]\n",
      " [  6.23941422e-02   1.74638844e+00   9.54714537e-01   1.42532229e+00\n",
      "    4.56809998e-01   1.52258348e+00   2.14802265e-01   1.56460786e+00\n",
      "    4.01906729e-01   1.54554939e+00]\n",
      " [  1.47251534e+00   1.29723167e+00   1.64880300e+00   1.01464105e+00\n",
      "    7.43130922e-01   1.72969794e+00   1.77060509e+00   1.13391614e+00\n",
      "    1.55341625e-01   4.16472435e-01]\n",
      " [  1.70672750e+00   5.74828625e-01   1.03721285e+00   4.62128878e-01\n",
      "    4.64726925e-01   1.89330459e+00   1.40552688e+00   1.57699227e+00\n",
      "    1.17717290e+00   2.59918690e-01]\n",
      " [  1.23572612e+00   9.99665260e-03   1.72672462e+00   1.92161131e+00\n",
      "    1.16911030e+00   1.86182618e+00   4.79050398e-01   1.87974358e+00\n",
      "    1.52531099e+00   8.52210760e-01]\n",
      " [  1.74667692e+00   4.95020628e-01   6.71769142e-01   1.13085270e-01\n",
      "    1.71779323e+00   8.50882053e-01   9.38043356e-01   5.65345287e-02\n",
      "    1.03096867e+00   5.38450003e-01]\n",
      " [  8.00256729e-01   1.36754417e+00   1.96155190e+00   2.55888224e-01\n",
      "    9.30438757e-01   1.63163829e+00   1.13683987e+00   1.24481559e+00\n",
      "    1.18279243e+00   1.66067529e+00]\n",
      " [  9.35327053e-01   1.63221431e+00   1.06273174e-01   8.10776472e-01\n",
      "    1.68538904e+00   3.67041111e-01   1.95708227e+00   6.50208473e-01\n",
      "    9.38489437e-01   1.97952199e+00]\n",
      " [  1.35178304e+00   1.03690457e+00   1.49082375e+00   5.51073313e-01\n",
      "    5.53525209e-01   4.43379641e-01   1.44599652e+00   1.70494747e+00\n",
      "    9.87621546e-01   1.63281274e+00]\n",
      " [  2.70936012e-01   1.67714906e+00   3.25933218e-01   9.48344946e-01\n",
      "    1.32564807e+00   1.40633559e+00   8.28938961e-01   1.32437015e+00\n",
      "    6.15152836e-01   4.47316170e-01]\n",
      " [  9.64822531e-01   3.83828163e-01   1.53226519e+00   8.97909164e-01\n",
      "    3.48057747e-02   6.26146555e-01   7.48691559e-02   4.35989141e-01\n",
      "    8.60842705e-01   1.34374046e+00]\n",
      " [  1.62001920e+00   1.86082506e+00   1.00016379e+00   4.83427286e-01\n",
      "    1.74863029e+00   3.97135496e-01   1.96595168e+00   9.66911077e-01\n",
      "    1.25632048e-01   1.57934618e+00]\n",
      " [  1.03003597e+00   1.21960187e+00   1.56608343e+00   1.90628672e+00\n",
      "    1.42072940e+00   1.97318077e+00   3.03538322e-01   7.61347055e-01\n",
      "    3.84501934e-01   1.23862576e+00]\n",
      " [  1.76554298e+00   1.90692663e+00   6.12002373e-01   1.12444115e+00\n",
      "    1.77081323e+00   5.56996346e-01   1.31888723e+00   1.26884389e+00\n",
      "    5.88555098e-01   1.78039074e+00]\n",
      " [  1.18648863e+00   4.09195662e-01   9.61945772e-01   1.67814207e+00\n",
      "    1.29733062e+00   1.80447721e+00   7.64328480e-01   2.66974688e-01\n",
      "    3.38185072e-01   1.40602088e+00]\n",
      " [  7.69725561e-01   1.48401165e+00   4.84943390e-03   6.61493301e-01\n",
      "    5.72015762e-01   8.59745264e-01   4.96500015e-01   2.58462191e-01\n",
      "    6.57261372e-01   2.47650623e-01]\n",
      " [  2.19268799e-01   1.17465997e+00   1.56155586e-01   1.07253242e+00\n",
      "    7.71590710e-01   8.99762392e-01   1.08071756e+00   7.11284399e-01\n",
      "    3.70397568e-01   1.40010715e+00]\n",
      " [  1.06978488e+00   1.93727970e+00   1.20800972e+00   3.38222504e-01\n",
      "    1.79254651e+00   1.59199643e+00   1.58363748e+00   6.72395945e-01\n",
      "    5.50159454e-01   1.60999799e+00]\n",
      " [  1.35281324e+00   1.39327598e+00   7.04326630e-01   3.51571798e-01\n",
      "    1.01587272e+00   1.33636737e+00   1.78767729e+00   1.51121378e+00\n",
      "    3.69310379e-04   1.90079832e+00]\n",
      " [  8.06483507e-01   9.39408302e-01   1.41517162e-01   1.52772665e-01\n",
      "    1.03240395e+00   5.16850471e-01   1.59083939e+00   7.97749758e-01\n",
      "    9.46286440e-01   9.98883486e-01]\n",
      " [  4.25867319e-01   7.84364462e-01   1.48991799e+00   1.61251044e+00\n",
      "    8.08008194e-01   5.68492651e-01   1.96433783e+00   8.57291222e-01\n",
      "    3.48720551e-01   1.18868947e+00]\n",
      " [  2.71310806e-02   4.63661909e-01   7.41631269e-01   1.51024818e-01\n",
      "    1.62858486e-01   1.08122540e+00   1.16842556e+00   1.12332177e+00\n",
      "    1.34493065e+00   1.52864528e+00]\n",
      " [  1.15052843e+00   7.63078690e-01   6.37076616e-01   1.06636310e+00\n",
      "    1.87461662e+00   2.87785530e-01   1.89683437e+00   1.34116411e-01\n",
      "    4.38346386e-01   1.29494596e+00]\n",
      " [  1.13960052e+00   1.56282663e+00   1.13248038e+00   1.15351129e+00\n",
      "    1.21188760e+00   7.80046225e-01   1.58789611e+00   7.62542486e-01\n",
      "    8.95073414e-01   6.96256876e-01]\n",
      " [  1.43016338e+00   1.04623246e+00   1.84080124e-01   3.30097437e-01\n",
      "    2.74998665e-01   2.02541828e-01   1.00505352e-01   1.46720290e+00\n",
      "    5.48669100e-01   6.89381361e-01]\n",
      " [  1.87993026e+00   5.10096788e-01   6.79752827e-01   1.21529341e+00\n",
      "    1.47867537e+00   5.14836311e-02   1.57808566e+00   1.74627900e+00\n",
      "    7.17762709e-01   9.00617361e-01]\n",
      " [  1.98797607e+00   7.87961483e-02   6.51611090e-01   1.58397746e+00\n",
      "    1.76908898e+00   1.14116311e+00   6.42817020e-02   3.29056740e-01\n",
      "    1.22071958e+00   1.61605954e+00]\n",
      " [  1.05611706e+00   1.71784830e+00   3.11572552e-01   1.74938130e+00\n",
      "    6.96360350e-01   2.24027634e-01   1.48862648e+00   1.23256302e+00\n",
      "    6.93961382e-01   4.32371855e-01]\n",
      " [  6.60815716e-01   1.35049582e-01   1.62645698e+00   1.35424018e+00\n",
      "    3.04870844e-01   1.28825212e+00   1.86367679e+00   3.13672304e-01\n",
      "    1.61848998e+00   4.55964327e-01]\n",
      " [  1.53055048e+00   7.38875628e-01   1.42018080e+00   1.89605069e+00\n",
      "    1.22191739e+00   1.34857726e+00   7.75612831e-01   1.48922539e+00\n",
      "    1.25694537e+00   1.89194560e+00]\n",
      " [  4.53352928e-02   8.80777597e-01   1.27441239e+00   3.89912367e-01\n",
      "    1.24808955e+00   1.02749848e+00   1.94043541e+00   1.96334004e+00\n",
      "    1.55023098e+00   1.83903289e+00]\n",
      " [  1.61542487e+00   4.27370548e-01   4.63087320e-01   8.40998411e-01\n",
      "    2.47497559e-01   1.12766099e+00   1.63559508e+00   1.06773973e+00\n",
      "    1.50688338e+00   1.66469431e+00]\n",
      " [  1.29008913e+00   2.48491287e-01   1.10403037e+00   1.42975712e+00\n",
      "    1.18402004e+00   1.11455035e+00   6.30378723e-04   1.56231356e+00\n",
      "    1.16954994e+00   1.12888956e+00]\n",
      " [  4.59920883e-01   1.74638271e-01   1.85184264e+00   1.72108102e+00\n",
      "    3.86602879e-02   1.37598491e+00   1.48911381e+00   1.29923749e+00\n",
      "    1.00728774e+00   1.80977750e+00]\n",
      " [  3.60863924e-01   1.89004517e+00   1.29116416e+00   7.30834007e-01\n",
      "    1.74056244e+00   5.43318510e-01   1.04288912e+00   1.48060393e+00\n",
      "    6.10134125e-01   1.26944351e+00]\n",
      " [  1.78596282e+00   1.17728138e+00   1.84862542e+00   1.93745589e+00\n",
      "    1.93016553e+00   1.45074463e+00   1.65252876e+00   9.16826963e-01\n",
      "    1.11806393e-02   1.07305861e+00]\n",
      " [  1.04810381e+00   1.22721791e+00   1.01361561e+00   1.26114559e+00\n",
      "    3.12000751e-01   6.50839329e-01   6.48660421e-01   1.06415510e+00\n",
      "    4.16287661e-01   2.47291088e-01]\n",
      " [  9.09317017e-01   1.48921108e+00   7.92204380e-01   9.76742029e-01\n",
      "    1.20426774e+00   1.44423485e-01   1.35499859e+00   1.40537953e+00\n",
      "    7.38804340e-01   1.54591608e+00]\n",
      " [  8.01454544e-01   1.58528256e+00   9.86623287e-01   5.71895599e-01\n",
      "    2.11454391e-01   1.10921574e+00   6.08097076e-01   1.75299644e-01\n",
      "    1.96235085e+00   9.97873068e-01]\n",
      " [  2.61807442e-03   1.93476653e+00   7.36571789e-01   3.74627590e-01\n",
      "    4.84458923e-01   1.88801050e-01   2.79682636e-01   4.88557100e-01\n",
      "    5.41942358e-01   1.64968491e+00]\n",
      " [  1.83585954e+00   2.86322832e-01   6.65348291e-01   6.99726105e-01\n",
      "    5.93636990e-01   1.12345529e+00   1.94794202e+00   1.25659871e+00\n",
      "    3.79828930e-01   8.39671135e-01]\n",
      " [  1.31506443e+00   1.91333294e-01   8.47888470e-01   6.50567770e-01\n",
      "    1.23990107e+00   7.63728142e-01   1.14542079e+00   1.80090833e+00\n",
      "    3.86160851e-01   3.58599186e-01]\n",
      " [  1.52292514e+00   2.66485214e-02   1.12477064e+00   1.04037571e+00\n",
      "    1.14138746e+00   1.35567021e+00   6.71748638e-01   3.02618742e-01\n",
      "    4.61616755e-01   1.72947621e+00]\n",
      " [  7.54102230e-01   6.01866245e-02   1.69474125e+00   1.97652149e+00\n",
      "    1.06521249e+00   1.11078668e+00   1.70389557e+00   1.67389059e+00\n",
      "    1.34502244e+00   7.32984781e-01]\n",
      " [  9.03512239e-01   1.99434805e+00   1.88596320e+00   1.44287229e+00\n",
      "    7.33164787e-01   2.82761335e-01   2.88959265e-01   1.14307356e+00\n",
      "    1.73327374e+00   7.20608711e-01]\n",
      " [  9.49373960e-01   9.02143478e-01   1.06073618e-01   1.35841489e+00\n",
      "    3.24967861e-01   2.90315151e-02   5.98584175e-01   1.30109072e-01\n",
      "    8.43183041e-01   8.69363070e-01]\n",
      " [  1.75854445e-01   1.42215848e+00   9.97180939e-02   1.93784714e+00\n",
      "    1.26248145e+00   8.35467815e-01   5.96467257e-01   5.04792690e-01\n",
      "    1.94795036e+00   1.09465694e+00]\n",
      " [  1.31197429e+00   1.83715415e+00   4.20453548e-01   5.40419817e-01\n",
      "    8.58472347e-01   1.81748366e+00   1.92652321e+00   5.42625666e-01\n",
      "    6.81597471e-01   1.36492324e+00]\n",
      " [  1.34184098e+00   5.84262848e-01   1.76611137e+00   1.55258179e-01\n",
      "    1.51569557e+00   1.79319620e+00   1.89186311e+00   1.92647767e+00\n",
      "    7.73160458e-02   9.34095383e-01]\n",
      " [  4.44247723e-02   1.51238966e+00   8.52781057e-01   1.63175082e+00\n",
      "    5.16069412e-01   9.03135300e-01   4.24989700e-01   9.25354481e-01\n",
      "    1.19507074e-01   8.86894226e-01]\n",
      " [  8.49039555e-01   9.46299076e-01   2.24764109e-01   1.11978197e+00\n",
      "    9.37568426e-01   7.15013027e-01   9.99968290e-01   3.00171852e-01\n",
      "    7.78993368e-01   1.55562139e+00]\n",
      " [  8.86927366e-01   1.40654993e+00   4.74396467e-01   6.58444166e-01\n",
      "    3.06339264e-01   1.43464160e+00   7.20775127e-02   1.93448019e+00\n",
      "    1.88957644e+00   5.18131018e-01]\n",
      " [  1.44380832e+00   1.64862490e+00   9.87828732e-01   9.55163240e-01\n",
      "    1.03641176e+00   3.14685345e-01   8.98830891e-01   1.99224186e+00\n",
      "    1.33824444e+00   1.01470232e+00]\n",
      " [  1.69695926e+00   1.87356472e-01   1.14590001e+00   3.79271746e-01\n",
      "    6.38878822e-01   7.60147572e-02   7.22449541e-01   1.23032117e+00\n",
      "    6.37770891e-01   6.66602850e-01]\n",
      " [  1.00949287e+00   9.06775713e-01   7.83012629e-01   5.73716402e-01\n",
      "    1.52157688e+00   1.01912355e+00   5.16131639e-01   9.91080999e-01\n",
      "    4.64181662e-01   5.57214737e-01]\n",
      " [  3.87728453e-01   9.60844755e-01   1.61840582e+00   5.80971479e-01\n",
      "    1.36786461e+00   4.35169697e-01   7.65872478e-01   1.79352593e+00\n",
      "    5.29401302e-02   1.22770119e+00]\n",
      " [  1.65996742e+00   1.58096886e+00   1.83413458e+00   8.60595703e-03\n",
      "    1.88140225e+00   9.38288450e-01   2.26993561e-01   1.25606966e+00\n",
      "    4.44913387e-01   1.42300248e+00]\n",
      " [  7.79422283e-01   7.43814707e-01   1.79002380e+00   1.12013102e+00\n",
      "    4.44904089e-01   1.05805993e+00   2.03532457e-01   1.97916222e+00\n",
      "    1.60734606e+00   8.47025156e-01]\n",
      " [  1.79466414e+00   1.06051445e-01   1.10209894e+00   8.89730453e-02\n",
      "    2.79773474e-01   7.20097303e-01   3.78530741e-01   1.38667154e+00\n",
      "    4.99618769e-01   2.34673738e-01]]\n"
     ]
    }
   ],
   "source": [
    "    from keras import regularizers\n",
    "    opt = None\n",
    "#     s_table =  BigTable((n_students, dim), -math.inf, math.inf, init_hilo= 10)\n",
    "#     qn_table = BigTable((n_qs,dim), -math.inf, math.inf, init_hilo= 10-p50)\n",
    "\n",
    "#     s_table =  BigTable((n_students, dim), -math.inf, math.inf, init_hilo=None)\n",
    "#     qn_table = BigTable((n_qs,dim), -math.inf, math.inf, init_hilo=None)\n",
    "\n",
    "    s_table =  BigTable((n_students, dim), 1, math.inf, init_hilo=1)#, regulariser=regularizers.l2(0))\n",
    "    qn_table = BigTable((n_qs,dim), 1, math.inf, init_hilo=1)#, regulariser=regularizers.l2(0))\n",
    "\n",
    "    \n",
    "#     print(\"s start value = \", offset+av)\n",
    "#     print(\"q start value = \", offset+av-p50)\n",
    "\n",
    "    q_wgts_history = []\n",
    "    s_wgts_history = []\n",
    "\n",
    "#     s_table =  BigTable((len(userz), 1), 0, 10)\n",
    "    qn_table.trainable = True\n",
    "    s_table.trainable = True\n",
    "    q_model = generate_qs_model(qn_table, s_table, Adam())\n",
    "    \n",
    "#     qn_table.trainable = False\n",
    "#     s_table.trainable = True\n",
    "#     s_model = generate_qs_model(qn_table, s_table, Adam())\n",
    "\n",
    "    psi_ws = s_table.get_weights()[0]\n",
    "    qn_ws = qn_table.get_weights()[0]\n",
    "    \n",
    "    print(\"PSI WS:\")\n",
    "    print(psi_ws)\n",
    "    print(\"QN WS:\")\n",
    "    print(qn_ws)\n",
    "#     input(\"start...\")\n",
    "    \n",
    "#     with tf.Session() as sess:\n",
    "#         print(sess.run(tf_corrcoef(tf.constant(qn_ws, dtype=tf.float32))))\n",
    "\n",
    "#     print(numpy.corrcoef(qn_ws.T))#-numpy.eye(qn_ws.shape[1])))\n",
    "\n",
    "    \n",
    "\n",
    "#     zer0z = numpy.zeros((len(pfz),dim))\n",
    "#     vzer0z = numpy.zeros((len(vpfz),dim))\n",
    "#     qz=list(qz)\n",
    "#     sz=list(sz)\n",
    "#     pfz = list(pfz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rjm49/.venvs/isaac/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 1s 6us/step - loss: 0.7068 - mean_squared_error: 0.0063 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 0s 4us/step - loss: 0.6944 - mean_squared_error: 6.0948e-04 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 0s 4us/step - loss: 0.6934 - mean_squared_error: 1.4005e-04 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 0s 4us/step - loss: 0.6933 - mean_squared_error: 5.1715e-05 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 0s 4us/step - loss: 0.6932 - mean_squared_error: 3.4290e-05 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 0s 4us/step - loss: 0.6932 - mean_squared_error: 2.8640e-05 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 0s 4us/step - loss: 0.6932 - mean_squared_error: 2.4980e-05 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 0s 4us/step - loss: 0.6932 - mean_squared_error: 2.2199e-05 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 0s 4us/step - loss: 0.6932 - mean_squared_error: 2.0044e-05 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 0s 4us/step - loss: 0.6932 - mean_squared_error: 1.8399e-05 - acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "    from keras.callbacks import EarlyStopping\n",
    "    \n",
    "    es = EarlyStopping(monitor=\"loss\", restore_best_weights=True, patience=5)\n",
    "    initpfz = [0.5 for _ in range(len(attempts))]\n",
    " \n",
    "    for _ in range(10):\n",
    "        q_model.fit(x=[attempts[:,1], attempts[:,0]], y=numpy.array(initpfz).reshape(-1,1), batch_size=1000, shuffle=True, epochs=1, verbose=1)\n",
    "#         s_model.set_weights(q_model.get_weights())\n",
    "#         s_model.fit(x=[attempts[:,1], attempts[:,0]], y=numpy.array(initpfz).reshape(-1,1), batch_size=1000, shuffle=True, epochs=1, verbose=1)\n",
    "#         q_model.set_weights(s_model.get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.49193469] 1.0\n",
      "[ 0.49700302] 0.0\n",
      "[ 0.49716392] 1.0\n",
      "[ 0.49552488] 0.0\n",
      "[ 0.50143427] 0.0\n",
      "[ 0.49334666] 0.0\n",
      "[ 0.50348002] 1.0\n",
      "[ 0.49397776] 0.0\n",
      "[ 0.49656385] 0.0\n",
      "[ 0.50743133] 0.0\n",
      "[ 0.49598455] 0.0\n",
      "[ 0.49616301] 0.0\n",
      "[ 0.50599694] 0.0\n",
      "[ 0.49457324] 1.0\n",
      "[ 0.50057048] 0.0\n",
      "[ 0.50156027] 1.0\n",
      "[ 0.50196022] 1.0\n",
      "[ 0.49395472] 0.0\n",
      "[ 0.50310826] 1.0\n",
      "[ 0.50720197] 0.0\n",
      "[ 0.49692002] 0.0\n",
      "[ 0.50252241] 0.0\n",
      "[ 0.49826449] 0.0\n",
      "[ 0.49722338] 0.0\n",
      "[ 0.50596017] 1.0\n",
      "[ 0.4997198] 1.0\n",
      "[ 0.49448156] 0.0\n",
      "[ 0.50151902] 1.0\n",
      "[ 0.50042999] 0.0\n",
      "[ 0.502168] 0.0\n",
      "[ 0.49312255] 1.0\n",
      "[ 0.49624008] 0.0\n",
      "[ 0.49548829] 1.0\n",
      "[ 0.49637377] 0.0\n",
      "[ 0.50817335] 1.0\n",
      "[ 0.49874482] 0.0\n",
      "[ 0.49481997] 0.0\n",
      "[ 0.51016116] 1.0\n",
      "[ 0.50369036] 1.0\n",
      "[ 0.50191593] 0.0\n",
      "[ 0.49833909] 0.0\n",
      "[ 0.49882197] 0.0\n",
      "[ 0.49365437] 0.0\n",
      "[ 0.49938688] 1.0\n",
      "[ 0.48545319] 0.0\n",
      "[ 0.49985072] 0.0\n",
      "[ 0.49292251] 0.0\n",
      "[ 0.50496948] 0.0\n",
      "[ 0.49992874] 0.0\n",
      "[ 0.49481609] 1.0\n",
      "[ 0.49581459] 0.0\n",
      "[ 0.50625336] 1.0\n",
      "[ 0.50499761] 0.0\n",
      "[ 0.49996367] 1.0\n",
      "[ 0.50377315] 1.0\n",
      "[ 0.49557942] 1.0\n",
      "[ 0.49568769] 0.0\n",
      "[ 0.49303904] 1.0\n",
      "[ 0.49606234] 1.0\n",
      "[ 0.49877834] 0.0\n",
      "[ 0.49486968] 1.0\n",
      "[ 0.49542361] 0.0\n",
      "[ 0.49719453] 0.0\n",
      "[ 0.49675333] 0.0\n",
      "[ 0.49336129] 0.0\n",
      "[ 0.49750075] 0.0\n",
      "[ 0.49773169] 0.0\n",
      "[ 0.50056553] 0.0\n",
      "[ 0.49527824] 1.0\n",
      "[ 0.50030106] 1.0\n",
      "[ 0.50000119] 1.0\n",
      "[ 0.49847102] 0.0\n",
      "[ 0.50292599] 0.0\n",
      "[ 0.49657822] 1.0\n",
      "[ 0.4946281] 0.0\n",
      "[ 0.49218914] 1.0\n",
      "[ 0.50363636] 1.0\n",
      "[ 0.50015014] 1.0\n",
      "[ 0.4951607] 1.0\n",
      "[ 0.50833428] 0.0\n",
      "[ 0.48925158] 0.0\n",
      "[ 0.49573931] 1.0\n",
      "[ 0.49746111] 1.0\n",
      "[ 0.49308932] 0.0\n",
      "[ 0.498126] 0.0\n",
      "[ 0.49565583] 0.0\n",
      "[ 0.49341741] 1.0\n",
      "[ 0.49576962] 0.0\n",
      "[ 0.5027737] 0.0\n",
      "[ 0.49791989] 1.0\n",
      "[ 0.50150347] 1.0\n",
      "[ 0.50605816] 0.0\n",
      "[ 0.49804679] 0.0\n",
      "[ 0.50304902] 1.0\n",
      "[ 0.49542865] 1.0\n",
      "[ 0.50352824] 1.0\n",
      "[ 0.4999707] 1.0\n",
      "[ 0.49989188] 0.0\n",
      "[ 0.50268584] 0.0\n",
      "[ 0.50656563] 0.0\n",
      "[ 0.49977079] 1.0\n",
      "[ 0.50110781] 0.0\n",
      "[ 0.50167775] 1.0\n",
      "[ 0.49352431] 0.0\n",
      "[ 0.49990895] 1.0\n",
      "[ 0.50267386] 0.0\n",
      "[ 0.49489874] 1.0\n",
      "[ 0.50239789] 1.0\n",
      "[ 0.50654668] 1.0\n",
      "[ 0.49954626] 1.0\n",
      "[ 0.49511853] 1.0\n",
      "[ 0.50352031] 1.0\n",
      "[ 0.50078666] 1.0\n",
      "[ 0.50026959] 1.0\n",
      "[ 0.49882448] 1.0\n",
      "[ 0.49887964] 0.0\n",
      "[ 0.49981853] 0.0\n",
      "[ 0.49715531] 0.0\n",
      "[ 0.49647582] 0.0\n",
      "[ 0.50622386] 1.0\n",
      "[ 0.49837157] 0.0\n",
      "[ 0.50329638] 0.0\n",
      "[ 0.50073791] 1.0\n",
      "[ 0.50242394] 0.0\n",
      "[ 0.49459141] 0.0\n",
      "[ 0.50134879] 0.0\n",
      "[ 0.50106966] 1.0\n",
      "[ 0.49732068] 1.0\n",
      "[ 0.50045431] 0.0\n",
      "[ 0.49672392] 0.0\n",
      "[ 0.50152397] 0.0\n",
      "[ 0.50153619] 0.0\n",
      "[ 0.49536753] 1.0\n",
      "[ 0.4909738] 1.0\n",
      "[ 0.49920478] 1.0\n",
      "[ 0.49330014] 1.0\n",
      "[ 0.50408214] 1.0\n",
      "[ 0.50513184] 1.0\n",
      "[ 0.50275767] 1.0\n",
      "[ 0.49690101] 1.0\n",
      "[ 0.49824449] 0.0\n",
      "[ 0.48900309] 0.0\n",
      "[ 0.50379115] 0.0\n",
      "[ 0.50307488] 0.0\n",
      "[ 0.50025636] 1.0\n",
      "[ 0.49838525] 0.0\n",
      "[ 0.50424945] 1.0\n",
      "[ 0.49911025] 0.0\n",
      "[ 0.50073957] 0.0\n",
      "[ 0.49934596] 1.0\n",
      "[ 0.50248462] 0.0\n",
      "[ 0.50231338] 1.0\n",
      "[ 0.50136149] 0.0\n",
      "[ 0.49943507] 0.0\n",
      "[ 0.50104779] 1.0\n",
      "[ 0.49935669] 1.0\n",
      "[ 0.50044078] 0.0\n",
      "[ 0.50124454] 1.0\n",
      "[ 0.50683677] 0.0\n",
      "[ 0.49966744] 1.0\n",
      "[ 0.50250739] 1.0\n",
      "[ 0.50197667] 1.0\n",
      "[ 0.49844247] 0.0\n",
      "[ 0.5011555] 0.0\n",
      "[ 0.49513423] 0.0\n",
      "[ 0.49723333] 0.0\n",
      "[ 0.49849102] 0.0\n",
      "[ 0.49810755] 0.0\n",
      "[ 0.5036577] 1.0\n",
      "[ 0.49525508] 1.0\n",
      "[ 0.4991242] 0.0\n",
      "[ 0.49680281] 0.0\n",
      "[ 0.49663782] 0.0\n",
      "[ 0.50343704] 0.0\n",
      "[ 0.49758682] 1.0\n",
      "[ 0.49511641] 0.0\n",
      "[ 0.49793097] 1.0\n",
      "[ 0.50520992] 0.0\n",
      "[ 0.50118524] 1.0\n",
      "[ 0.49684149] 1.0\n",
      "[ 0.49562183] 1.0\n",
      "[ 0.49628186] 1.0\n",
      "[ 0.49662542] 0.0\n",
      "[ 0.49377719] 1.0\n",
      "[ 0.49956825] 1.0\n",
      "[ 0.50068593] 0.0\n",
      "[ 0.49753129] 1.0\n",
      "[ 0.49887258] 1.0\n",
      "[ 0.49884227] 0.0\n",
      "[ 0.50280803] 1.0\n",
      "[ 0.49778819] 1.0\n",
      "[ 0.50058645] 1.0\n",
      "[ 0.49898124] 1.0\n",
      "[ 0.49218142] 1.0\n",
      "[ 0.49901518] 1.0\n",
      "[ 0.49726412] 0.0\n",
      "[ 0.50158811] 0.0\n",
      "[ 0.50619084] 1.0\n",
      "[ 0.49645627] 0.0\n",
      "[ 0.50013179] 1.0\n",
      "[ 0.49766749] 1.0\n",
      "[ 0.49990663] 1.0\n",
      "[ 0.50463068] 0.0\n",
      "[ 0.49825531] 0.0\n",
      "[ 0.50740141] 0.0\n",
      "[ 0.50146914] 0.0\n",
      "[ 0.50212014] 0.0\n",
      "[ 0.49864972] 1.0\n",
      "[ 0.49743947] 0.0\n",
      "[ 0.50482267] 1.0\n",
      "[ 0.49904603] 1.0\n",
      "[ 0.50009513] 0.0\n",
      "[ 0.49793184] 1.0\n",
      "[ 0.50298601] 0.0\n",
      "[ 0.49637681] 1.0\n",
      "[ 0.491164] 0.0\n",
      "[ 0.49941966] 1.0\n",
      "[ 0.49738362] 1.0\n",
      "[ 0.50642866] 0.0\n",
      "[ 0.49981803] 1.0\n",
      "[ 0.50460672] 1.0\n",
      "[ 0.49840325] 1.0\n",
      "[ 0.49708784] 1.0\n",
      "[ 0.50047028] 0.0\n",
      "[ 0.49810556] 1.0\n",
      "[ 0.50248212] 1.0\n",
      "[ 0.49713367] 1.0\n",
      "[ 0.50296736] 0.0\n",
      "[ 0.49959996] 1.0\n",
      "[ 0.50572509] 1.0\n",
      "[ 0.4993383] 1.0\n",
      "[ 0.49791169] 0.0\n",
      "[ 0.49884924] 1.0\n",
      "[ 0.49736342] 1.0\n",
      "[ 0.49605155] 1.0\n",
      "[ 0.50060213] 0.0\n",
      "[ 0.5015797] 1.0\n",
      "[ 0.50030756] 1.0\n",
      "[ 0.50230277] 0.0\n",
      "[ 0.49603361] 0.0\n",
      "[ 0.50405401] 0.0\n",
      "[ 0.49838036] 1.0\n",
      "[ 0.50650394] 0.0\n",
      "[ 0.49748328] 0.0\n",
      "[ 0.49899518] 0.0\n",
      "[ 0.49792314] 1.0\n",
      "[ 0.49990913] 1.0\n",
      "[ 0.49948969] 1.0\n",
      "[ 0.50242174] 1.0\n",
      "[ 0.4992232] 1.0\n",
      "[ 0.49934337] 0.0\n",
      "[ 0.50058222] 1.0\n",
      "[ 0.49862134] 1.0\n",
      "[ 0.49629229] 0.0\n",
      "[ 0.50445747] 0.0\n",
      "[ 0.49691099] 1.0\n",
      "[ 0.49803653] 0.0\n",
      "[ 0.49701804] 0.0\n",
      "[ 0.50118417] 0.0\n",
      "[ 0.50116217] 0.0\n",
      "[ 0.49475142] 1.0\n",
      "[ 0.49726421] 1.0\n",
      "[ 0.49773219] 0.0\n",
      "[ 0.50042313] 1.0\n",
      "[ 0.49810019] 0.0\n",
      "[ 0.49557838] 1.0\n",
      "[ 0.50103611] 0.0\n",
      "[ 0.49584848] 0.0\n",
      "[ 0.4940567] 1.0\n",
      "[ 0.49735665] 0.0\n",
      "[ 0.49427164] 1.0\n",
      "[ 0.50150883] 1.0\n",
      "[ 0.49877131] 1.0\n",
      "[ 0.49575359] 1.0\n",
      "[ 0.49623895] 1.0\n",
      "[ 0.49743474] 1.0\n",
      "[ 0.50250769] 0.0\n",
      "[ 0.50270289] 1.0\n",
      "[ 0.50034916] 1.0\n",
      "[ 0.50252914] 1.0\n",
      "[ 0.49630475] 0.0\n",
      "[ 0.48982656] 0.0\n",
      "[ 0.49784511] 0.0\n",
      "[ 0.50488484] 1.0\n",
      "[ 0.49835542] 1.0\n",
      "[ 0.49382806] 1.0\n",
      "[ 0.49590805] 1.0\n",
      "[ 0.50074446] 0.0\n",
      "[ 0.50009006] 0.0\n",
      "[ 0.4960492] 0.0\n",
      "[ 0.49717286] 1.0\n",
      "[ 0.49988493] 0.0\n",
      "[ 0.50242507] 0.0\n",
      "[ 0.49848872] 0.0\n",
      "[ 0.51034051] 0.0\n",
      "[ 0.50200027] 0.0\n",
      "[ 0.50387782] 0.0\n",
      "[ 0.49344686] 1.0\n",
      "[ 0.49693024] 1.0\n",
      "[ 0.50067389] 1.0\n",
      "[ 0.49990207] 0.0\n",
      "[ 0.49612466] 0.0\n",
      "[ 0.4989017] 0.0\n",
      "[ 0.49718609] 0.0\n",
      "[ 0.49311647] 0.0\n",
      "[ 0.49419972] 1.0\n",
      "[ 0.50243098] 1.0\n",
      "[ 0.50014657] 1.0\n",
      "[ 0.50044757] 0.0\n",
      "[ 0.50331771] 0.0\n",
      "[ 0.50465238] 1.0\n",
      "[ 0.49650064] 1.0\n",
      "[ 0.49760792] 1.0\n",
      "[ 0.49299675] 0.0\n",
      "[ 0.50093269] 1.0\n",
      "[ 0.49878091] 1.0\n",
      "[ 0.49835226] 1.0\n",
      "[ 0.50022596] 1.0\n",
      "[ 0.49410567] 0.0\n",
      "[ 0.49821517] 1.0\n",
      "[ 0.49370128] 1.0\n",
      "[ 0.50349057] 1.0\n",
      "[ 0.49916247] 0.0\n",
      "[ 0.50889409] 0.0\n",
      "[ 0.50033426] 1.0\n",
      "[ 0.49686554] 0.0\n",
      "[ 0.49443358] 1.0\n",
      "[ 0.50224352] 0.0\n",
      "[ 0.50168657] 0.0\n",
      "[ 0.49369439] 0.0\n",
      "[ 0.49634323] 1.0\n",
      "[ 0.49498233] 0.0\n",
      "[ 0.49943709] 1.0\n",
      "[ 0.49677521] 0.0\n",
      "[ 0.50157613] 1.0\n",
      "[ 0.50037557] 0.0\n",
      "[ 0.49993911] 1.0\n",
      "[ 0.50162458] 1.0\n",
      "[ 0.50470197] 0.0\n",
      "[ 0.49868488] 0.0\n",
      "[ 0.49634469] 1.0\n",
      "[ 0.49642774] 1.0\n",
      "[ 0.50222826] 0.0\n",
      "[ 0.50224149] 1.0\n",
      "[ 0.49691388] 1.0\n",
      "[ 0.50168633] 1.0\n",
      "[ 0.49885264] 0.0\n",
      "[ 0.4965736] 0.0\n",
      "[ 0.50291264] 0.0\n",
      "[ 0.49959913] 0.0\n",
      "[ 0.49994853] 0.0\n",
      "[ 0.49537116] 1.0\n",
      "[ 0.49670565] 0.0\n",
      "[ 0.50004601] 0.0\n",
      "[ 0.50016612] 0.0\n",
      "[ 0.49641582] 0.0\n",
      "[ 0.49804589] 0.0\n",
      "[ 0.50012499] 1.0\n",
      "[ 0.5036208] 0.0\n",
      "[ 0.49440682] 1.0\n",
      "[ 0.49536499] 0.0\n",
      "[ 0.49992615] 1.0\n",
      "[ 0.49403638] 0.0\n",
      "[ 0.49510539] 0.0\n",
      "[ 0.49325851] 1.0\n",
      "[ 0.49792725] 0.0\n",
      "[ 0.50382119] 0.0\n",
      "[ 0.50967753] 1.0\n",
      "[ 0.50348818] 1.0\n",
      "[ 0.49162039] 0.0\n",
      "[ 0.50096673] 1.0\n",
      "[ 0.50640494] 1.0\n",
      "[ 0.4963398] 1.0\n",
      "[ 0.4995527] 1.0\n",
      "[ 0.51015133] 1.0\n",
      "[ 0.49871629] 0.0\n",
      "[ 0.49599129] 1.0\n",
      "[ 0.49740559] 1.0\n",
      "[ 0.49716517] 1.0\n",
      "[ 0.4962059] 0.0\n",
      "[ 0.49780333] 0.0\n",
      "[ 0.50424153] 0.0\n",
      "[ 0.49587998] 0.0\n",
      "[ 0.49957171] 0.0\n",
      "[ 0.49921429] 0.0\n",
      "[ 0.49996364] 0.0\n",
      "[ 0.50100064] 1.0\n",
      "[ 0.48453397] 0.0\n",
      "[ 0.49604696] 0.0\n",
      "[ 0.50228959] 1.0\n",
      "[ 0.50193465] 0.0\n",
      "[ 0.49401903] 0.0\n",
      "[ 0.49837694] 1.0\n",
      "[ 0.51053607] 0.0\n",
      "[ 0.49817157] 1.0\n",
      "[ 0.49834272] 0.0\n",
      "[ 0.49725136] 0.0\n",
      "[ 0.49664417] 0.0\n",
      "[ 0.49664497] 0.0\n",
      "[ 0.50491357] 0.0\n",
      "[ 0.49550632] 0.0\n",
      "[ 0.50257879] 1.0\n",
      "[ 0.50278932] 1.0\n",
      "[ 0.50326729] 0.0\n",
      "[ 0.4998503] 0.0\n",
      "[ 0.49312639] 1.0\n",
      "[ 0.50145501] 0.0\n",
      "[ 0.49814436] 0.0\n",
      "[ 0.4936372] 1.0\n",
      "[ 0.49794656] 1.0\n",
      "[ 0.50699359] 1.0\n",
      "[ 0.50151479] 1.0\n",
      "[ 0.49557024] 1.0\n",
      "[ 0.50606352] 1.0\n",
      "[ 0.50391573] 1.0\n",
      "[ 0.49407154] 1.0\n",
      "[ 0.49450329] 1.0\n",
      "[ 0.49263579] 0.0\n",
      "[ 0.49966621] 1.0\n",
      "[ 0.50403774] 0.0\n",
      "[ 0.50619203] 0.0\n",
      "[ 0.50061059] 1.0\n",
      "[ 0.496479] 1.0\n",
      "[ 0.50345063] 0.0\n",
      "[ 0.50460392] 0.0\n",
      "[ 0.49573886] 0.0\n",
      "[ 0.49566197] 1.0\n",
      "[ 0.49966007] 1.0\n",
      "[ 0.49635649] 0.0\n",
      "[ 0.50660402] 0.0\n",
      "[ 0.49479997] 0.0\n",
      "[ 0.49856505] 1.0\n",
      "[ 0.49781349] 1.0\n",
      "[ 0.50080991] 1.0\n",
      "[ 0.49898198] 1.0\n",
      "[ 0.49843332] 0.0\n",
      "[ 0.49446589] 0.0\n",
      "[ 0.50252354] 1.0\n",
      "[ 0.50084811] 1.0\n",
      "[ 0.49948409] 1.0\n",
      "[ 0.49550363] 1.0\n",
      "[ 0.50009495] 0.0\n",
      "[ 0.49683267] 1.0\n",
      "[ 0.50050271] 0.0\n",
      "[ 0.50134677] 1.0\n",
      "[ 0.49859753] 1.0\n",
      "[ 0.50134581] 1.0\n",
      "[ 0.49945685] 1.0\n",
      "[ 0.49916843] 1.0\n",
      "[ 0.51256162] 0.0\n",
      "[ 0.49993283] 1.0\n",
      "[ 0.49510697] 1.0\n",
      "[ 0.50031805] 1.0\n",
      "[ 0.49765664] 1.0\n",
      "[ 0.49867517] 0.0\n",
      "[ 0.49784097] 0.0\n",
      "[ 0.49750012] 1.0\n",
      "[ 0.49942589] 1.0\n",
      "[ 0.50486088] 0.0\n",
      "[ 0.49620745] 1.0\n",
      "[ 0.50360084] 0.0\n",
      "[ 0.49750629] 0.0\n",
      "[ 0.49891227] 0.0\n",
      "[ 0.50410861] 0.0\n",
      "[ 0.49942791] 0.0\n",
      "[ 0.49898285] 0.0\n",
      "[ 0.49922147] 0.0\n",
      "[ 0.50300711] 0.0\n",
      "[ 0.49443984] 0.0\n",
      "[ 0.49750865] 1.0\n",
      "[ 0.49872321] 1.0\n",
      "[ 0.49652371] 0.0\n",
      "[ 0.49448121] 1.0\n",
      "[ 0.50204587] 1.0\n",
      "[ 0.5069682] 1.0\n",
      "[ 0.5021466] 1.0\n",
      "[ 0.49652493] 0.0\n",
      "[ 0.50002432] 1.0\n",
      "[ 0.49874458] 1.0\n",
      "[ 0.50248367] 0.0\n",
      "[ 0.50041801] 1.0\n",
      "[ 0.50827444] 0.0\n",
      "[ 0.49402153] 1.0\n",
      "[ 0.50230026] 0.0\n",
      "[ 0.50105029] 1.0\n",
      "[ 0.49861008] 0.0\n",
      "[ 0.49794969] 0.0\n",
      "[ 0.49041814] 0.0\n",
      "[ 0.49863088] 0.0\n",
      "[ 0.49619833] 0.0\n",
      "[ 0.49888694] 0.0\n",
      "[ 0.4974139] 0.0\n",
      "[ 0.50153792] 0.0\n",
      "[ 0.49839804] 0.0\n",
      "[ 0.49357706] 1.0\n",
      "[ 0.49972203] 0.0\n",
      "[ 0.49721098] 1.0\n",
      "[ 0.4959029] 0.0\n",
      "[ 0.49849579] 0.0\n",
      "[ 0.49628672] 1.0\n",
      "[ 0.49459183] 0.0\n",
      "[ 0.50001019] 0.0\n",
      "[ 0.49916577] 0.0\n",
      "[ 0.49340212] 0.0\n",
      "[ 0.48820767] 1.0\n",
      "[ 0.49669436] 1.0\n",
      "[ 0.50634134] 0.0\n",
      "[ 0.4986499] 1.0\n",
      "[ 0.50179619] 0.0\n",
      "[ 0.50092769] 0.0\n",
      "[ 0.50801933] 0.0\n",
      "[ 0.50346196] 1.0\n",
      "[ 0.5016436] 1.0\n",
      "[ 0.50545019] 0.0\n",
      "[ 0.4954856] 1.0\n",
      "[ 0.50199682] 0.0\n",
      "[ 0.50492823] 1.0\n",
      "[ 0.50802034] 0.0\n",
      "[ 0.4953354] 0.0\n",
      "[ 0.50144893] 0.0\n",
      "[ 0.49745864] 1.0\n",
      "[ 0.50460112] 1.0\n",
      "[ 0.50467771] 1.0\n",
      "[ 0.49562773] 1.0\n",
      "[ 0.49590123] 1.0\n",
      "[ 0.4949961] 1.0\n",
      "[ 0.50411862] 0.0\n",
      "[ 0.49680513] 1.0\n",
      "[ 0.51217151] 1.0\n",
      "[ 0.50020206] 1.0\n",
      "[ 0.49428394] 0.0\n",
      "[ 0.50603485] 1.0\n",
      "[ 0.49681583] 1.0\n",
      "[ 0.49843514] 0.0\n",
      "[ 0.50101304] 0.0\n",
      "[ 0.50166237] 1.0\n",
      "[ 0.50274056] 0.0\n",
      "[ 0.49921441] 0.0\n",
      "[ 0.49618408] 0.0\n",
      "[ 0.49815306] 1.0\n",
      "[ 0.49973634] 1.0\n",
      "[ 0.50102806] 1.0\n",
      "[ 0.49554947] 1.0\n",
      "[ 0.49510503] 0.0\n",
      "[ 0.50254852] 0.0\n",
      "[ 0.50200945] 1.0\n",
      "[ 0.50932521] 0.0\n",
      "[ 0.49884039] 0.0\n",
      "[ 0.49787617] 1.0\n",
      "[ 0.49776697] 1.0\n",
      "[ 0.50093514] 0.0\n",
      "[ 0.50742924] 1.0\n",
      "[ 0.50093049] 0.0\n",
      "[ 0.49276939] 1.0\n",
      "[ 0.50792408] 1.0\n",
      "[ 0.50037026] 1.0\n",
      "[ 0.49833211] 1.0\n",
      "[ 0.50406736] 1.0\n",
      "[ 0.4978531] 0.0\n",
      "[ 0.49263924] 1.0\n",
      "[ 0.49907687] 0.0\n",
      "[ 0.50103009] 0.0\n",
      "[ 0.49237704] 1.0\n",
      "[ 0.49844769] 0.0\n",
      "[ 0.49797136] 0.0\n",
      "[ 0.50179172] 0.0\n",
      "[ 0.50252652] 1.0\n",
      "[ 0.50508165] 1.0\n",
      "[ 0.49536893] 1.0\n",
      "[ 0.49564719] 1.0\n",
      "[ 0.49731976] 0.0\n",
      "[ 0.49370444] 1.0\n",
      "[ 0.4989633] 0.0\n",
      "[ 0.49683934] 0.0\n",
      "[ 0.49650833] 1.0\n",
      "[ 0.50184315] 1.0\n",
      "[ 0.50158387] 0.0\n",
      "[ 0.51001447] 0.0\n",
      "[ 0.50377858] 1.0\n",
      "[ 0.49633563] 0.0\n",
      "[ 0.49364683] 0.0\n",
      "[ 0.50238836] 0.0\n",
      "[ 0.49766457] 1.0\n",
      "[ 0.5023846] 1.0\n",
      "[ 0.4943929] 0.0\n",
      "[ 0.49805519] 1.0\n",
      "[ 0.50492674] 0.0\n",
      "[ 0.50079262] 1.0\n",
      "[ 0.49872324] 1.0\n",
      "[ 0.49935338] 1.0\n",
      "[ 0.49712428] 0.0\n",
      "[ 0.5033775] 0.0\n",
      "[ 0.49567607] 0.0\n",
      "[ 0.49851024] 0.0\n",
      "[ 0.501266] 1.0\n",
      "[ 0.49543011] 0.0\n",
      "[ 0.50610083] 1.0\n",
      "[ 0.50321102] 1.0\n",
      "[ 0.49980387] 1.0\n",
      "[ 0.49667677] 1.0\n",
      "[ 0.49512529] 1.0\n",
      "[ 0.49733204] 1.0\n",
      "[ 0.5007484] 1.0\n",
      "[ 0.4975442] 1.0\n",
      "[ 0.49865386] 0.0\n",
      "[ 0.50790334] 1.0\n",
      "[ 0.503344] 0.0\n",
      "[ 0.49938861] 0.0\n",
      "[ 0.4932209] 0.0\n",
      "[ 0.49658263] 0.0\n",
      "[ 0.49637526] 1.0\n",
      "[ 0.49820486] 0.0\n",
      "[ 0.49675959] 0.0\n",
      "[ 0.49554178] 1.0\n",
      "[ 0.49607116] 0.0\n",
      "[ 0.49305439] 1.0\n",
      "[ 0.50866169] 1.0\n",
      "[ 0.50328428] 1.0\n",
      "[ 0.49641219] 1.0\n",
      "[ 0.49679631] 1.0\n",
      "[ 0.49722734] 1.0\n",
      "[ 0.50182629] 0.0\n",
      "[ 0.50600463] 0.0\n",
      "[ 0.49262851] 0.0\n",
      "[ 0.49739912] 0.0\n",
      "[ 0.49676031] 0.0\n",
      "[ 0.50174063] 1.0\n",
      "[ 0.49477991] 0.0\n",
      "[ 0.49882656] 1.0\n",
      "[ 0.49724925] 0.0\n",
      "[ 0.50945866] 0.0\n",
      "[ 0.50376457] 0.0\n",
      "[ 0.50884902] 0.0\n",
      "[ 0.49716678] 1.0\n",
      "[ 0.5164808] 0.0\n",
      "[ 0.49784756] 0.0\n",
      "[ 0.49996725] 1.0\n",
      "[ 0.50234616] 0.0\n",
      "[ 0.50581425] 0.0\n",
      "[ 0.49839306] 1.0\n",
      "[ 0.49649191] 0.0\n",
      "[ 0.50061148] 0.0\n",
      "[ 0.49774098] 0.0\n",
      "[ 0.49677479] 0.0\n",
      "[ 0.49748114] 1.0\n",
      "[ 0.49474764] 0.0\n",
      "[ 0.49176684] 0.0\n",
      "[ 0.50027394] 0.0\n",
      "[ 0.49533546] 0.0\n",
      "[ 0.50339693] 1.0\n",
      "[ 0.50127649] 1.0\n",
      "[ 0.49930173] 1.0\n",
      "[ 0.4979268] 1.0\n",
      "[ 0.50000936] 0.0\n",
      "[ 0.50033969] 1.0\n",
      "[ 0.49669999] 1.0\n",
      "[ 0.49613285] 1.0\n",
      "[ 0.50069165] 0.0\n",
      "[ 0.50365907] 0.0\n",
      "[ 0.49568471] 1.0\n",
      "[ 0.50012457] 0.0\n",
      "[ 0.50461483] 0.0\n",
      "[ 0.49280474] 0.0\n",
      "[ 0.4962121] 0.0\n",
      "[ 0.50393552] 0.0\n",
      "[ 0.49909776] 0.0\n",
      "[ 0.50066221] 1.0\n",
      "[ 0.49674037] 1.0\n",
      "[ 0.50059193] 1.0\n",
      "[ 0.49496982] 1.0\n",
      "[ 0.49574426] 1.0\n",
      "[ 0.50519693] 1.0\n",
      "[ 0.49883437] 1.0\n",
      "[ 0.50073075] 1.0\n",
      "[ 0.49459821] 1.0\n",
      "[ 0.49785513] 0.0\n",
      "[ 0.49978849] 0.0\n",
      "[ 0.49679863] 1.0\n",
      "[ 0.4966183] 0.0\n",
      "[ 0.50428188] 0.0\n",
      "[ 0.50364584] 0.0\n",
      "[ 0.50299752] 1.0\n",
      "[ 0.4981989] 0.0\n",
      "[ 0.49697036] 0.0\n",
      "[ 0.49887681] 1.0\n",
      "[ 0.50775254] 0.0\n",
      "[ 0.49506485] 1.0\n",
      "[ 0.49947765] 0.0\n",
      "[ 0.49852878] 0.0\n",
      "[ 0.49959102] 1.0\n",
      "[ 0.5007599] 1.0\n",
      "[ 0.50120181] 0.0\n",
      "[ 0.50655335] 0.0\n",
      "[ 0.50217742] 1.0\n",
      "[ 0.49872899] 1.0\n",
      "[ 0.49673492] 0.0\n",
      "[ 0.50190181] 0.0\n",
      "[ 0.50349015] 1.0\n",
      "[ 0.4980008] 0.0\n",
      "[ 0.49734074] 0.0\n",
      "[ 0.49055898] 1.0\n",
      "[ 0.49945399] 0.0\n",
      "[ 0.50330961] 0.0\n",
      "[ 0.49759787] 0.0\n",
      "[ 0.49820858] 1.0\n",
      "[ 0.50526625] 1.0\n",
      "[ 0.49865261] 0.0\n",
      "[ 0.49681872] 1.0\n",
      "[ 0.49841169] 0.0\n",
      "[ 0.49749681] 1.0\n",
      "[ 0.49916163] 0.0\n",
      "[ 0.49755189] 1.0\n",
      "[ 0.49613789] 0.0\n",
      "[ 0.50366521] 1.0\n",
      "[ 0.50435674] 1.0\n",
      "[ 0.5009262] 1.0\n",
      "[ 0.49916932] 0.0\n",
      "[ 0.49570689] 1.0\n",
      "[ 0.49530971] 1.0\n",
      "[ 0.5004071] 0.0\n",
      "[ 0.49944487] 0.0\n",
      "[ 0.5013926] 0.0\n",
      "[ 0.49562892] 1.0\n",
      "[ 0.49647817] 1.0\n",
      "[ 0.49392816] 0.0\n",
      "[ 0.49925718] 0.0\n",
      "[ 0.503932] 0.0\n",
      "[ 0.50324494] 1.0\n",
      "[ 0.50117987] 1.0\n",
      "[ 0.49158183] 0.0\n",
      "[ 0.49545163] 0.0\n",
      "[ 0.49319145] 0.0\n",
      "[ 0.50443006] 0.0\n",
      "[ 0.49603075] 0.0\n",
      "[ 0.49880439] 0.0\n",
      "[ 0.49457654] 1.0\n",
      "[ 0.50167239] 0.0\n",
      "[ 0.50540024] 1.0\n",
      "[ 0.50139618] 0.0\n",
      "[ 0.49513745] 0.0\n",
      "[ 0.49559361] 0.0\n",
      "[ 0.49903685] 0.0\n",
      "[ 0.49658617] 1.0\n",
      "[ 0.5013504] 0.0\n",
      "[ 0.49169508] 1.0\n",
      "[ 0.50184762] 1.0\n",
      "[ 0.50005949] 1.0\n",
      "[ 0.49793658] 1.0\n",
      "[ 0.50046057] 0.0\n",
      "[ 0.50295502] 1.0\n",
      "[ 0.49442565] 0.0\n",
      "[ 0.49990988] 1.0\n",
      "[ 0.50205797] 1.0\n",
      "[ 0.50296313] 0.0\n",
      "[ 0.49884191] 1.0\n",
      "[ 0.49532095] 1.0\n",
      "[ 0.4987472] 0.0\n",
      "[ 0.49805] 0.0\n",
      "[ 0.49955833] 1.0\n",
      "[ 0.49495551] 1.0\n",
      "[ 0.49755323] 0.0\n",
      "[ 0.50564408] 1.0\n",
      "[ 0.49756655] 0.0\n",
      "[ 0.49811086] 1.0\n",
      "[ 0.50049168] 0.0\n",
      "[ 0.50178993] 0.0\n",
      "[ 0.49803686] 1.0\n",
      "[ 0.49832663] 0.0\n",
      "[ 0.5029394] 1.0\n",
      "[ 0.49133056] 0.0\n",
      "[ 0.48687574] 1.0\n",
      "[ 0.50336945] 0.0\n",
      "[ 0.49460512] 0.0\n",
      "[ 0.50228] 1.0\n",
      "[ 0.50415844] 0.0\n",
      "[ 0.49818808] 1.0\n",
      "[ 0.49433586] 1.0\n",
      "[ 0.50026923] 1.0\n",
      "[ 0.50397891] 0.0\n",
      "[ 0.50387681] 1.0\n",
      "[ 0.50679547] 1.0\n",
      "[ 0.4949376] 1.0\n",
      "[ 0.4962827] 0.0\n",
      "[ 0.49572399] 1.0\n",
      "[ 0.49360657] 0.0\n",
      "[ 0.49980834] 1.0\n",
      "[ 0.49403539] 0.0\n",
      "[ 0.49582282] 1.0\n",
      "[ 0.49899575] 1.0\n",
      "[ 0.50724328] 1.0\n",
      "[ 0.49942344] 1.0\n",
      "[ 0.49289364] 0.0\n",
      "[ 0.50066859] 1.0\n",
      "[ 0.49653235] 0.0\n",
      "[ 0.50043797] 0.0\n",
      "[ 0.50091434] 0.0\n",
      "[ 0.50008643] 0.0\n",
      "[ 0.4961904] 0.0\n",
      "[ 0.50210083] 0.0\n",
      "[ 0.49806622] 1.0\n",
      "[ 0.50462949] 1.0\n",
      "[ 0.49516585] 0.0\n",
      "[ 0.49723914] 1.0\n",
      "[ 0.4957118] 1.0\n",
      "[ 0.49350709] 1.0\n",
      "[ 0.50431663] 0.0\n",
      "[ 0.49471778] 1.0\n",
      "[ 0.50665665] 0.0\n",
      "[ 0.49697655] 1.0\n",
      "[ 0.49916539] 1.0\n",
      "[ 0.49704695] 1.0\n",
      "[ 0.50346631] 0.0\n",
      "[ 0.49974418] 0.0\n",
      "[ 0.49291551] 1.0\n",
      "[ 0.50030595] 1.0\n",
      "[ 0.50195986] 1.0\n",
      "[ 0.49153036] 0.0\n",
      "[ 0.49543172] 1.0\n",
      "[ 0.49446952] 1.0\n",
      "[ 0.49767974] 1.0\n",
      "[ 0.49520946] 0.0\n",
      "[ 0.51070905] 0.0\n",
      "[ 0.50261921] 0.0\n",
      "[ 0.50209099] 1.0\n",
      "[ 0.50265419] 0.0\n",
      "[ 0.49924549] 1.0\n",
      "[ 0.49832284] 1.0\n",
      "[ 0.49599126] 0.0\n",
      "[ 0.49619046] 1.0\n",
      "[ 0.50111824] 1.0\n",
      "[ 0.50384986] 1.0\n",
      "[ 0.49966851] 0.0\n",
      "[ 0.50796831] 1.0\n",
      "[ 0.49924967] 0.0\n",
      "[ 0.49988934] 0.0\n",
      "[ 0.49924141] 0.0\n",
      "[ 0.50424623] 0.0\n",
      "[ 0.49590585] 0.0\n",
      "[ 0.50004023] 0.0\n",
      "[ 0.50401026] 0.0\n",
      "[ 0.50325716] 0.0\n",
      "[ 0.50889081] 1.0\n",
      "[ 0.50236851] 1.0\n",
      "[ 0.49930817] 0.0\n",
      "[ 0.49423546] 1.0\n",
      "[ 0.49589658] 1.0\n",
      "[ 0.49658829] 1.0\n",
      "[ 0.50132567] 1.0\n",
      "[ 0.50478256] 1.0\n",
      "[ 0.50165719] 0.0\n",
      "[ 0.50206244] 0.0\n",
      "[ 0.49732226] 1.0\n",
      "[ 0.48938903] 0.0\n",
      "[ 0.51024652] 1.0\n",
      "[ 0.49088845] 0.0\n",
      "[ 0.4994483] 0.0\n",
      "[ 0.5018031] 0.0\n",
      "[ 0.50155777] 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.50343961] 1.0\n",
      "[ 0.5000819] 1.0\n",
      "[ 0.49865025] 0.0\n",
      "[ 0.49681726] 1.0\n",
      "[ 0.49356502] 1.0\n",
      "[ 0.49230942] 1.0\n",
      "[ 0.50901389] 1.0\n",
      "[ 0.4972553] 0.0\n",
      "[ 0.49377158] 1.0\n",
      "[ 0.50426728] 1.0\n",
      "[ 0.49804002] 0.0\n",
      "[ 0.50168842] 1.0\n",
      "[ 0.51162958] 1.0\n",
      "[ 0.51099432] 0.0\n",
      "[ 0.48830202] 0.0\n",
      "[ 0.50224847] 0.0\n",
      "[ 0.49980524] 1.0\n",
      "[ 0.50359964] 1.0\n",
      "[ 0.49590388] 1.0\n",
      "[ 0.50837487] 1.0\n",
      "[ 0.49407265] 0.0\n",
      "[ 0.4993909] 1.0\n",
      "[ 0.4998689] 1.0\n",
      "[ 0.50375253] 0.0\n",
      "[ 0.50332403] 1.0\n",
      "[ 0.5004974] 0.0\n",
      "[ 0.5034942] 0.0\n",
      "[ 0.49988356] 0.0\n",
      "[ 0.49604708] 0.0\n",
      "[ 0.4939509] 1.0\n",
      "[ 0.50321656] 1.0\n",
      "[ 0.50214088] 1.0\n",
      "[ 0.50598139] 1.0\n",
      "[ 0.49866584] 1.0\n",
      "[ 0.50062346] 1.0\n",
      "[ 0.49695024] 1.0\n",
      "[ 0.4970617] 1.0\n",
      "[ 0.50326949] 0.0\n",
      "[ 0.49301413] 0.0\n",
      "[ 0.49294585] 0.0\n",
      "[ 0.49600267] 0.0\n",
      "[ 0.4972885] 1.0\n",
      "[ 0.50562668] 1.0\n",
      "[ 0.50260741] 0.0\n",
      "[ 0.49854508] 1.0\n",
      "[ 0.49949947] 0.0\n",
      "[ 0.49800453] 1.0\n",
      "[ 0.4985387] 0.0\n",
      "[ 0.50457788] 0.0\n",
      "[ 0.49526873] 1.0\n",
      "[ 0.49969324] 1.0\n",
      "[ 0.4987312] 1.0\n",
      "[ 0.49826232] 0.0\n",
      "[ 0.49562755] 1.0\n",
      "[ 0.50452667] 0.0\n",
      "[ 0.49248835] 1.0\n",
      "[ 0.49210832] 0.0\n",
      "[ 0.50382793] 0.0\n",
      "[ 0.49487314] 0.0\n",
      "[ 0.50376624] 0.0\n",
      "[ 0.50576872] 1.0\n",
      "[ 0.50090259] 1.0\n",
      "[ 0.49683595] 1.0\n",
      "[ 0.49668235] 0.0\n",
      "[ 0.49890539] 0.0\n",
      "[ 0.50181299] 1.0\n",
      "[ 0.49607345] 1.0\n",
      "[ 0.49523926] 0.0\n",
      "[ 0.4957287] 1.0\n",
      "[ 0.50096798] 0.0\n",
      "[ 0.50704831] 0.0\n",
      "[ 0.50782979] 0.0\n",
      "[ 0.49938765] 0.0\n",
      "[ 0.51050252] 0.0\n",
      "[ 0.50003219] 0.0\n",
      "[ 0.50060076] 0.0\n",
      "[ 0.49165207] 1.0\n",
      "[ 0.50411427] 1.0\n",
      "[ 0.50034732] 1.0\n",
      "[ 0.50277215] 0.0\n",
      "[ 0.49839887] 0.0\n",
      "[ 0.49842229] 1.0\n",
      "[ 0.50042623] 0.0\n",
      "[ 0.49589741] 1.0\n",
      "[ 0.49890423] 1.0\n",
      "[ 0.50139183] 1.0\n",
      "[ 0.50962383] 0.0\n",
      "[ 0.50102514] 1.0\n",
      "[ 0.50356799] 0.0\n",
      "[ 0.49824539] 1.0\n",
      "[ 0.50034535] 1.0\n",
      "[ 0.50002676] 1.0\n",
      "[ 0.50532526] 0.0\n",
      "[ 0.50164896] 1.0\n",
      "[ 0.49749514] 0.0\n",
      "[ 0.49799341] 0.0\n",
      "[ 0.49914849] 0.0\n",
      "[ 0.50024968] 1.0\n",
      "[ 0.50053799] 0.0\n",
      "[ 0.50269538] 1.0\n",
      "[ 0.50189209] 1.0\n",
      "[ 0.49595439] 1.0\n",
      "[ 0.49988618] 1.0\n",
      "[ 0.50585878] 0.0\n",
      "[ 0.49824232] 1.0\n",
      "[ 0.50734293] 0.0\n",
      "[ 0.50179929] 0.0\n",
      "[ 0.49795151] 1.0\n",
      "[ 0.51117277] 0.0\n",
      "[ 0.49707448] 0.0\n",
      "[ 0.49378633] 1.0\n",
      "[ 0.49553886] 1.0\n",
      "[ 0.49320284] 1.0\n",
      "[ 0.49522758] 1.0\n",
      "[ 0.50380665] 1.0\n",
      "[ 0.49363568] 1.0\n",
      "[ 0.49688339] 0.0\n",
      "[ 0.49490276] 0.0\n",
      "[ 0.49750727] 0.0\n",
      "[ 0.50182384] 1.0\n",
      "[ 0.49725494] 0.0\n",
      "[ 0.49570829] 1.0\n",
      "[ 0.49767983] 0.0\n",
      "[ 0.49797043] 1.0\n",
      "[ 0.5021444] 1.0\n",
      "[ 0.50186568] 1.0\n",
      "[ 0.49328682] 0.0\n",
      "[ 0.50106746] 0.0\n",
      "[ 0.49164182] 1.0\n",
      "[ 0.49032679] 0.0\n",
      "[ 0.50754935] 0.0\n",
      "[ 0.50480294] 0.0\n",
      "[ 0.4988758] 0.0\n",
      "[ 0.50312561] 0.0\n",
      "[ 0.50420433] 1.0\n",
      "[ 0.49604806] 0.0\n",
      "[ 0.49590319] 0.0\n",
      "[ 0.5030185] 0.0\n",
      "[ 0.50350052] 0.0\n",
      "[ 0.49560025] 1.0\n",
      "[ 0.49528995] 1.0\n",
      "[ 0.49418265] 1.0\n",
      "1000/1000 [==============================] - 0s 67us/step\n",
      "[0.69340801048278811, 0.25013041353225707, 0.49199999999999999]\n"
     ]
    }
   ],
   "source": [
    "init_predz = q_model.predict(x=[vqz, vsz])\n",
    "for hat,ture in zip(init_predz,vpfz):\n",
    "    print(hat, ture)\n",
    "    \n",
    "print(q_model.evaluate(x=[vqz,vsz], y=vpfz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NN_utils.BigTable object at 0x7faddab0fbe0> <NN_utils.BigTable object at 0x7fadda57f3c8> Tensor(\"psi_select_3:0\", shape=(?, 1), dtype=int32) Tensor(\"q_select_3:0\", shape=(?, 1), dtype=int32)\n",
      "psi_sel shape (?, 1)\n",
      "selector shape (?, 1)\n",
      "flat selector shape (?,)\n",
      "call kk (100, 10)\n",
      "'rows' shape, (?, 10)\n",
      "selector shape (?, 1)\n",
      "flat selector shape (?,)\n",
      "call kk (1000, 10)\n",
      "'rows' shape, (?, 10)\n",
      "Pr (?, 1)\n"
     ]
    }
   ],
   "source": [
    "    wgts = q_model.get_weights()\n",
    "#     q_model = generate_qs_model(qn_table, s_table, Adam())\n",
    "#     q_model = generate_qs_model(qn_table, s_table, Adam())\n",
    "    \n",
    "    qn_table.trainable = True\n",
    "    s_table.trainable = True\n",
    "    q_model = generate_qs_model(qn_table, s_table, Adam())\n",
    "    q_model.set_weights(wgts)\n",
    "    \n",
    "#     qn_table.trainable = False\n",
    "#     s_table.trainable = True\n",
    "#     s_model = generate_qs_model(qn_table, s_table, Adam())\n",
    "    \n",
    "    min_loss = math.inf\n",
    "    max_acc = 0\n",
    "    max_tr_acc = 0\n",
    "    init_patience = 10\n",
    "    patience = init_patience\n",
    "    best_q_weights = None\n",
    "    best_s_weights = None\n",
    "\n",
    "    zz = numpy.zeros(len(pfz))\n",
    "    vzz = numpy.zeros(len(vpfz))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98000\n"
     ]
    }
   ],
   "source": [
    "print(len(pfz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Train on 98000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "98000/98000 [==============================] - 14s 141us/step - loss: 0.6524 - mean_squared_error: 0.2306 - acc: 0.6071 - val_loss: 0.6023 - val_mean_squared_error: 0.2075 - val_acc: 0.6890\n",
      "SUM DCOV 0.0476473840081\n",
      "Q MAG 1.01949\n",
      "VAL: 0.602342459559 0.689000001252\n",
      "av no of qn components: 7.63\n",
      "av qn component: 1.01949\n",
      "new min loss\n",
      "patience is now 10\n",
      "1\n",
      "Train on 98000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "98000/98000 [==============================] - 13s 133us/step - loss: 0.6137 - mean_squared_error: 0.2130 - acc: 0.6604 - val_loss: 0.6089 - val_mean_squared_error: 0.2107 - val_acc: 0.6490\n",
      "SUM DCOV 0.127082737809\n",
      "Q MAG 1.29191\n",
      "VAL: 0.608935218155 0.649000002444\n",
      "av no of qn components: 10.0\n",
      "av qn component: 1.29191\n",
      "patience is now 9\n",
      "2\n",
      "Train on 98000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "98000/98000 [==============================] - 12s 127us/step - loss: 0.6107 - mean_squared_error: 0.2119 - acc: 0.6607 - val_loss: 0.6057 - val_mean_squared_error: 0.2089 - val_acc: 0.6720\n",
      "SUM DCOV 0.198728359902\n",
      "Q MAG 1.29701\n",
      "VAL: 0.605680993497 0.672000001073\n",
      "av no of qn components: 10.0\n",
      "av qn component: 1.29701\n",
      "patience is now 8\n",
      "3\n",
      "Train on 98000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "98000/98000 [==============================] - 12s 125us/step - loss: 0.6093 - mean_squared_error: 0.2113 - acc: 0.6624 - val_loss: 0.6066 - val_mean_squared_error: 0.2077 - val_acc: 0.6820\n",
      "SUM DCOV 0.269560099932\n",
      "Q MAG 1.30029\n",
      "VAL: 0.606619377434 0.682000001073\n",
      "av no of qn components: 10.0\n",
      "av qn component: 1.30029\n",
      "patience is now 7\n",
      "4\n",
      "Train on 98000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "98000/98000 [==============================] - 13s 130us/step - loss: 0.6078 - mean_squared_error: 0.2106 - acc: 0.6651 - val_loss: 0.6094 - val_mean_squared_error: 0.2094 - val_acc: 0.6680\n",
      "SUM DCOV 0.312016968967\n",
      "Q MAG 1.30151\n",
      "VAL: 0.609436362684 0.668000002205\n",
      "av no of qn components: 10.0\n",
      "av qn component: 1.30151\n",
      "patience is now 6\n",
      "5\n",
      "Train on 98000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "98000/98000 [==============================] - 15s 154us/step - loss: 0.6071 - mean_squared_error: 0.2104 - acc: 0.6659 - val_loss: 0.6058 - val_mean_squared_error: 0.2074 - val_acc: 0.6850\n",
      "SUM DCOV 0.354368230073\n",
      "Q MAG 1.30277\n",
      "VAL: 0.60583227396 0.68500000298\n",
      "av no of qn components: 10.0\n",
      "av qn component: 1.30277\n",
      "patience is now 5\n",
      "6\n",
      "Train on 98000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "98000/98000 [==============================] - 22s 222us/step - loss: 0.6063 - mean_squared_error: 0.2100 - acc: 0.6672 - val_loss: 0.6086 - val_mean_squared_error: 0.2082 - val_acc: 0.6870\n",
      "SUM DCOV 0.38711566222\n",
      "Q MAG 1.30479\n",
      "VAL: 0.60855009526 0.687000001073\n",
      "av no of qn components: 10.0\n",
      "av qn component: 1.30479\n",
      "patience is now 4\n",
      "7\n",
      "Train on 98000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "98000/98000 [==============================] - 31s 317us/step - loss: 0.6060 - mean_squared_error: 0.2098 - acc: 0.6668 - val_loss: 0.6099 - val_mean_squared_error: 0.2082 - val_acc: 0.6930\n",
      "SUM DCOV 0.414822644511\n",
      "Q MAG 1.30767\n",
      "VAL: 0.609945459664 0.693000001609\n",
      "av no of qn components: 10.0\n",
      "av qn component: 1.30767\n",
      "patience is now 3\n",
      "8\n",
      "Train on 98000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "98000/98000 [==============================] - 25s 255us/step - loss: 0.6056 - mean_squared_error: 0.2097 - acc: 0.6680 - val_loss: 0.6088 - val_mean_squared_error: 0.2078 - val_acc: 0.6910\n",
      "SUM DCOV 0.435581973036\n",
      "Q MAG 1.30862\n",
      "VAL: 0.608847621083 0.691000002623\n",
      "av no of qn components: 10.0\n",
      "av qn component: 1.30862\n",
      "patience is now 2\n",
      "9\n",
      "Train on 98000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "98000/98000 [==============================] - 26s 263us/step - loss: 0.6054 - mean_squared_error: 0.2096 - acc: 0.6678 - val_loss: 0.6175 - val_mean_squared_error: 0.2113 - val_acc: 0.6750\n",
      "SUM DCOV 0.453037746057\n",
      "Q MAG 1.31367\n",
      "VAL: 0.61749532491 0.67500000447\n",
      "av no of qn components: 10.0\n",
      "av qn component: 1.31367\n",
      "patience is now 1\n",
      "10\n",
      "Train on 98000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "98000/98000 [==============================] - 27s 274us/step - loss: 0.6047 - mean_squared_error: 0.2093 - acc: 0.6676 - val_loss: 0.6083 - val_mean_squared_error: 0.2074 - val_acc: 0.6770\n",
      "SUM DCOV 0.469646524016\n",
      "Q MAG 1.31827\n",
      "VAL: 0.608285875022 0.677000000775\n",
      "av no of qn components: 10.0\n",
      "av qn component: 1.31827\n",
      "patience spent\n",
      "final: 0.602342459559 0.668040819571 0.689000001252\n"
     ]
    }
   ],
   "source": [
    "    for it in range(1000):\n",
    "        print(it)\n",
    "        psi_ws = s_table.get_weights()[0]\n",
    "        qn_ws = qn_table.get_weights()[0]\n",
    "#         preds = qs_model.predict(x=[qz, sz])\n",
    "#         for prix, p in enumerate(preds[0:100]):\n",
    "#             print(prix,\":\",qz.iloc[prix], sz.iloc[prix], p,pfz.iloc[prix])\n",
    "        \n",
    "#         h = qs_model.fit(x=[qz, sz], y=pfz, batch_size=256, shuffle=True, epochs=1, verbose=1, validation_split=min(.1, 1000/len(pfz)))\n",
    "#         print(h.history)\n",
    "#         loss, mse, acc = qs_model.train_on_batch(x=[qz,sz], y=pfz)\n",
    "#         val_loss, val_mse, val_acc = qs_model.evaluate(x=[vqz, vsz], y=vpfz)\n",
    "        h = q_model.fit(x=[qz,sz], y=numpy.array(pfz).reshape(-1,1), batch_size=10, shuffle=True, epochs=1, verbose=1, validation_data=[[vqz,vsz],vpfz])# validation_split=min(.1, 1000/len(pfz)))\n",
    "#         s_model.set_weights(q_model.get_weights())\n",
    "#         h = s_model.fit(x=[qz,sz], y=numpy.array(pfz).reshape(-1,1), batch_size=1000, shuffle=True, epochs=1, verbose=2)#, validation_data=[[vsz,vqz],vpfz])# validation_split=min(.1, 1000/len(pfz)))\n",
    "#         q_model.set_weights(s_model.get_weights())\n",
    "\n",
    "    \n",
    "        loss = h.history['loss'][-1]\n",
    "#         val_loss = loss\n",
    "#         mse = h.history['add_w_mean_squared_error'][-1]\n",
    "#         acc = h.history['sPr_prod_acc'][-1]\n",
    "\n",
    "        mse = 0 #h.history['sPr_prod_mean_squared_error'][-1]\n",
    "\n",
    "        acc = h.history['acc'][-1]\n",
    "\n",
    "#         val_acc = acc\n",
    "#         val_loss = loss\n",
    "        val_acc = h.history['val_acc'][-1]\n",
    "        val_loss = h.history['val_loss'][-1]\n",
    "\n",
    "#         val_acc = h.history['val_sPr_prod_acc'][-1]\n",
    "#         val_loss = h.history['val_sPr_prod_loss'][-1]\n",
    "#         mse = h.history['mean_squared_error'][-1]\n",
    "\n",
    "#         qs_model.train_on_batch(x=[qz,sz], y=pfz)\n",
    "#         loss,mse,acc = qs_model.evaluate(x=[qz,sz], y=pfz)\n",
    "#         print(loss, mse, acc)\n",
    "        print(\"SUM DCOV\", 0.5*(numpy.mean(numpy.abs(numpy.corrcoef(qn_ws.T))-numpy.eye(qn_ws.shape[1])) + numpy.mean(numpy.abs(numpy.corrcoef(psi_ws.T))-numpy.eye(psi_ws.shape[1]))))\n",
    "        print(\"Q MAG\", (numpy.mean(qn_ws)))\n",
    "        print(\"VAL:\", val_loss, val_acc)\n",
    "        \n",
    "        print(\"av no of qn components:\",numpy.mean(numpy.sum((qn_ws > .5), axis=1)))\n",
    "        print(\"av qn component:\",numpy.mean(numpy.mean(qn_ws, axis=1)))\n",
    "#         if val_loss <= min_loss or val_acc > max_acc:\n",
    "        if max_tr_acc < acc:\n",
    "            max_tr_acc = acc\n",
    "\n",
    "        if min_loss >= val_loss:\n",
    "            print(\"new min loss\")\n",
    "            min_loss = val_loss\n",
    "            max_acc = val_acc\n",
    "            best_q_weights = qn_table.get_weights()\n",
    "            best_s_weights = s_table.get_weights()\n",
    "            patience = init_patience\n",
    "        else:\n",
    "            patience -= 1\n",
    "#             qn_table.set_weights(best_q_weights * numpy.random.uniform(low=0.5, high=1.5, size=(len(best_q_weights), dim)))\n",
    "#             s_table.set_weights(best_s_weights * numpy.random.uniform(low=0.5, high=1.5, size=(len(best_s_weights), dim)))\n",
    "            \n",
    "#         if it % 10 == 0:\n",
    "        q_wgts_history.append( qn_table.get_weights()[0] ) \n",
    "        s_wgts_history.append( s_table.get_weights()[0] )\n",
    "    \n",
    "        if patience <= 0:\n",
    "#         else:\n",
    "            print(\"patience spent\")\n",
    "            break\n",
    "        print(\"patience is now {}\".format(patience))\n",
    "\n",
    "    qn_table.set_weights(best_q_weights)\n",
    "    s_table.set_weights(best_s_weights)\n",
    "\n",
    "    print(\"final:\", min_loss, max_tr_acc, max_acc)\n",
    "    \n",
    "#10000s testing, default LR, BS=256, no noise no q-gates\n",
    "# 1 = 64.0\n",
    "# 2 = 65.2\n",
    "# 4 = 66.2 \n",
    "# 8 = 65.2 \n",
    "# 16= 66.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 34us/step\n",
      "0.646543523788 0.227174803257 0.644\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [ True]\n",
      "True [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [False]\n",
      "False [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [ True]\n",
      "True [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [False]\n",
      "False [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "False [ True]\n",
      "True [False]\n",
      "False [ True]\n",
      "True [ True]\n",
      "True [False]\n",
      "False [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [False]\n",
      "True [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [ True]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [False]\n",
      "False [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "True [False]\n",
      "False [False]\n",
      "False [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [False]\n",
      "True [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [ True]\n",
      "False [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [False]\n",
      "False [False]\n",
      "True [False]\n",
      "False [ True]\n",
      "False [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [ True]\n",
      "False [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [False]\n",
      "True [False]\n",
      "False [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "False [ True]\n",
      "True [False]\n",
      "False [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "False [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [ True]\n",
      "False [False]\n",
      "False [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "True [False]\n",
      "True [False]\n",
      "False [ True]\n",
      "False [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "False [ True]\n",
      "True [ True]\n",
      "False [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [False]\n",
      "False [False]\n",
      "True [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "True [False]\n",
      "False [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "False [False]\n",
      "False [ True]\n",
      "True [ True]\n",
      "True [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [False]\n",
      "False [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [ True]\n",
      "False [ True]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [ True]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [ True]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "False [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "True [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "False [ True]\n",
      "False [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "True [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "False [ True]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "False [False]\n",
      "False [ True]\n",
      "True [ True]\n",
      "True [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [ True]\n",
      "True [False]\n",
      "False [ True]\n",
      "True [ True]\n",
      "False [ True]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [ True]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [False]\n",
      "False [False]\n",
      "False [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "True [False]\n",
      "False [False]\n",
      "False [ True]\n",
      "False [ True]\n",
      "False [False]\n",
      "False [ True]\n",
      "True [False]\n",
      "False [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "False [ True]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [False]\n",
      "False [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "True [False]\n",
      "False [ True]\n",
      "True [False]\n",
      "False [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [ True]\n",
      "False [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [ True]\n",
      "False [False]\n",
      "False [ True]\n",
      "False [ True]\n",
      "False [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "False [ True]\n",
      "False [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [ True]\n",
      "True [ True]\n",
      "False [ True]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "False [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [ True]\n",
      "True [False]\n",
      "True [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [False]\n",
      "False [False]\n",
      "False [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [ True]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "True [False]\n",
      "True [False]\n",
      "False [False]\n",
      "False [ True]\n",
      "False [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [ True]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [False]\n",
      "False [ True]\n",
      "False [ True]\n",
      "False [False]\n",
      "False [ True]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "False [ True]\n",
      "True [False]\n",
      "False [False]\n",
      "True [False]\n",
      "False [ True]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "False [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [ True]\n",
      "False [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [ True]\n",
      "False [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "True [False]\n",
      "True [False]\n",
      "True [False]\n",
      "False [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "False [ True]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [False]\n",
      "False [False]\n",
      "True [False]\n",
      "True [False]\n",
      "False [False]\n",
      "True [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "False [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "True [False]\n",
      "False [False]\n",
      "True [False]\n",
      "True [False]\n",
      "True [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "True [False]\n",
      "False [False]\n",
      "False [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "False [ True]\n",
      "True [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [ True]\n",
      "False [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "False [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [ True]\n",
      "True [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [ True]\n",
      "True [ True]\n",
      "True [False]\n",
      "False [False]\n",
      "False [ True]\n",
      "True [ True]\n",
      "True [False]\n",
      "False [False]\n",
      "False [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "False [ True]\n",
      "True [ True]\n",
      "False [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [ True]\n",
      "True [False]\n",
      "False [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [False]\n",
      "False [ True]\n",
      "False [False]\n",
      "False [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [ True]\n",
      "True [False]\n",
      "False [False]\n",
      "True [False]\n",
      "False [False]\n",
      "True [False]\n",
      "False [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [ True]\n",
      "False [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "False [False]\n",
      "False [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "True [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [ True]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [False]\n",
      "False [False]\n",
      "True [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [False]\n",
      "False [False]\n",
      "True [False]\n",
      "False [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [False]\n",
      "False [ True]\n",
      "True [ True]\n",
      "False [ True]\n",
      "False [False]\n",
      "False [ True]\n",
      "False [False]\n",
      "False [ True]\n",
      "False [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "True [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [False]\n",
      "False [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "False [False]\n",
      "True [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "False [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "False [False]\n",
      "True [False]\n",
      "False [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "True [False]\n",
      "True [False]\n",
      "True [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [False]\n",
      "False [ True]\n",
      "False [False]\n",
      "False [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [ True]\n",
      "True [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [ True]\n",
      "False [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "False [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "False [False]\n",
      "True [False]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [False]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [ True]\n",
      "True [False]\n",
      "False [False]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False [ True]\n",
      "True [False]\n",
      "False [False]\n",
      "False [ True]\n",
      "False [False]\n",
      "True [False]\n",
      "False [False]\n",
      "False [False]\n",
      "False [ True]\n",
      "True [False]\n",
      "0.644\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "val_loss, val_mse, val_acc = q_model.evaluate(x=[tqz, tsz], y=tpfz)\n",
    "print(val_loss, val_mse, val_acc)\n",
    "\n",
    "# evpfz = (numpy.exp(-vpfz) >= 0.5)\n",
    "# phat = (numpy.exp(-q_model.predict(x=[vqz, vsz])) >= 0.5)\n",
    "\n",
    "etpfz = (tpfz >= 0.5)\n",
    "phat = (q_model.predict(x=[tqz, tsz]) >= 0.5)\n",
    "\n",
    "\n",
    "for hat,ture in zip(etpfz, phat):\n",
    "    print(hat, ture)\n",
    "\n",
    "print(accuracy_score(etpfz, phat))\n",
    "#30 comp = .581\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from sklearn.metrics import confusion_matrix\n",
    "    y_hatz_raw = q_model.predict(x=[vqz, vsz]) \n",
    "    print(q_model.evaluate(x=[vqz, vsz], y=vpfz) )\n",
    "    \n",
    "    print(\"TN FP\\nFN TP\")\n",
    "    for thresh in [0.3, 0.4, 0.45, 0.5, 0.55, 0.6, 0.75, 0.8, 0.9]:\n",
    "        y_hatz = (y_hatz_raw >= thresh)\n",
    "        print(\"Threshold {}\".format(thresh))\n",
    "        print(len(y_hatz), sum(y_hatz))\n",
    "        print(confusion_matrix(vpfz, y_hatz),\"\\n\")\n",
    "        #     ix=0\n",
    "# #     if last_qn_table:\n",
    "#     for lw,w in zip(last_qn_table, qn_table.get_weights()[0]):\n",
    "#         if w != lw:\n",
    "#             print(\"{} : {} - {}->{}\".format(ix,qids_seen[ix],lw,w))            \n",
    "#         ix+=1\n",
    "#     last_qn_table = qn_table.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D | acc      |w>0       |igate     \\rng=20    |corr_reg\n",
    "# 2k|  \n",
    "# 1k| .        | .858/807 |\n",
    "# 5c| .\n",
    "# 4c| .849/795 | .848/796 | .848/795 |\n",
    "# 3c| .\n",
    "# 2c| .\n",
    "# 1c| .857/771 | .857/772 | .856/765 |\n",
    "# 50| .844/783\n",
    "# 30| .845/756 | .845/763 | .852/760 | .850/757 | .833/754\n",
    "# 20| .839/763\n",
    "# 10| .834/756 | .823/769 |                     | .815/745\n",
    "#  5| .818/747 | .-/-\n",
    "#  1| .771/684 | \n",
    "qn_ws = qn_table.get_weights()[0]\n",
    "psi_ws = s_table.get_weights()[0]\n",
    "# numpy.set_printoptions(precision=1)\n",
    "# with tf.Session() as sess:\n",
    "#     I = numpy.eye(qn_ws.shape[0])\n",
    "#     print(sess.run(tf.abs(tf_cov(tf.constant(qn_ws, dtype=tf.float32)) -I) ))\n",
    "#     print(sess.run(tf.reduce_sum(tf.abs(tf_corrcoef(tf.constant(qn_ws, dtype=tf.float32)) -I) )))\n",
    "\n",
    "\n",
    "history_len = range(len(q_wgts_history))\n",
    "ww = qn_table.get_weights()[0].shape[0]\n",
    "\n",
    "\n",
    "# print(len(q_wgts_history))\n",
    "\n",
    "numpy.set_printoptions(precision=2)\n",
    "# print(qn_table.get_weights()[0])\n",
    "# print(s_table.get_weights()[0])\n",
    "# # for row in s_table.get_weights()[0]:\n",
    "# #     print(row)\n",
    "# # print(numpy.argwhere(numpy.isnan(psi_ws)))\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     print(sess.run(K.sum(tf.abs(tf_corrcoef(tf.constant(qn_ws, dtype=tf.float32)) - numpy.eye(qn_ws.shape[1] )))))\n",
    "#     print(sess.run(K.sum(tf.abs(tf_corrcoef(tf.constant(psi_ws, dtype=tf.float32)) - numpy.eye(qn_ws.shape[1] )))))\n",
    "\n",
    "# print(numpy.cov(psi_ws.T))\n",
    "    \n",
    "print(\"q:\")\n",
    "print(numpy.corrcoef(qn_ws.T))\n",
    "print(\"Psi:\")\n",
    "print(numpy.corrcoef(psi_ws.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_pairs = []\n",
    "q_book_tups = []\n",
    "book_qixs=[]\n",
    "\n",
    "min_w = numpy.min(qn_table.get_weights()[0]) #Q weights\n",
    "max_w = numpy.max(qn_table.get_weights()[0]) #Q weights\n",
    "\n",
    "print(\"min_w is\",min_w)\n",
    "print(\"max_w is\",max_w)\n",
    "print(max_w - min_w)\n",
    "print(ww)\n",
    "\n",
    "for wix in range(ww):\n",
    "#     if (qids_seen[wix].startswith(\"ch_\") or qids_seen[wix].startswith(\"ch-\")):\n",
    "#         book_qixs.append(wix)\n",
    "    mags = []\n",
    "    ncs = []\n",
    "    for hix in range(len(q_wgts_history)):\n",
    "        wgts = q_wgts_history[hix][wix] - min_w\n",
    "        mag = numpy.linalg.norm(wgts) # calculate the Frobinius norm\n",
    "        mags.append(mag)\n",
    "    plt.plot(range(len(q_wgts_history)), mags, alpha=0.1)\n",
    "plt.gcf().set_size_inches(20,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_pairs = []\n",
    "raw_wgts = qn_table.get_weights()[0]\n",
    "\n",
    "min_w = numpy.min(raw_wgts)\n",
    "wgts = raw_wgts #- min_w\n",
    "\n",
    "# for rw, w in zip(raw_wgts,wgts):\n",
    "#     print(sum(rw), sum(w), min(w))\n",
    "\n",
    "for ix in set(qz):\n",
    "    wgts_i = wgts[ix] - min_w\n",
    "#     print(wgts_i)\n",
    "    prob_qid = qids_seen[ix]\n",
    "#     mag = numpy.sum(wgts_i) \n",
    "    mag = numpy.linalg.norm(wgts_i)\n",
    "    n_comps = numpy.sum((wgts_i > .1))\n",
    "    q_pairs.append((ix, prob_qid, mag, n_comps))\n",
    "\n",
    "q_pairs.sort(key=lambda x: x[2])\n",
    "\n",
    "\n",
    "hist_data = []\n",
    "for p in q_pairs:\n",
    "    hist_data.append(p[2])\n",
    "\n",
    "\n",
    "dim_data = []\n",
    "for p in q_pairs:\n",
    "    dim_data.append(p[3])\n",
    "\n",
    "    \n",
    "# # df = pandas.DataFrame(columns=[\"qid\",\"diff\",\"dims\"])\n",
    "for p in q_pairs:#[0:100]:\n",
    "    print(p[0], p[1], p[2], p[3])\n",
    "    if p[0] in unfailed:\n",
    "        print(\"SHDN'T BE HERE: UNFAILED\")\n",
    "    if p[0] in unpassed:\n",
    "        print(\"SHDN'T BE HERE: UNPASSED\")\n",
    "    \n",
    "# for p in q_pairs[-100:]:\n",
    "#     print(p[0], p[1], p[2])\n",
    "\n",
    "numpy.savetxt(\"./{}d_all_qs.csv\".format(dim), q_pairs , fmt=\"%s\", delimiter=\",\")\n",
    "bookpairs=[(p[0],p[1]) for p in q_book_tups]\n",
    "bookpairs.sort(key=lambda x: x[1])\n",
    "numpy.savetxt(\"./{}d_book_qs.csv\".format(dim), bookpairs , fmt=\"%s\", delimiter=\",\")\n",
    "\n",
    "plt.hist(hist_data)\n",
    "plt.gcf().set_size_inches(20,5)\n",
    "plt.show()\n",
    "\n",
    "print(numpy.mean(dim_data))\n",
    "plt.hist(dim_data)\n",
    "plt.gcf().set_size_inches(20,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "# swgts = s_table.get_weights()[0]\n",
    "# print(\"student range:\", min(swgts), max(swgts))\n",
    "\n",
    "uids_across_days = list(uids_across_days)\n",
    "# uids_across_days = sorted(uids_across_days)\n",
    "\n",
    "# print(uids_across_days)\n",
    "# for u in uids_across_days[0:100]:\n",
    "#     print(u)\n",
    "\n",
    "xs = range(len(s_wgts_history))\n",
    "# ww = s_wgts_history[0][0]\n",
    "print(s_wgts_history[0])\n",
    "\n",
    "u_pairs = []\n",
    "unchanged = set()\n",
    "\n",
    "ww_choices = numpy.random.choice(uids_across_days, size=min(len(uids_across_days),20000), replace=False)\n",
    "# ww_choices = uids_across_days\n",
    "print(len(ww_choices))\n",
    "for uid in ww_choices:\n",
    "    uix = uids_across_days.index(uid)\n",
    "    mags = []\n",
    "    for hix in xs:\n",
    "        wgts = s_wgts_history[hix][uix] - min_w\n",
    "        mag = numpy.linalg.norm(wgts) # calculate the Frobinius norm\n",
    "#         mag = sum(wgts)\n",
    "        n_comps = numpy.sum(wgts > 0)\n",
    "        mags.append(mag)\n",
    "    if mags[0] == mags[-1]:\n",
    "        unchanged.add(uid)\n",
    "    else:\n",
    "        u_pairs.append((uid, mag, n_comps))\n",
    "        plt.plot(xs,mags, alpha=0.1)\n",
    "plt.gcf().set_size_inches(20,5)\n",
    "plt.show()\n",
    "\n",
    "u_pairs.sort(key=lambda x: x[1])\n",
    "hist_data = []\n",
    "for p in u_pairs:\n",
    "    print(p[0], p[1])\n",
    "    hist_data.append(p[1])\n",
    "\n",
    "for p in sorted(u_pairs)[0:100]:\n",
    "    print(p[0], p[1], p[2])\n",
    "print(\"--\")\n",
    "for p in sorted(u_pairs)[-100:]:\n",
    "    print(p[0], p[1], p[2])\n",
    "    \n",
    "plt.hist(hist_data, bins=30)\n",
    "plt.gcf().set_size_inches(20,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "u_dt_map = defaultdict(dict)\n",
    "st_hist_data = []\n",
    "ed_hist_data = []\n",
    "for p in u_pairs:\n",
    "    ustr = p[0].split(\"_\")[0]\n",
    "    dt = datetime.strptime(p[0].split(\"_\")[1].split(\" \")[0], \"%Y-%m-%d\")\n",
    "    u_dt_map[ustr][dt]=p[1]\n",
    "\n",
    "# for k in u_dt_map:\n",
    "#     dts = sorted(u_dt_map[k])\n",
    "#     st_hist_data.append(u_dt_map[k][dts[0]])\n",
    "#     ed_hist_data.append(u_dt_map[k][dts[-1]])\n",
    "\n",
    "kct=0\n",
    "cols = 5\n",
    "rows = 10\n",
    "f,pltz = plt.subplots(rows,cols, sharey=True, sharex=True)\n",
    "for k in u_dt_map:\n",
    "#     if len(u_dt_map[k]) < 10:\n",
    "#         continue\n",
    "    dts = sorted(u_dt_map[k])\n",
    "    delta = (max(dts) - min(dts))\n",
    "    \n",
    "#     d = datetime(1979,7,16)/\n",
    "    d = delta.days\n",
    "    st_hist_data.append(u_dt_map[k][dts[0]])\n",
    "    ed_hist_data.append(u_dt_map[k][dts[-1]])\n",
    "\n",
    "    if d < 180:\n",
    "        continue    \n",
    "#     print(k,\":\", dts)\n",
    "    mgs = [u_dt_map[k][dt] for dt in dts]\n",
    "#     print(mgs)\n",
    "#     print(kct, kct//cols, kct%cols)\n",
    "    if kct<(cols*rows):\n",
    "        pltz[kct//cols, kct%cols].plot_date(x=dts, y=mgs, label=k, ls=\"-\", fmt=\"\")\n",
    "    kct+=1\n",
    "    \n",
    "plt.gcf().set_size_inches(20,40)\n",
    "# plt.ylim(0)\n",
    "plt.show()\n",
    "    \n",
    "# print(\"...\")\n",
    "# for el in unchanged:\n",
    "#     print(el)\n",
    "    \n",
    "st_mean = numpy.mean(st_hist_data)\n",
    "ed_mean = numpy.mean(ed_hist_data)\n",
    "\n",
    "plt.hist(st_hist_data, alpha=0.5, label=\"Before\", bins=30)\n",
    "plt.axvline(numpy.mean(st_mean), color='blue', linestyle='dashed', linewidth=1)\n",
    "plt.hist(ed_hist_data, alpha=0.5, label=\"After\", bins=30)\n",
    "plt.axvline(numpy.mean(ed_mean), color='red', linestyle='dashed', linewidth=1)\n",
    "\n",
    "_, max_ = plt.ylim()\n",
    "plt.text(st_mean+0.3, max_ - max_/10, 'Start Mean:\\n{:.2f}'.format(st_mean))\n",
    "\n",
    "plt.text(ed_mean+0.3, \n",
    "         max_ - max_/5, \n",
    "         'End Mean:\\n{:.2f}'.format(ed_mean))\n",
    "\n",
    "plt.gcf().set_size_inches(20,5)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(len(st_hist_data))\n",
    "print(numpy.mean(st_hist_data), numpy.std(st_hist_data))\n",
    "print(numpy.mean(ed_hist_data), numpy.std(ed_hist_data))\n",
    "print(len(unchanged), len(u_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student calibration and change over time\n",
    "\n",
    "### Daily snapshots\n",
    "For a student s, the student's attempts at questions are partitioned into daily snapshots: {s0, s1, s2 ...}\n",
    "For the purposes of calibration, student snapshots are mutually independent: that is the scores for sm are conditionally independent of the scores for sn n!=m.  The independence is conditional because information is transmitted via the calibration of the questions encountered on day n: {q}\\_n.\n",
    "\n",
    "Note that since a student cannot pass an item twice, it's impossible for two \"passes\" on different days to exist.  It is possible for a student to make several failed attempts at an item, and then pass it on a later date.  Since item calibrations hold across all time, but students are recalibrated daily, the student's ability will be calibrated such that it \"straddles\" the item across this timeframe.\n",
    "\n",
    "An example of a student's scores over time:\n",
    "\n",
    "10214_2015-11-09 169.724 20\n",
    "\n",
    "10214_2015-11-10 192.286 20\n",
    "\n",
    "10214_2015-11-11 201.752 20\n",
    "\n",
    "10214_2016-05-10 196.241 20\n",
    "\n",
    "10214_2016-05-15 202.679 20\n",
    "\n",
    "10214_2016-05-16 201.018 20\n",
    "\n",
    "10214_2016-05-20 182.211 20\n",
    "\n",
    "10214_2016-05-21 184.925 20\n",
    "\n",
    "10214_2016-06-23 190.13 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "indices = [qids_seen.index(tup[0]) for tup in q_book_tups]\n",
    "print(indices)\n",
    "\n",
    "mutate = TSNE(n_components=2)\n",
    "vqz_set = list(set(vqz))\n",
    "warr = numpy.array([qn_table.get_weights()[0][vq] for vq in vqz_set])\n",
    "\n",
    "data_flat = mutate.fit_transform(warr)\n",
    "# data_flat = warr\n",
    "\n",
    "# plt.scatter(x=[tvp[2][0] for tvp in q_book_tups], y=[tvp[2][1] for tvp in q_book_tups])\n",
    "plt.scatter(x=data_flat[:,0], y=data_flat[:,1], alpha=0.5)\n",
    "\n",
    "for ix,vq in enumerate(vqz_set):\n",
    "#     qix = qids_seen.index(tup[0])\n",
    "#     L = tup[1].split(\"|\")[0]\n",
    "#     L = qids_seen[vq].split(\"|\")[0]\n",
    "    L = qids_seen[vq].split(\"|\")[0]\n",
    "    plt.annotate(L, (data_flat[ix,0], data_flat[ix,1]), alpha=0.5 )\n",
    "plt.gcf().set_size_inches(20,20)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# warr = numpy.array([qn_table.get_weights()[0][bq] for bq in book_qixs])\n",
    "# data_flat = mutate.fit_transform(warr)\n",
    "# # data_flat = warr\n",
    "# # plt.scatter(x=[tvp[2][0] for tvp in q_book_tups], y=[tvp[2][1] for tvp in q_book_tups])\n",
    "# plt.scatter(x=data_flat[:,0], y=data_flat[:,1], alpha=0.5)\n",
    "\n",
    "# for ix, bq in enumerate(book_qixs):\n",
    "#     L = qids_seen[bq].split(\"|\")[0] + \"|\" + qids_seen[bq].split(\"|\")[1][0:5] \n",
    "#     plt.annotate(L, (data_flat[ix,0], data_flat[ix,1]), alpha=0.5 )\n",
    "# plt.gcf().set_size_inches(20,20)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix,q in enumerate(qn_table.get_weights()[0]):\n",
    "    print(ix, qids_seen[ix], q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix,s in enumerate(s_table.get_weights()[0]):\n",
    "    print(ix, uids_across_days[ix], s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_w = 6\n",
    "plot_h = (1+len(d_hist_data)//plot_w)\n",
    "\n",
    "# fig, axes = plt.subplots(nrows=(len(d_hist_data)), ncols=1)            \n",
    "plt.gcf().set_size_inches(3*plot_w,3*plot_h)\n",
    "for ax_i, d in enumerate(d_hist_data):\n",
    "    data = d_hist_data[d]\n",
    "    print(d, len(data))\n",
    "#     print(min(data), max(data))\n",
    "#     print(data)\n",
    "    subplot = plt.subplot(plot_h, plot_w, ax_i+1)\n",
    "    subplot.hist(data)\n",
    "plt.show()\n",
    "\n",
    "print(len(d_hist_data), \"days of activity\")\n",
    "print(len(qids_seen), \"qids\")\n",
    "print(len(uids_seen), \"uids\")\n",
    "print(len(book_qids), \"book qids\")\n",
    "print(book_qids)\n",
    "            \n",
    "#         for item in attz4date[attz4date[\"question_id\"]==qid].loc[[\"user_id\", \"correct\"]]:\n",
    "#             print(\"\\t\",item)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(passes)\n",
    "plt.hist(passes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assignments.loc[:, \"creation_date\"] = pandas.to_datetime(assignments[\"creation_date\"]).dt.floor(\"D\")\n",
    "# pid_override = set()\n",
    "# for h in ass_summ[\"hexes\"]: # go through all hexes ever assigned...\n",
    "#     pid_override.update(eval(h))\n",
    "# pid_override = sorted(list(pid_override))\n",
    "\n",
    "LOAD_PQIDS=True\n",
    "\n",
    "if LOAD_PQIDS:\n",
    "    pid_override = joblib.load(base+\"pid_override.pkl\")\n",
    "    all_qids = joblib.load(base+\"all_qids.pkl\")\n",
    "    print(\"loaded pqids....\")\n",
    "\n",
    "else:\n",
    "    pid_override = set()\n",
    "    qids_in_play = set()\n",
    "    ss = set()\n",
    "    for aix in ass_summ.index:\n",
    "        ss.update(eval(ass_summ.loc[aix,\"students\"]))\n",
    "        # hxz = [h for h in eval(ass_summ.loc[aix,\"hexes\"]) if (h.startswith(\"ch_\") or h.startswith(\"ch-i\"))]\n",
    "        # ass_summ.loc[aix, \"hexes\"] = str(hxz)\n",
    "\n",
    "    for s in ss:\n",
    "        attz = get_attempts_from_db(s)\n",
    "        qids_in_play.update([q for q in list(attz[\"question_id\"])])# if (q.startswith(\"ch_\") or q.startswith(\"ch-i\"))] )\n",
    "        pid_override.update([s.split(\"|\")[0].replace(\"-\",\"_\") for s in list(attz[\"question_id\"]) if (s.startswith(\"ch_\") or s.startswith(\"ch-i\"))])\n",
    "    all_qids = sorted(qids_in_play)\n",
    "    pid_override = sorted(pid_override)\n",
    "    all_page_ids = pid_override\n",
    "\n",
    "    print(all_qids)\n",
    "    print(\"Qids in play len\", len(all_qids))\n",
    "\n",
    "    print(pid_override)\n",
    "    print(\"Pids in play len\", len(pid_override))\n",
    "    joblib.dump(all_qids, base+\"all_qids.pkl\")\n",
    "    joblib.dump(pid_override, base+\"pid_override.pkl\")\n",
    "    \n",
    "all_page_ids = list(set([ s.split(\"|\")[0] for s in all_qids ]))\n",
    "print(all_page_ids)\n",
    "print(len(all_page_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from hwgen.common import get_user_data\n",
    "start_dates = {}\n",
    "ss = set()\n",
    "first_assign = {}\n",
    "min_ts = None\n",
    "max_ts = None\n",
    "for aix in ass_summ.index:\n",
    "    ts, grid, students = ass_summ.loc[aix,[\"creation_date\",\"group_id\",\"students\"]]\n",
    "    \n",
    "    if (not min_ts) or ts < min_ts:\n",
    "        min_ts = ts\n",
    "    if (not max_ts) or ts > max_ts:\n",
    "        max_ts = ts\n",
    "    \n",
    "    students = eval(students) # convert str rep to list\n",
    "    ss.update(students)\n",
    "    for s in students:\n",
    "        if s not in first_assign:\n",
    "            first_assign[s] = copy.copy(ts)\n",
    "psi_df = get_user_data(list(ss))\n",
    "for ix in psi_df.index:\n",
    "    psi, rd = psi_df.loc[ix, [\"id\",\"registration_date\"]]\n",
    "    if psi not in start_dates:\n",
    "        ts = first_assign[psi]\n",
    "        reg_date = pandas.Timestamp(psi_df.loc[psi_df[\"id\"]==psi,\"registration_date\"].values[0])\n",
    "        start_dates[psi] = (ts, reg_date)\n",
    "\n",
    "for s in start_dates:\n",
    "    print(s, start_dates[s])\n",
    "\n",
    "print(min_ts, max_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUILD_SXUA = False\n",
    "# print(ass_summ[0:10])\n",
    "numpy.set_printoptions(precision=1, suppress=True)\n",
    "if BUILD_SXUA:\n",
    "    print(\"getting start dates\")\n",
    "#     try:\n",
    "#         start_dates = joblib.load(base + \"start_dates.pkl\")\n",
    "#         print(\"loaded\")\n",
    "#     except:\n",
    "#         start_dates = build_start_dates(ass_summ)\n",
    "    if True:\n",
    "        joblib.dump(start_dates, base+\"start_dates.pkl\")\n",
    "        print(\"built\")\n",
    "    ass_summ = ass_summ[ass_summ[\"has_book_hexes\"] == True]  # discard activity that has nowt to do with the book\n",
    "#     for psi in list(start_dates.keys())[0:10]:\n",
    "#         print(psi, start_dates[psi])\n",
    "    print(\"building SXUA\")\n",
    "    SXUA = build_SXUA(base, ass_summ, all_qids, pid_override=pid_override, start_dates=start_dates)\n",
    "    f = open(base + \"SXUA.comp.pkl\", 'wb')\n",
    "    pickle.dump(SXUA, f)\n",
    "    f.close()\n",
    "    print(\"compressed and SAVED\")\n",
    "else:\n",
    "    print(\"loading SXUA\")\n",
    "    f = open(base + \"SXUA.comp.pkl\", 'rb')\n",
    "    SXUA = pickle.load(f)\n",
    "    print(\"SXUA loaded with {} entries\", len(SXUA))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(SXUA))\n",
    "for k in SXUA:\n",
    "#     print(k)\n",
    "    tss = SXUA[k]\n",
    "    print(\"{}: ({} {}), {} \".format(k, min(tss), max(tss), len(tss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ass_summ = ass_summ[0:8000]\n",
    "ass_summ = ass_summ[ass_summ[\"has_book_hexes\"]==True] # discard activity that has nowt to do with the book\n",
    "# ass_summ = ass_summ[ass_summ[\"num_hexes\"]==1] # discard activity that has nowt to do with the book\n",
    "print(\"ass summ pre filtered, shape {}\".format(ass_summ.shape))\n",
    "ass_summ = ass_summ[ass_summ[\"include\"]==True]\n",
    "print(\"ass summ post filtered, shape {}\".format(ass_summ.shape))\n",
    "\n",
    "tss = ass_summ.loc[:,\"creation_date\"]\n",
    "print(numpy.min(tss), numpy.max(tss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zlib\n",
    "# numpy.set_printoptions(precision=1, suppress=True)\n",
    "\n",
    "# for ix in ass_summ.index[0:100]:\n",
    "#     aid,stz,ts = ass_summ.loc[ix,[\"id\",\"students\",\"creation_date\"]]\n",
    "#     stz = eval(stz)\n",
    "#     print(\"----Aid =\",aid)\n",
    "#     for psi in stz:\n",
    "#         psi_sxua = SXUA[psi]\n",
    "# #         for ts in psi_sxua:\n",
    "#         S,X,U,A = pickle.loads(zlib.decompress(psi_sxua[ts]))\n",
    "#         print(aid,psi,ts,S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.set_printoptions(precision=4, suppress=True)\n",
    "aug = augment_data(ass_summ, SXUA, pid_override, filter=True)\n",
    "aid_list, s_list, x_list, u_list, a_list, y_list, psi_list, hexes_to_try_list, hexes_tried_list, _, gr_id_list, ts_list, maxdops = aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# i=0\n",
    "# for aid, psi, sl,xl,ul,al,hxtt,hxtd in zip(aid_list, psi_list, s_list,x_list,u_list,a_list,hexes_to_try_list, hexes_tried_list):\n",
    "#     print(psi,\":\",aid,\":\",sl)\n",
    "# #     print([x for x in xl])\n",
    "#     print(\"...\",sum(xl), numpy.nonzero(xl))\n",
    "#     print([all_qids[ix] for ix in numpy.nonzero(xl)[0]])\n",
    "#     i += 1\n",
    "#     if i == 1000:\n",
    "#         break\n",
    "# #     print(xl)\n",
    "# #     print(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNT_TEACHERS=False\n",
    "if COUNT_TEACHERS:\n",
    "    t_test = None\n",
    "    t_train = None\n",
    "    ct = Counter()\n",
    "    for t in numpy.unique(ass_summ[\"owner_user_id\"]):\n",
    "        t_assignments = ass_summ[ass_summ[\"owner_user_id\"] == t]\n",
    "        ct[t] = ass_summ.shape[0]\n",
    "    print(ct.most_common(20))\n",
    "    print(\"teachers counted\")\n",
    "\n",
    "    assct = 0\n",
    "    teacherct = 0\n",
    "    kept_ts = []\n",
    "    for (t, tct) in list(ct.most_common(len(ct))):\n",
    "        t_assignments = ass_summ[ass_summ[\"owner_user_id\"] == t]\n",
    "        this_t_assts = t_assignments.shape[0]\n",
    "        if this_t_assts < 5:\n",
    "            print(\"not enough assignments\",t,tct,this_t_assts)\n",
    "        else:\n",
    "            teacherct += 1\n",
    "            kept_ts.append((t,ct[t]))\n",
    "    print(teacherct, assct)\n",
    "\n",
    "    teacherN = teacherct\n",
    "    test_insts = 0\n",
    "    target_test = 6000 #teacherN // 10\n",
    "\n",
    "    t_test = []\n",
    "    t_train = []\n",
    "    put_in_test_set = False\n",
    "    n_train = 0\n",
    "    n_test = 0\n",
    "    for (t, tct) in kept_ts:\n",
    "        num_students = 0\n",
    "        t_assignments = ass_summ[ass_summ[\"owner_user_id\"] == t]\n",
    "        print(t,\":\",t_assignments.shape[0], \"new training assts\")\n",
    "\n",
    "        if put_in_test_set and n_test<target_test:\n",
    "            # test_insts += 1\n",
    "            print(\"TEST>>\")\n",
    "            aug = augment_data(t_assignments, SXUA, pid_override=pid_override, filter=True)\n",
    "            num_students = len(aug[0])\n",
    "            n_test += num_students\n",
    "            if len(t_test) == 0:\n",
    "                t_test = t_assignments\n",
    "            else:\n",
    "                t_test = pandas.concat([t_test, t_assignments])\n",
    "            put_in_test_set = False\n",
    "        else:\n",
    "            print(\"TRAIN>>\")\n",
    "            aug = augment_data(t_assignments, SXUA, pid_override=pid_override, filter=False)\n",
    "            num_students = len(aug[0])\n",
    "            n_train += num_students\n",
    "            if len(t_train) == 0:\n",
    "                t_train = t_assignments\n",
    "            else:\n",
    "                t_train = pandas.concat([t_train, t_assignments])\n",
    "            put_in_test_set = True\n",
    "\n",
    "        if n_train >= 50000:\n",
    "            break\n",
    "\n",
    "    tr = t_train\n",
    "    tt = t_test\n",
    "    print(\"numbers of (student,asst) pairs:\", n_train, n_test)\n",
    "    tr.to_csv(base + \"tr_ttb.csv\")\n",
    "    tt.to_csv(base + \"tt_ttb.csv\")\n",
    "else:\n",
    "    tr = pandas.read_csv(base + \"tr_ttb.csv\")\n",
    "    tt = pandas.read_csv(base + \"tt_ttb.csv\")\n",
    "\n",
    "print(\"len t_train, t_test\",len(tr), len(tt))\n",
    "ass_summ = pandas.concat([tr,tt])\n",
    "print(\"Split complete!\")\n",
    "print(\"{} {}\".format(len(tt), len(tr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_x_val = True\n",
    "xrmses = []\n",
    "xaccs = []\n",
    "if do_x_val:\n",
    "    n_folds = 10\n",
    "    start_indices = [len(tr)*x//n_folds for x in range(n_folds)]\n",
    "    print(start_indices)\n",
    "    fold_size = len(tr)//n_folds\n",
    "    for start_target in start_indices:\n",
    "        ttx = tr[start_target:(start_target+fold_size)]        \n",
    "        trx = tr.drop(tr.index[range(start_target,start_target+fold_size)])\n",
    "        aug_trx = augment_data(trx, SXUA, pid_override=pid_override, all_qids=all_qids, all_page_ids=all_page_ids, filter=False)\n",
    "        modelx = train_deep_model(aug_trx)\n",
    "        aug_ttx = augment_data(ttx, SXUA, pid_override=pid_override, all_qids=all_qids, all_page_ids=all_page_ids, filter=False)\n",
    "        (rmse,acc) = evaluate3(aug_ttx, modelx, pid_override)\n",
    "        xrmses.append(rmse)\n",
    "        xaccs.append(acc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xrmses)\n",
    "print(xaccs)\n",
    "print(numpy.mean(xrmses))\n",
    "print(numpy.mean(xaccs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    model = train_deep_model(aug)\n",
    "    print(\"...deleted original X,y\")\n",
    "    model.save(base + 'hwg_model.hd5')\n",
    "    # joblib.dump(xmask, base + 'hwg_xmask.pkl')\n",
    "    # joblib.dump(sc, base + 'hwg_mlb.pkl')\n",
    "    # joblib.dump((sscaler,levscaler,volscaler), base + 'hwg_scaler.pkl')\n",
    "    print(\"saved model and exit\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
