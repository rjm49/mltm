{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from importlib import reload\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy\n",
    "\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from random import shuffle, choice, randint\n",
    "\n",
    "import math\n",
    "import keras\n",
    "import tensorflow\n",
    "\n",
    "from NN_utils import BigTable, WeightClip\n",
    "\n",
    "import pickle\n",
    "import zlib\n",
    "\n",
    "def logistic(x, b,off):\n",
    "    z = b*(x-off)\n",
    "    return numpy.exp(z)/(1+numpy.exp(z))\n",
    "\n",
    "def pr_to_spread(p, comps=1, as_A_and_D=True):\n",
    "    per_comp_p = p**(1/(comps))\n",
    "#     print(\"p         \", p)\n",
    "#     print(\"per comp p\", per_comp_p)\n",
    "#     spread = -numpy.log((1.0/per_comp_p)-1.0)\n",
    "    inv_sigmoid = lambda pr : ( -numpy.log((1/pr) -1) )\n",
    "    spread = inv_sigmoid(per_comp_p)\n",
    "#     print(\"spread    \", spread)\n",
    "    if as_A_and_D:\n",
    "        a = spread/2.0\n",
    "        d = -spread/2.0\n",
    "        return a,d\n",
    "    else:\n",
    "        return spread\n",
    "\n",
    "print(\"started\")\n",
    "\n",
    "\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import uniform, random_integers\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "def gen_bayes_students_questions(n_students, n_questions, a0, a1, n_factors, min_active_traits, max_active_traits):\n",
    "    students = numpy.zeros((n_students, n_factors))\n",
    "    for six in range(n_students):\n",
    "#         true_comps = numpy.random.normal(a0, a1, size=n_factors)#+0.2#+2#+1.2\n",
    "#         true_comps = numpy.random.normal(0, 5/3, size=n_factors)#+0.2#+2#+1.2\n",
    "        true_comps = numpy.random.uniform(0,1, size=n_factors)#+0.2#+2#+1.2\n",
    "        for cix,c in zip(range(n_factors), true_comps):\n",
    "            students[six,cix] = c\n",
    "\n",
    "    questions = numpy.zeros((n_questions, n_factors))\n",
    "    for qix in range(n_questions):\n",
    "#         n_comps = randint(min_active_traits, max_active_traits)\n",
    "        n_comps = n_factors\n",
    "        comp_ixs = numpy.random.choice(range(n_factors), size=n_comps, replace=False)\n",
    "        true_comps = numpy.random.uniform(a0[0],a0[1], size=n_comps)\n",
    "\n",
    "        for cix,c in zip(comp_ixs,true_comps):\n",
    "            questions[qix,cix] = c#+boost\n",
    "    \n",
    "    print(\"genqs\",students.shape, questions.shape)\n",
    "    return students, questions\n",
    "\n",
    "def gen_bayes_run(n_traits, a0, a1, min_active_traits, max_active_traits, test_w=None, n_students=8, n_questions=8):\n",
    "    students, questions = gen_bayes_students_questions(n_students, n_questions, a0, a1, n_factors, min_active_traits, max_active_traits)\n",
    "    print(students.shape, questions.shape)\n",
    "    obs = numpy.zeros((len(students), len(questions)))\n",
    "    probs = numpy.zeros((len(students), len(questions)))\n",
    "    vz = []\n",
    "    mz = []\n",
    "    scz =[]\n",
    "    for vi in range(len(students)):\n",
    "        for mi in range(len(questions)):\n",
    "            prs = (1-questions[mi]) + (questions[mi]*students[vi])\n",
    "            pr = numpy.prod(prs)\n",
    "            obs[vi,mi] = (random.random() < pr)\n",
    "            probs[vi,mi] = pr\n",
    "    return obs, probs, students, questions\n",
    "\n",
    "# n_students, n_questions, n_factors, min_active, max_active = 100,100,10,10,10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import uniform, random_integers\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "def gen_rasch_students_questions(n_students, n_questions, a0, a1, n_factors, min_active_traits, max_active_traits, test_w):\n",
    "    students = numpy.zeros((n_students, n_factors))\n",
    "#     for six in range(n_students):\n",
    "#         true_comps = numpy.random.normal(0, a1, size=n_factors)#+0.2#+2#+1.2\n",
    "# #         true_comps = numpy.random.normal(0, 5/3, size=n_factors)#+0.2#+2#+1.2\n",
    "# #         true_comps = numpy.random.uniform(0,1, size=n_factors)#+0.2#+2#+1.2\n",
    "#         for cix,c in zip(range(n_factors), true_comps):\n",
    "#             students[six,cix] = c\n",
    "\n",
    "    students = numpy.random.normal(0, a1, size=(n_students,n_factors))\n",
    "            \n",
    "    av_c = (min_active_traits + max_active_traits)/2\n",
    "    d50 = pr_to_spread(.5, av_c, as_A_and_D=False)\n",
    "    \n",
    "    questions = numpy.zeros((n_questions, n_factors)) - 10\n",
    "    \n",
    "    minb=-(test_w/2) -a0 - d50\n",
    "    maxb=(test_w/2) -a0 - d50\n",
    "    questions = questions\n",
    "    minb, maxb = sorted([minb, maxb])\n",
    "    minb = float(minb)\n",
    "    maxb = float(maxb)\n",
    "    \n",
    "    for qix in range(n_questions):\n",
    "        n_comps = randint(min_active_traits, max_active_traits)\n",
    "#         print(\"n_comps\", n_comps)\n",
    "        comp_ixs = numpy.random.choice(range(n_factors), size=n_comps, replace=False)  \n",
    "#         print(\"range=\", minb,maxb)\n",
    "        true_comps = numpy.random.uniform(minb, maxb, size=n_comps)\n",
    "        for cix,c in zip(comp_ixs,true_comps):\n",
    "            questions[qix,cix] = c\n",
    "            \n",
    "    return students, questions\n",
    "\n",
    "\n",
    "def calc_probs_from_embs(students,questions):\n",
    "    students2 = numpy.repeat(students, len(questions), axis=0)\n",
    "    questions2 = numpy.tile(questions, (len(students),1))\n",
    "    zmask = numpy.isclose(questions2,-10).astype(int)\n",
    "    diffs = students2-questions2\n",
    "    prs = 1.0/(1.0+ numpy.exp(-diffs))\n",
    "    prs = numpy.maximum(zmask,prs)\n",
    "    probs2 = numpy.prod(prs, axis=1).reshape(len(students), len(questions))\n",
    "    return probs2\n",
    "\n",
    "def gen_rasch_run(n_factors, a0, a1, min_active_traits, max_active_traits, test_w=None, n_students=8, n_questions=8):\n",
    "    students, questions = gen_rasch_students_questions(n_students, n_questions, a0, a1, n_factors, min_active_traits, max_active_traits, test_w=test_w)\n",
    "    obs = numpy.zeros((len(students), len(questions)))\n",
    "    vz = []\n",
    "    mz = []\n",
    "    scz =[]\n",
    "    \n",
    "#     students2 = numpy.repeat(students, n_questions, axis=0)\n",
    "#     questions2 = numpy.tile(questions, (n_students,1))\n",
    "    \n",
    "# #     print(\"st and qn shapes:\")\n",
    "# #     print(students.shape)\n",
    "# #     print(questions.shape)\n",
    "    \n",
    "#     zmask = numpy.isclose(questions2,-10).astype(int)\n",
    "# #     print(\"zmask shape:\", zmask.shape)\n",
    "#     diffs = students2-questions2\n",
    "# #     print(\"diffs shape:\", diffs.shape)\n",
    "#     prs = 1.0/(1.0+ numpy.exp(-diffs))\n",
    "#     prs = numpy.maximum(zmask,prs)\n",
    "#     probs2 = numpy.prod(prs, axis=1).reshape(n_students, n_questions)\n",
    "        \n",
    "    probs2 = calc_probs_from_embs(students, questions)\n",
    "#     for vi in range(len(students)):\n",
    "#         for mi in range(len(questions)):\n",
    "#             zmask = numpy.isclose(questions[mi],-10).astype(int)\n",
    "#             diffs = students[vi]-questions[mi]\n",
    "#             prs = logistic(diffs,1,0)\n",
    "#             prs = numpy.maximum(zmask,prs)\n",
    "\n",
    "#             pr = numpy.prod(prs)\n",
    "#             obs[vi,mi] = (random.random() < pr)\n",
    "#             probs[vi,mi] = pr\n",
    "            \n",
    "    return obs, probs2, students, questions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import beta\n",
    "# # n_questions = 100\n",
    "# # n_students = 100\n",
    "# # n_factors = 2\n",
    "\n",
    "# def gen_run(n_movies, n_viewers, n_factors, min_active, max_active):\n",
    "#     from numpy.random import randint, uniform\n",
    "\n",
    "#     mina,maxa =(0,1)\n",
    "#     minb,maxb =(0,1)\n",
    "#     movies = randint(minb, maxb+1, size=(n_movies, n_factors))\n",
    "#     viewers = randint(mina, maxa+1, size=(n_viewers, n_factors))\n",
    "# #     movies =  beta.rvs(2, 2, size=(n_movies, n_factors))\n",
    "# #     viewers = beta.rvs(2, 2, size=(n_movies, n_factors))\n",
    "\n",
    "#     plt.hist(movies.flatten(), alpha=0.5)\n",
    "#     plt.hist(viewers.flatten(), alpha=0.5)\n",
    "#     plt.show()\n",
    "    \n",
    "#     print(movies)\n",
    "#     print(viewers)\n",
    "\n",
    "#     sig = lambda z : 1/(1+numpy.exp(-z))\n",
    "\n",
    "#     obs = numpy.zeros((len(viewers), len(movies)))\n",
    "#     #obs = numpy.matmul(viewers, movies.T)/n_factors\n",
    "#     vz = []\n",
    "#     mz = []\n",
    "#     scz =[]\n",
    "#     for vi in range(len(viewers)):\n",
    "#         for mi in range(len(movies)):\n",
    "# #             obs[vi,mi] = numpy.prod(sig(viewers[vi] - movies[mi]))\n",
    "#             obs[vi,mi] = numpy.dot(viewers[vi], movies[mi])\n",
    "# #             prxd = numpy.multiply(viewers[vi], movies[mi])\n",
    "# #             obs[vi,mi] = numpy.sum(prxd)\n",
    "# #             obs[vi,mi] = numpy.random.randint(0,2)\n",
    "#             print(viewers[vi], movies[mi],\"=\",obs[vi,mi])\n",
    "#     plt.hist(obs.flatten())\n",
    "#     plt.show()\n",
    "    \n",
    "#     return obs, None, viewers, movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_students, n_questions, n_factors, min_active, max_active = 100,150,100,1,5\n",
    "\n",
    "# numpy.set_printoptions(precision=3)\n",
    "# test_w = 5\n",
    "# #rasch a0 = 3\n",
    "# # a0 = 0.26 #bayes10\n",
    "# a0 = (0.2,1)\n",
    "# a1 = None\n",
    "# # obs, probs, students, questions  = gen_bayes_run(n_factors, a0, a1, min_active, max_active, test_w=test_w, n_students=n_students, n_questions=n_questions)\n",
    "# obs, probs, students, questions  = gen_rasch_run(n_factors, a0, a1, min_active, max_active, test_w=test_w, n_students=n_students, n_questions=n_questions)\n",
    "\n",
    "# plt.hist(students.flatten(), alpha=0.5)\n",
    "# plt.hist(questions.flatten(), alpha=0.5)\n",
    "# plt.show()\n",
    "\n",
    "# plt.hist(probs.flatten())\n",
    "# plt.show()\n",
    "\n",
    "# plt.hist(obs.flatten())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l1\n",
    "from keras.layers import Dropout, multiply, subtract, GaussianNoise, GaussianDropout, Input, Lambda, Embedding, Flatten, concatenate\n",
    "from keras import backend as K, Model\n",
    "def generate_qs_model(qn_table, psi_table, optimiser, _mode=\"MXFN\", loss=\"MSE\"):\n",
    "    \n",
    "    psi_sel = Input(shape=(1,), name=\"psi_select\", dtype=\"int32\")\n",
    "    qn_sel = Input(shape=(1,), name=\"q_select\", dtype=\"int32\")\n",
    "\n",
    "#     psi_row = Flatten(Embedding(psi_n, 100, input_length=1)(psi_sel))\n",
    "#     qn_row = Flatten(Embedding(qn_n, 100, input_length=1)(qn_sel))\n",
    "    \n",
    "#     print(qn_table, psi_table, psi_sel, qn_sel)\n",
    "#     print(\"psi_sel shape\", psi_sel.shape)\n",
    "\n",
    "    psi_table.trainable=True\n",
    "    qn_table.trainable=True\n",
    "    \n",
    "    qn_row = qn_table(qn_sel)\n",
    "    psi_row = psi_table(psi_sel)\n",
    "\n",
    "    print(\"Mode is\", _mode)\n",
    "    if _mode==\"DEEP\":\n",
    "#         difs = concatenate([psi_row, qn_row])\n",
    "        difs = subtract([psi_row, qn_row])\n",
    "#         h = Dense(128, activation=\"relu\")(difs)\n",
    "        h = Dense(64, activation=\"relu\")(difs)\n",
    "#         h = Dense(32, activation=\"relu\")(h)\n",
    "#         h = Dense(16, activation=\"relu\")(h)\n",
    "#         h = Dense(8, activation=\"relu\")(h)\n",
    "        score = Dense(1, activation=\"sigmoid\")(h)        \n",
    "    elif _mode==\"COND\":\n",
    "        Prs = Lambda(lambda qs: (1 - qs[0])+(qs[0]*qs[1]), name=\"Prs\")([qn_row, psi_row])\n",
    "        score = Lambda(lambda ps: K.prod(ps, axis=1, keepdims=True), name=\"score\")(Prs)\n",
    "#     elif _mode==\"BINQ\":\n",
    "#         Prs = Lambda(lambda qs: (1 - qs[0])+(qs[0]*qs[1]), name=\"Prs\")([qn_row, psi_row])\n",
    "#         score = Lambda(lambda ps: K.prod(ps, axis=1, keepdims=True), name=\"score\")(Prs)\n",
    "    elif _mode==\"MLTM\":\n",
    "        klip = Lambda(lambda qk: K.clip(qk,-10, -9)+10)\n",
    "        q_masque = klip(qn_row)\n",
    "        difs = subtract([psi_row, qn_row])\n",
    "        Prs = Lambda(lambda z: (1.0 / (1.0 + K.exp(-z))), name=\"Pr_sigmoid1\")(difs)\n",
    "        Prs = Lambda(lambda ps_q:  ps_q[0]*ps_q[1] + (1-ps_q[1]) ) ([Prs, q_masque])\n",
    "        score = Lambda(lambda ps: K.prod(ps, axis=1, keepdims=True), name=\"score\")(Prs)\n",
    "    else:\n",
    "        if _mode!=\"MXFN\":\n",
    "            print(\"Invalid mode:\", _mode, \"- valid modes are COND, MLTM, MXFN (using default: MXFN)\")\n",
    "        _mode==\"MXFN\"\n",
    "        scores = Lambda(lambda qp: qp[0] * qp[1])([qn_row, psi_row])\n",
    "        score = Lambda(lambda s: K.sum(s, keepdims=True, axis=1), name=\"sum\")(scores)\n",
    "#         score = Lambda(lambda qp: K.batch_dot(qp[0], qp[1], axes=1), name=\"dot_prod\")([qn_row, psi_row])\n",
    "    #     score = Lambda(lambda z: 1.0 / (1.0 + K.exp(-z)))(score)\n",
    "    \n",
    "    model = Model(inputs=[qn_sel, psi_sel], outputs=score)\n",
    "\n",
    "#     if _mode==\"BINQ\":\n",
    "#         from_half = Lambda(lambda x: 1+K.sum((0.25-(x-0.5)**2)) )\n",
    "#         s_loss = from_half(psi_table.kernel)\n",
    "#         q_loss = from_half(qn_table.kernel)\n",
    "#         def custom_loss(s_loss,q_loss):\n",
    "#             def orig_loss(yt,yh):\n",
    "#                 return K.binary_crossentropy(yt,yh) * s_loss * q_loss\n",
    "# #             return K.mean(K.square(yt-yh)) + 5000*aux_av + 1000*aux_std + aux_loss/10000\n",
    "#             return orig_loss\n",
    "#         model.compile(optimizer=optimiser, loss=custom_loss(s_loss, q_loss), metrics=[\"accuracy\"])\n",
    "#         return model\n",
    "    \n",
    "    print(\"loss mode is\", loss)\n",
    "    if loss==\"MSE\":\n",
    "        model.compile(optimizer=optimiser, loss=\"mse\", metrics=[\"accuracy\"])\n",
    "    else:\n",
    "        if loss!=\"XENT\":\n",
    "            print(\"loss mode must be MSE or XENT, not\", loss,\" - setting to XENT.\")\n",
    "            loss=\"XENT\"            \n",
    "        model.compile(optimizer=optimiser, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])     \n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l1\n",
    "from keras.layers import Dropout, multiply, subtract, GaussianNoise, GaussianDropout, Input, Lambda, Dense\n",
    "from keras import backend as K, Model\n",
    "from keras.optimizers import Adam\n",
    "def generate_offset_generator(w=5,d=4):\n",
    "    #width, dispersal, target_EV\n",
    "    h=None\n",
    "    inp = Input(shape=(3,))\n",
    "    for _ in range(d):\n",
    "        if h==None:\n",
    "            h = Dense(w, activation=\"relu\")(inp)\n",
    "        else:\n",
    "            h = Dense(w, activation=\"relu\")(h)\n",
    "    last_layer=inp if (h is None) else h\n",
    "    offset = Dense(1, activation=\"linear\")(last_layer)\n",
    "    model = Model(inputs=[inp], outputs=[offset])\n",
    "    model.compile(optimizer=Adam(), loss=\"mse\")\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a1 = 5/3\n",
    "# a0 = 1.75\n",
    "# tw = 3.5\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "def create_offset_generator(n_factors, min_active, max_active, sampsize=14, n_iter=20000, rasch=True):\n",
    "    n_questions = int(sampsize / 0.9)\n",
    "    n_students = int(sampsize / 0.9)\n",
    "    inps = []\n",
    "    outs = []\n",
    "\n",
    "    i=0\n",
    "#     for a in range(n_iter):\n",
    "    print(\"generating generator training mini-datasets...\")\n",
    "    n_dropped = 0\n",
    "    while i < n_iter:\n",
    "\n",
    "#         tw = random.uniform(0, 5)\n",
    "#         a0 = random.uniform(-4, 5)\n",
    "#         a1 = random.uniform(.5, 4)\n",
    "\n",
    "        tw = random.uniform(0, 5)\n",
    "        a0 = random.uniform(-5, 5)\n",
    "        a1 = random.uniform(0, 4)\n",
    "\n",
    "#         tw = random.uniform(0, 10)\n",
    "#         a0 = random.uniform(-10, 10)\n",
    "#         a1 = random.uniform(0, 10)\n",
    "\n",
    "\n",
    "#         print(tw,a1,\"...\",a0)\n",
    "        \n",
    "        if rasch:\n",
    "            _, probs, students_temp, qz_temp  = gen_rasch_run(n_factors, a0, a1, min_active, max_active, test_w = tw, n_students=n_students, n_questions=n_questions)\n",
    "        else:\n",
    "            _, _, students_temp, qz_temp  = gen_bayes_run(n_factors, a0, a1, min_active, max_active, test_w = tw, n_students=n_students, n_questions=n_questions)\n",
    "\n",
    "        students2 = students_temp\n",
    "        questions = qz_temp\n",
    "\n",
    "\n",
    "        obs = (random.random() < probs).astype(int)\n",
    "        exp_ob = numpy.mean(probs.flatten())\n",
    "        if exp_ob <= 0.1 or exp_ob >= 0.9:\n",
    "            n_dropped +=1\n",
    "            continue\n",
    "        i+=1\n",
    "            \n",
    "#         hard =numpy.round(probs)\n",
    "#         plt.hist(hard.flatten())\n",
    "#         plt.show()\n",
    "\n",
    "#         agt = 0 #numpy.zeros_like(probs)\n",
    "#         n_agt_runs = 20\n",
    "#         for _ in range(n_agt_runs):\n",
    "#             this_obs  = (numpy.random.random(probs.shape) < probs).astype(int)\n",
    "# #             plt.hist(this_obs.flatten())\n",
    "# #             plt.show()\n",
    "#             this_agt = numpy.mean(((hard==this_obs).astype(float)).flatten())\n",
    "# #             this_agt= numpy.std(probs.flatten() -0.5 )\n",
    "# #             print(\"Agt:\", this_agt)\n",
    "#             agt += this_agt \n",
    "#         agt /= n_agt_runs\n",
    "        \n",
    "#         plt.hist(probs.flatten())\n",
    "#         plt.show()\n",
    "#         print(\"tw, a1, a0\", tw, a1, a0)\n",
    "#         print(\"Mean agt:\", agt)\n",
    "#         print(\"Exp pr:\", exp_ob)\n",
    "        \n",
    "        \n",
    "        print(i, exp_ob)\n",
    "        inps.append([tw,a1, exp_ob])\n",
    "        outs.append(a0)\n",
    "#         for p in probs.flatten():\n",
    "#             inps.append([tw,a1, p])\n",
    "#             outs.append(a0)\n",
    "        \n",
    "    print(\"done!\")\n",
    "\n",
    "    es = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "    \n",
    "    split = len(inps)//10\n",
    "    vinps = numpy.array(inps[split:])\n",
    "    vouts = numpy.array(outs[split:])\n",
    "    inps = numpy.array(inps[:split])\n",
    "    outs = numpy.array(outs[:split])\n",
    "    print(inps.shape, outs.shape)\n",
    "\n",
    "    best_mse = math.inf\n",
    "    best = None\n",
    "    best_dims = None\n",
    "#     seen = set()\n",
    "                        \n",
    "    for nnw in [16]: # [2,4,8,16,32]:\n",
    "        for nnd in [2]: #[0,1,2,3,5]:\n",
    "#             if (nnw,nnd) in seen:\n",
    "#                 continue\n",
    "#             seen.add((nnw,nnd))\n",
    "            gen_m = generate_offset_generator(nnw, nnd)\n",
    "            h = gen_m.fit(inps,outs, epochs=10000, shuffle=True, batch_size=len(inps), callbacks=[es], validation_split=0.1, verbose=1)\n",
    "            \n",
    "#             gen_m = SVR()\n",
    "#             gen_m.fit(inps, outs)\n",
    "            \n",
    "            mse = mean_squared_error(vouts,gen_m.predict(vinps))\n",
    "            if mse < best_mse:\n",
    "                print(\"new best:\",nnw,nnd,\":\",mse)\n",
    "                best_mse = mse\n",
    "                best_dims = (nnw,nnd)\n",
    "                best = gen_m\n",
    "    \n",
    "    predz = best.predict(inps)\n",
    "#     for i,p,o in zip(inps,predz, outs):\n",
    "#         print(i, p, o)\n",
    "    print(\"Best generator for\", (n_factors, min_active, max_active), \"is\", best_dims)\n",
    "    print(\"avg\", numpy.mean(outs), \"vs\", numpy.mean(predz))\n",
    "    print(\"number of extreme cases = \", n_dropped, \"as prop of \",n_iter,\" iters\", n_dropped/n_iter)\n",
    "    print(mean_squared_error(vouts, best.predict(vinps)))\n",
    "    \n",
    "    h=None\n",
    "    return best, h, best_dims, best_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_arr_arr_err(split, real_wgts, pred_wgts, max_iter=10):\n",
    "    from scipy.spatial.distance import cosine\n",
    "# pred_wgts = numpy.round(pred_wgts,1)\n",
    "\n",
    "    print(split, real_wgts.shape, pred_wgts.shape, max_iter)\n",
    "\n",
    "    out_cols = [None] * len(real_wgts.T)\n",
    "    curr_sel = None\n",
    "    curr_ix = None\n",
    "    n_iters = 10\n",
    "    chosen = None\n",
    "    curr_real_ix = None\n",
    "    \n",
    "    indices = range(len(real_wgts.T))\n",
    "\n",
    "    min_total_err = math.inf\n",
    "    best_dis = math.inf\n",
    "    for i in range(max_iter): #len(indices)**2):\n",
    "        real_used = set()\n",
    "        pred_used = set()\n",
    "        while len(pred_used) < len(indices):\n",
    "            curr_err = math.inf\n",
    "            curr_cos = math.inf\n",
    "            for rix in numpy.random.permutation(indices):\n",
    "                if rix in real_used:\n",
    "                    continue\n",
    "                real_col = real_wgts.T[rix]\n",
    "                for cix in numpy.random.permutation(indices):\n",
    "                    if cix in pred_used:\n",
    "                        continue\n",
    "                    pred_col = pred_wgts.T[cix]\n",
    "                    pred_col = pred_col #* pred_q_col\n",
    "                    err = numpy.mean(numpy.abs( pred_col - real_col))\n",
    "                    \n",
    "                    if err < curr_err:\n",
    "                        curr_sel = pred_col\n",
    "                        curr_err = err\n",
    "                        curr_cos = 0#cosine(pred_col, real_col)\n",
    "                        curr_ix = cix\n",
    "                        curr_real_ix = rix\n",
    "            real_used.add(curr_real_ix)\n",
    "            pred_used.add(curr_ix)\n",
    "            out_cols[curr_real_ix] = curr_sel\n",
    "        out_col_arr = numpy.array(out_cols).T\n",
    "        total_err = numpy.mean(numpy.abs( out_col_arr - real_wgts ))\n",
    "        \n",
    "        dis = 0\n",
    "        mean_ll = numpy.mean( out_col_arr - real_wgts )\n",
    "        if total_err < min_total_err:\n",
    "            min_total_err = total_err\n",
    "            total_q_err = numpy.mean(numpy.abs( out_col_arr[0:split] - real_wgts[0:split] ))\n",
    "            total_s_err = numpy.mean(numpy.abs( out_col_arr[split:] - real_wgts[split:] ))\n",
    "            best_ll = mean_ll\n",
    "            chosen = out_col_arr\n",
    "            best_dis = dis\n",
    "    return chosen, min_total_err, total_q_err, total_s_err, mean_ll, best_dis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "def plot_items(pred_list, real_items, s_offset):\n",
    "    if real_items.shape[1]<2:\n",
    "        print(\"real_items is only 1 component wide .. needs to be >1 to plot on a PCA graph\")\n",
    "        return None\n",
    "    elif real_items.shape[1]==2:\n",
    "        print(\"2 comps, so no dim reduc\")\n",
    "        tx=None\n",
    "    else:\n",
    "        tx = PCA(n_components=2)\n",
    "#         tx = TSNE(n_components=2)\n",
    "    fyrst = True\n",
    "    Cs = []\n",
    "    C_labs = []\n",
    "    pred_list = numpy.array(pred_list)\n",
    "    print(\"pred list shape\", pred_list.shape)\n",
    "    print(\"real items shape\", real_items.shape)\n",
    "\n",
    "\n",
    "    fitted_pred_list = []\n",
    "#     offset = numpy.median(pred_list[(pred_list>0.1)], axis=0) - numpy.median(real_items[(real_items>0.1)], axis=0)\n",
    "#     real_mean = numpy.min(real_items[real_items > 0.1])\n",
    "#     offset = numpy.min(pred_list[pred_list > 0.1]) - real_mean\n",
    "#     print(\"real mean\", real_mean)\n",
    "#     print(\"offset\", offset)\n",
    "    \n",
    "    m = len(real_items)\n",
    "    cols = list(range(m))\n",
    "    shuffle(cols)\n",
    "    \n",
    "    xmeans = numpy.zeros(m)\n",
    "    ymeans = numpy.zeros(m)\n",
    "    pairs = defaultdict(list)\n",
    "    iter = 0\n",
    "    \n",
    "    cp_real = copy.copy(real_items)\n",
    "    cp_real[cp_real < 1] = numpy.nan\n",
    "    r_offset=numpy.nanmedian(cp_real, axis=0)\n",
    "    \n",
    "    itemz_2 = real_items\n",
    "    n = len(real_items)\n",
    "    \n",
    "    for opreds in pred_list:\n",
    "        preds = copy.copy(opreds) #- s_offset[iter] + r_offset\n",
    "        split = 0\n",
    "        \n",
    "        items_chosen, min_total_err, total_q_err, total_s_err, mean_ll, best_cos_dis = calc_arr_arr_err(0, real_items, preds, max_iter=10)\n",
    "\n",
    "        itemz_pred = items_chosen\n",
    "        print(itemz_pred)\n",
    "#         itemz_pred = numpy.maximum(itemz_pred,0)\n",
    "        fitted_pred_list.append(itemz_pred)\n",
    "        \n",
    "#         itemz = real_items #- offset\n",
    "#         print(numpy.min(itemz), numpy.mean(itemz), numpy.max(itemz))\n",
    "#         itemz = numpy.maximum(itemz,0)\n",
    "#         print(numpy.min(itemz), numpy.mean(itemz), numpy.max(itemz))\n",
    "#         if itemz_2 is None:\n",
    "#             itemz_2 = numpy.concatenate([real_itemz, itemz_pred], axis=0)\n",
    "#         else:\n",
    "        itemz_2 = numpy.concatenate([itemz_2, itemz_pred], axis=0)\n",
    "\n",
    "    if tx:\n",
    "        itemz_2 = tx.fit_transform(itemz_2)\n",
    "#         itemz_2 = numpy.concatenate([itemz, itemz_pred], axis=0)\n",
    "#         if fyrst:\n",
    "#         itemz_2 = tx.fit_transform(itemz_2)\n",
    "#             fyrst = False\n",
    "#         else:\n",
    "#             itemz_2 = tx.transform(itemz_2)\n",
    "\n",
    "    from sklearn.cluster import KMeans\n",
    "    iter=0\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(10, 10)\n",
    "    \n",
    "    for opreds, itemz_pred in zip(pred_list, fitted_pred_list):\n",
    "        n = len(itemz_pred)\n",
    "        km = KMeans()\n",
    "        km.fit(itemz_pred)\n",
    "        cluster_labels = km.predict(itemz_pred)\n",
    "        print(cluster_labels)\n",
    "        \n",
    "        C = []\n",
    "        for l in set(cluster_labels):\n",
    "            cluster = list(numpy.where(cluster_labels==l)[0])\n",
    "            print(\"X\", cluster)\n",
    "            C.append(cluster)\n",
    "        Cs.append(C)\n",
    "        C_labs.append(cluster_labels)\n",
    "                \n",
    "#         NUM_COLORS = 100\n",
    "#         cm = plt.get_cmap('gist_rainbow')\n",
    "#         fig.gca().set_color_cycle([cm(1.*i/NUM_COLORS) for i in range(NUM_COLORS)])\n",
    "        print(type(itemz_2))\n",
    "        minix=n*(iter+1)\n",
    "        maxix=n*(iter+1)+n\n",
    "        \n",
    "        #i=0 -> 100,199\n",
    "        #i=2 -> 200,299\n",
    "        \n",
    "        print(\"no pts=\",n,\" indices=\", minix, maxix)\n",
    "        print(\"shape of itemz_2\", itemz_2.shape)\n",
    "        fig.gca().scatter(itemz_2[minix:maxix,0], itemz_2[minix:maxix,1], alpha=0.7, c=numpy.array(cols), cmap=plt.get_cmap('nipy_spectral'))\n",
    "        j=0\n",
    "        for j in range(n):\n",
    "            x,xh,y,yh = itemz_2[j+(n*iter),0], itemz_2[j+(n*iter+n) ,0], itemz_2[j+(n*iter),1], itemz_2[j+(n*iter+n),1]\n",
    "#             fig.gca().plot([x,xh],[y,yh],  color=\"#aaaaaa80\")\n",
    "            xmeans[j] += xh\n",
    "            ymeans[j] += yh\n",
    "            pairs[iter].append((xh, yh))\n",
    "        iter+=1\n",
    "        \n",
    "    for j in range(n):\n",
    "        fig.gca().annotate(j, (itemz_2[j,0], itemz_2[j,1]))\n",
    "        \n",
    "    fig.gca().scatter(itemz_2[0:n,0], itemz_2[0:n,1], c=\"b\", zorder=10, alpha=0.5)\n",
    "#     fig.gca().axhline(y=1e-6, linestyle=\"--\")\n",
    "#     fig.gca().axvline(x=1e-6, linestyle=\"--\")\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    \n",
    "    xmeans = xmeans / len(pred_list)\n",
    "    ymeans = ymeans / len(pred_list)\n",
    "    \n",
    "\n",
    "    if len(pred_list)>=1:\n",
    "        for it in range(len(pred_list)):\n",
    "            xhyh_pairs = pairs[it]\n",
    "            for j,hat_pair in enumerate(xhyh_pairs):\n",
    "                xh,yh = hat_pair\n",
    "                x,y = itemz_2[j,0], itemz_2[j,1]\n",
    "                mux = xmeans[j]\n",
    "                muy = ymeans[j]\n",
    "#                 fig.gca().scatter(xh, yh, alpha=0.7, c=plt.get_cmap('nipy_spectral')(cols[j]))\n",
    "#                 fig.gca().scatter(mux,muy, c=\"#888888ff\", marker=\"*\", zorder=10)\n",
    "#                 fig.gca().plot([mux,xh],[muy,yh],color=\"#aaaaaa80\", linestyle=\"--\")\n",
    "#                 fig.gca().plot([mux,x],[muy,y],color=\"#888888dd\", linestyle=\"-\")\n",
    "                fig.gca().plot([x,xh],[y,yh],color=\"#aaaaaa80\", linestyle=\"--\")\n",
    "        \n",
    "    plt.show()\n",
    "    print(\"len Cs\", len(Cs))\n",
    "    from sklearn.metrics.cluster import adjusted_rand_score\n",
    "    rands = []\n",
    "    for ix in range(len(Cs)):\n",
    "#         print(ix)\n",
    "        for jx in range(len(Cs)):\n",
    "#             print(jx)\n",
    "            if ix!=jx:\n",
    "#             print(Cs[ix], Cs[jx])\n",
    "#                 print(\"VI:\", ix,jx, varinfo(Cs[ix],Cs[jx]))\n",
    "                a_rand = adjusted_rand_score(C_labs[ix], C_labs[jx])\n",
    "                print(\"Rand:\", a_rand)\n",
    "                rands.append(a_rand)\n",
    "    print(\"Mean rand score =\", numpy.mean(rands), numpy.std(rands))\n",
    "\n",
    "# qws = qn_table.get_weights()[0]\n",
    "# qws2 = qn_table2.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, accuracy_score\n",
    "def generate_and_train(n_students, n_questions, qz,sz,pfz, vqz,vsz,vpfz, w, n_factors, min_active, max_active, nn_mode=None, loss_mode=None):\n",
    "    btm = 0\n",
    "    top = math.sqrt(.1/w)\n",
    "#     init= (btm,top)\n",
    "#     init = math.sqrt(.5/w)\n",
    "    init_s = (0,1)\n",
    "    init_q = (0,1)\n",
    "    \n",
    "#     1-p + pq = s\n",
    "#     q=0.3 : 1-p + p/3 = s\n",
    "#           : 3-3p + p = s\n",
    "#           : p = (3-s)/2\n",
    "        \n",
    "    if nn_mode==\"COND\":\n",
    "        percompp = .5**(1/w)\n",
    "        print(\"percompp\", percompp)\n",
    "\n",
    "        s_table =  BigTable((n_students, w), 0,1, init_hilo= percompp )#, regulariser=regularizers.l2(10e-6))\n",
    "        qn_table = BigTable((n_questions, w), 0,1, init_hilo= percompp )#, regulariser=regularizers.l1(10e-6))\n",
    "    elif nn_mode==\"MXFN\":\n",
    "        init = math.sqrt(.5/w)\n",
    "        print(\"MXFN init'n\")\n",
    "        print(init)\n",
    "        print(init*init*w)\n",
    "        s_table =  BigTable((n_students, w), -math.inf, math.inf, init_hilo= init) #, regulariser=regularizers.l2(10e-6))\n",
    "        qn_table = BigTable((n_questions, w), -math.inf, math.inf, init_hilo= init) #, regulariser=regularizers.l1(10e-6))\n",
    "    elif nn_mode==\"MLTM\":\n",
    "        sp = pr_to_spread(.5, w, as_A_and_D=False)\n",
    "        print(\"sp is \",sp)\n",
    "        s_table =  BigTable((n_students, w), -math.inf, math.inf, init_hilo= 0) #, regulariser=regularizers.l2(10e-6))\n",
    "        qn_table = BigTable((n_questions, w), -math.inf, math.inf, init_hilo= -sp) #, regulariser=regularizers.l1(10e-6))        \n",
    "    else:\n",
    "        s_table =  BigTable((n_students, w), -math.inf, math.inf, init_hilo= 0) #, regulariser=regularizers.l2(10e-6))\n",
    "        qn_table = BigTable((n_questions, w), -math.inf, math.inf, init_hilo= 0) #, regulariser=regularizers.l1(10e-6))        \n",
    "                \n",
    "    from keras.layers import Embedding\n",
    "    from keras.constraints import NonNeg, MinMaxNorm\n",
    "    from keras.initializers import RandomNormal, RandomUniform\n",
    "    \n",
    "#     wc=WeightClip(0,1)\n",
    "    \n",
    "#     q_gates = None #Embedding(n_questions,w, input_length=1, embeddings_initializer=RandomUniform(minval=0, maxval=1, seed=None), embeddings_constraint=wc)\n",
    "#     qn_table = Embedding(n_questions,w, input_length=1, embeddings_initializer=RandomNormal(mean=6, stddev=0.3))\n",
    "#     s_table = Embedding(n_students,w, input_length=1, embeddings_constraint=WeightClip(0,math.inf), embeddings_initializer=RandomNormal(mean=6, stddev=0.3))\n",
    "    \n",
    "    from keras.optimizers import Adam\n",
    "    from keras.callbacks import EarlyStopping\n",
    "    \n",
    "#     vqz=[]\n",
    "    if len(vqz)>0:\n",
    "        lozz=\"val_loss\"\n",
    "        val_dat= [[vqz,vsz], vpfz]\n",
    "    else:\n",
    "        lozz=\"loss\"\n",
    "        val_dat=None\n",
    "    \n",
    "    fiftiez = numpy.zeros_like(pfz) + .50\n",
    "    for _ in range(1):\n",
    "#         es = EarlyStopping(monitor=\"loss\", restore_best_weights=True, patience=10)\n",
    "#         m = generate_qs_model(qn_table, s_table, Adam(lr=0.001))\n",
    "#         h = m.fit(x=[qz,sz], y=numpy.array(fiftiez).reshape(-1,1), batch_size=len(pfz), shuffle=True, epochs=10000, verbose=1, callbacks=[es])\n",
    "#         wz = m.get_weights()\n",
    "        m = generate_qs_model(qn_table, s_table, Adam(), _mode=nn_mode, loss=loss_mode)\n",
    "#         m.set_weights(wz)\n",
    "        tr_predz = (m.predict([qz,sz]) > 0.5)\n",
    "        v_predz  = (m.predict([vqz,vsz]) > 0.5)\n",
    "#         for vs,vq,tp,pp in zip(vsz,vqz,predz, vpfz):\n",
    "#             print(vs,vq,\"-\",tp,pp)\n",
    "        print(\"PRE-TR AVG  = \", numpy.mean(tr_predz))\n",
    "        print(\"PRE-TR VAVG = \", numpy.mean(v_predz))\n",
    "\n",
    "        es = EarlyStopping(monitor=lozz, restore_best_weights=True, patience=10)\n",
    "        \n",
    "        _bs = len(pfz)\n",
    "#         _bs = 32\n",
    "        h = m.fit(x=[qz,sz], y=numpy.array(pfz).reshape(-1,1), batch_size=_bs, shuffle=True, epochs=10000, verbose=1, callbacks=[es], validation_data=val_dat)\n",
    "        tr_predz = m.predict([qz,sz])  \n",
    "        v_predz  = m.predict([vqz,vsz])\n",
    "\n",
    "        print(\"TR AVG = \", numpy.mean(tr_predz))\n",
    "        print(\"TR R2  = \", r2_score(pfz, tr_predz))\n",
    "        print(\"TR MAE = \", mean_absolute_error(pfz, tr_predz))\n",
    "        print(\"TR ACC = \", accuracy_score((pfz>0.5), (tr_predz>0.5)))\n",
    "        print(\"TR AGT = \", accuracy_score([random.random() < p for p in pfz], [random.random() < p for p in tr_predz]))\n",
    "        \n",
    "        if val_dat:\n",
    "            print(\"VA AVG = \", numpy.mean(v_predz))\n",
    "            print(\"VA R2  = \", r2_score(vpfz, v_predz))\n",
    "            print(\"VA MAE = \", mean_absolute_error(vpfz, v_predz))\n",
    "            print(\"VA ACC = \", accuracy_score((vpfz>0.5), (v_predz>0.5)))\n",
    "            print(\"VA AGT = \", accuracy_score([random.random() < p for p in vpfz], [random.random() < p for p in v_predz]))\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(classification_report((pfz>0.5), (tr_predz>0.5)))\n",
    "    print(classification_report((vpfz>0.5), (v_predz>0.5)))\n",
    "            \n",
    "#     h = m.fit(x=[qz,sz], y=pfz.flatten(), batch_size=32, shuffle=True, epochs=1000, verbose=1, callbacks=[es], validation_data=[[vqz,vsz], vpfz])\n",
    "    return s_table, qn_table, m, h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_n_split(_pairs, _probs):\n",
    "    _prob_list =  numpy.array([_probs[vi,mi] for (vi,mi) in _pairs])\n",
    "    _pfz = _prob_list\n",
    "#     _pfz = (numpy.random.random(len(_prob_list)) < _prob_list).astype(int)\n",
    "#     _pfz = (0.5 < _prob_list).astype(int)\n",
    "    print(_prob_list)\n",
    "    print(_pfz)\n",
    "#     _pfz = numpy.array([probs[vi,mi] for (vi,mi) in _pairs])\n",
    "\n",
    "    _matches = ( numpy.round(_prob_list) == _pfz).astype(int)\n",
    "    print(_matches)\n",
    "    print(numpy.sum(_matches), \"correctly labelled out of\", len(_matches), \"%=\", numpy.sum(_matches)/len(_matches))\n",
    "\n",
    "    _sz = [p[0] for p in _pairs]\n",
    "    _qz = [p[1] for p in _pairs]\n",
    "    return _pfz, _sz, _qz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "1 0.01 0.01\n",
      "c [0 0 0 0 3 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 2 0 0 1 0 2 0 0 1 2 0 1 1 0 0 0\n",
      " 1 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 1 0 2 0 1 1 0 0 0 0 0 0 0 0 1 0\n",
      " 1 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0]\n",
      "17 0.39 0.45\n",
      "c [2 0 0 1 3 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 2 3 0 1 2 1 3 1 0 2 2 0 2 1 1 0 0\n",
      " 1 3 1 0 1 1 0 0 0 1 1 2 1 0 2 0 0 0 2 1 1 2 0 3 0 1 1 1 0 1 0 2 0 0 0 1 0\n",
      " 1 4 0 1 0 1 0 2 2 1 1 0 1 1 3 3 1 0 1 0 0 2 1 1 0 1]\n",
      "34 0.62 0.93\n",
      "c [3 0 2 1 3 2 2 0 1 2 3 2 1 2 0 1 0 1 0 0 3 4 0 4 2 1 3 3 2 2 3 0 4 1 1 2 1\n",
      " 1 4 1 0 1 1 0 0 0 2 2 2 1 0 3 0 0 0 2 1 2 4 1 3 0 1 1 2 0 1 0 3 1 0 0 1 1\n",
      " 1 4 1 1 0 1 0 2 3 2 1 0 1 1 3 3 1 1 2 0 1 2 2 1 0 2]\n",
      "50 0.74 1.4\n",
      "c [4 0 2 2 3 2 2 0 1 3 4 2 2 2 0 1 0 1 0 0 3 4 1 5 2 1 6 3 2 2 3 0 4 1 1 2 1\n",
      " 1 6 1 1 1 1 1 1 0 3 2 3 1 0 4 1 1 1 3 2 2 4 1 4 1 1 1 2 0 1 0 3 2 0 0 1 1\n",
      " 2 4 1 2 0 2 1 2 3 2 1 1 2 3 3 4 1 1 2 0 2 2 3 1 1 2]\n",
      "67 0.85 1.78\n",
      "c [6 1 2 2 3 2 2 0 2 4 4 2 3 2 1 2 1 2 2 0 3 4 1 5 2 1 8 3 4 4 4 0 4 2 1 2 1\n",
      " 2 6 1 1 2 1 1 2 1 3 5 3 2 0 4 2 1 3 4 3 2 4 1 4 1 2 1 5 2 1 0 5 3 1 0 2 1\n",
      " 3 4 2 2 0 4 1 2 3 2 1 1 2 3 4 4 1 1 2 2 2 2 4 1 2 2]\n",
      "83 0.93 2.31\n",
      "c [7 4 2 3 4 4 4 1 2 5 4 2 4 2 2 3 1 2 4 0 3 5 1 6 2 1 8 3 4 4 4 0 5 2 1 2 1\n",
      " 2 6 2 1 3 3 1 3 3 3 5 3 2 0 7 2 1 4 5 3 2 4 1 4 2 2 1 6 2 2 1 6 4 2 1 2 1\n",
      " 3 4 2 3 0 4 1 3 3 2 1 2 2 3 4 5 2 1 3 2 2 2 5 1 2 3]\n",
      "100 0.96 2.79\n",
      "c [ 7  4  2  4  4  4  6  1  2  6  5  3  5  2  2  3  1  2  4  1  3  5  2  7\n",
      "  2  1 10  3  4  4  4  0  6  2  2  2  1  3  6  2  1  3  4  2  3  3  4  5\n",
      "  4  2  1  8  2  1  5  5  3  3  4  1  4  3  2  2  6  3  2  1  6  4  2  1\n",
      "  3  2  4  4  2  3  1  5  1  3  3  2  3  2  2  4  4  5  3  1  4  2  3  2\n",
      "  5  1  2  3]\n",
      "116 0.99 3.16\n",
      "c [ 7  4  2  4  5  4  6  2  2  6  6  5  5  3  2  4  2  3  5  1  3  5  3  7\n",
      "  4  2 12  4  4  4  4  1  6  2  2  2  1  4  6  2  1  3  5  2  4  3  4  5\n",
      "  5  2  1  8  3  1  5  5  5  3  4  1  4  3  2  2  6  4  3  1  6  5  2  1\n",
      "  4  4  5  4  3  3  1  5  2  3  3  2  4  2  2  5  5  5  4  3  4  2  3  3\n",
      "  6  1  2  3]\n",
      "133 1.0 3.58\n",
      "c [ 7  4  2  6  5  4  6  4  3  7  7  5  5  3  2  5  3  4  5  1  3  5  5  8\n",
      "  4  2 13  5  4  4  4  1  6  2  2  3  1  4  7  2  3  4  6  2  4  3  5  6\n",
      "  6  2  2  8  3  1  6  6  6  4  5  2  5  3  2  2  7  4  3  1  7  6  3  1\n",
      "  4  4  6  5  4  3  1  5  4  4  3  2  5  4  3  6  6  6  4  6  4  3  4  3\n",
      "  7  3  4  3]\n",
      "150 1.0 4.17\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7ca7af1d0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calc_skills_coverage():\n",
    "    xs=[]\n",
    "    ys=[]\n",
    "    ycnts=[]\n",
    "    n_factors, min_active, max_active = 100,1,5\n",
    "    n_students = 1\n",
    "    tw=1\n",
    "    a0=1\n",
    "    a1=1\n",
    "    max_n_qns = 150\n",
    "    n_questions_list = numpy.linspace(1,max_n_qns,num=10).astype(\"int\")\n",
    "\n",
    "    _, _, students_temp, qz_temp  = gen_rasch_run(n_factors, a0, a1, min_active, max_active, test_w = tw, n_students=n_students, n_questions=max_n_qns)\n",
    "    for n_questions in n_questions_list:\n",
    "        cnt = numpy.array([False]*n_factors).astype(\"int\")\n",
    "#         plt.hist(qz_temp.flatten(), alpha=0.5)\n",
    "#         plt.show()\n",
    "        for q in qz_temp[0:n_questions]:\n",
    "            active = (q > -10).astype(\"int\")\n",
    "#             print(\"a\",active)\n",
    "            cnt = cnt + active\n",
    "            seen = numpy.clip(cnt, 0,1)\n",
    "        print(\"c\",cnt)\n",
    "        print(n_questions, numpy.mean(seen), numpy.mean(cnt))\n",
    "        xs.append(n_questions)\n",
    "        ys.append(numpy.mean(seen))\n",
    "        ycnts.append(numpy.mean(cnt))\n",
    "    plt.plot(xs,ys)\n",
    "    plt.plot(xs,ycnts)\n",
    "    plt.show()\n",
    "    \n",
    "calc_skills_coverage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_m_cache = pickle.load(open(\"generators.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_m_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tup = (100,100,100)\n",
    "n_factors, min_active, max_active = tup\n",
    "\n",
    "# sss = [10,20,30,50,70]\n",
    "# nits = [1000,10000,50000]\n",
    "sss = [70]\n",
    "nits = [10000]\n",
    "\n",
    "\n",
    "for ss in sss:\n",
    "    for nit in nits:\n",
    "        key = ((tup,(nit,ss)))\n",
    "#         if key in gen_m_cache:\n",
    "#             del gen_m_cache[key]\n",
    "        print(\"creating genny\")\n",
    "        gen_m, history, best_dims, best_mse = create_offset_generator(n_factors, min_active, max_active, sampsize=ss, n_iter=nit)\n",
    "        gen_m_cache[key] = (gen_m, history, best_dims, best_mse)\n",
    "\n",
    "        # summarize history for loss\n",
    "#         plt.plot(history.history['loss'])\n",
    "#         plt.plot(history.history['val_loss'])\n",
    "#         plt.title('Model loss for {}x{}'.format(ss,nit))\n",
    "#         plt.ylabel('loss')\n",
    "#         plt.xlabel('epoch')\n",
    "#         plt.legend(['train', 'test'], loc='upper right')\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss   = [10,20,30,50,70]\n",
    "nits = [1000, 10000, 50000]\n",
    "tup = (100,1,5)\n",
    "for ss in sss:\n",
    "    for nit in nits:\n",
    "        key = ((tup,(nit,ss)))\n",
    "        \n",
    "        (gen_m, history, best_dims, best_mse) = gen_m_cache[key]\n",
    "        # summarize history for loss\n",
    "#         plt.plot(history.history['loss'])\n",
    "#         plt.plot(history.history['val_loss'])\n",
    "        print(key, best_dims, best_mse)\n",
    "# plt.title('Model loss for {}x{}'.format(ss,nit))\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train {} {}'.format(ss,nit), 'test {} {}'.format(ss,nit)], loc='upper right')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [10,20,30,50,70]\n",
    "ys = [\n",
    "    0.26942141892910004,\n",
    "    0.12947245151996611,\n",
    "    0.08696433037519455,\n",
    "    0.051164472541213035,\n",
    "    0.04078790558874607\n",
    "] \n",
    "x2s = [x**2 for x in xs]\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(x2s, ys)\n",
    "plt.title(\"Best model loss, 10000 samples \")\n",
    "plt.xlabel(\"Data points per sample\")\n",
    "plt.ylabel(\"$J_{MSE}$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(gen_m_cache, open(\"generators.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tup = (100,1,5)\n",
    "key = (tup, (10000,70))\n",
    "\n",
    "if key in gen_m_cache:\n",
    "#     gen_m = gen_m_cache[tup]\n",
    "    (gen_m, history, best_dims, best_mse) = gen_m_cache[key]\n",
    "else:\n",
    "    print(key)\n",
    "    print(gen_m_cache.keys())\n",
    "    raise Exception(\"Trying to use generator that does not exist\")\n",
    "\n",
    "factors_master = [tup]\n",
    "w_list = [200]\n",
    "factors_list = [ m+(w,) for m in factors_master for w in w_list ]\n",
    "\n",
    "bal=0.5\n",
    "explore_mode = False\n",
    "\n",
    "distro_params = []\n",
    "pfz_list = []\n",
    "\n",
    "pfzz = None #numpy.array([])\n",
    "lookup = {}\n",
    "\n",
    "i=0\n",
    "explore_mode=True\n",
    "toomuch, toolittle, goldi = 0,0,0\n",
    "for (n_factors, min_active, max_active, _) in factors_list:\n",
    "\n",
    "    model_list=[]\n",
    "    rasch=True\n",
    "\n",
    "    questions=None\n",
    "\n",
    "    qws_list = []\n",
    "    sws_list = []\n",
    "    tr_list = []\n",
    "    params_list = []\n",
    "    # questions=None\n",
    "    real_stu_list=[]\n",
    "    real_que_list=[]\n",
    "    perseverance_list=[]\n",
    "    test_datasets=[]\n",
    "\n",
    "    pred_list = []\n",
    "\n",
    "    found = False\n",
    "    tensorflow.random.set_seed(666)\n",
    "    seed(666)\n",
    "    mode = \"n\"\n",
    "    \n",
    "    print(gen_m_cache.keys())\n",
    "    \n",
    "    for ss in sss:\n",
    "        for nit in nits:\n",
    "            tup = n_factors, min_active, max_active\n",
    "            key = ((tup,(nit,ss)))\n",
    "#             gen_m = gen_m_cache[key]\n",
    "            (gen_m, history, best_dims, best_mse) = gen_m_cache[key]\n",
    "            while i < 1:\n",
    "                tw = random.uniform(0.5, 3.5)\n",
    "                a1 = random.uniform(1, 3.5)\n",
    "        #         a1 = random.uniform(1, 5)\n",
    "\n",
    "                if mode==\"n\":\n",
    "                    a0n = gen_m.predict(numpy.array([[tw,a1, bal]]).reshape(1,-1))\n",
    "                else:\n",
    "                    a0r = random.uniform(-1,8.7)\n",
    "\n",
    "                print(\"gening data\")\n",
    "                _, probs, students_temp, qz_temp  = gen_rasch_run(n_factors, a0, a1, min_active, max_active, test_w = tw, n_students=100, n_questions=100)\n",
    "\n",
    "                print(\"run created\")\n",
    "                students2 = students_temp\n",
    "\n",
    "            #     if questions is None:\n",
    "                questions = qz_temp\n",
    "\n",
    "        #                         qn_av = numpy.mean(questions, axis=0)\n",
    "        #                         qn_std = numpy.std(questions, axis=0)\n",
    "\n",
    "                if explore_mode:\n",
    "                    plot_items([], questions, None)\n",
    "\n",
    "                    bin_spread = lambda x: max(1,int(abs(2*(numpy.max(x)-numpy.min(x)))))\n",
    "\n",
    "                    plt.hist(students2.flatten(), alpha=0.5, bins=bin_spread(students2))\n",
    "                    plt.hist(questions.flatten(), alpha=0.5, bins=bin_spread(questions))\n",
    "                    plt.show()\n",
    "\n",
    "                obs=numpy.zeros((len(students2), len(questions)))\n",
    "                pfz = probs.flatten()\n",
    "                if pfzz is None:\n",
    "                    pfzz = numpy.array(pfz)\n",
    "                else:\n",
    "                    numpy.concatenate((pfzz, pfz))\n",
    "\n",
    "                if explore_mode:\n",
    "                #     plt.hist(probs.flatten(), alpha=0.5)\n",
    "                    plt.title(\"Histogram of $p_{pass}$\")\n",
    "                    plt.xlabel(\"$p_{pass}\")\n",
    "                    plt.ylabel(\"Frequency\")\n",
    "                    plt.legend()\n",
    "                    plt.show()\n",
    "                    plt.hist(pfz.flatten(), alpha=0.5)\n",
    "                    plt.title(\"pfz\")\n",
    "                    plt.show()          \n",
    "\n",
    "                hard = (probs >= 0.5)\n",
    "                agt = 0 #numpy.zeros_like(probs)\n",
    "                n_agt_runs = 10\n",
    "                for _ in range(n_agt_runs):\n",
    "                    this_obs  = (numpy.random.random(probs.shape) < probs).astype(int)\n",
    "                    this_agt = numpy.mean((hard==this_obs).astype(int).flatten())\n",
    "                    agt += this_agt / n_agt_runs\n",
    "                print(\"*** AGT:\", agt)                    \n",
    "\n",
    "                print(tw, a1, a0)\n",
    "                mn = numpy.mean(probs.flatten())\n",
    "                if mn > 0.9:\n",
    "                    toomuch+=1\n",
    "                if mn < 0.1:\n",
    "                    toolittle+=1\n",
    "                if (mn < 0.6) and (mn > 0.4):\n",
    "                    goldi +=1\n",
    "\n",
    "        #         distro_params.append((vmn,agt))\n",
    "                pfz_list.append(pfz)\n",
    "                i+=1\n",
    "\n",
    "        if explore_mode:\n",
    "        #     plt.hist(probs.flatten(), alpha=0.5)\n",
    "            plt.title(\"Histogram of $p_{pass}$\")\n",
    "            plt.xlabel(\"$p_{pass}\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            plt.hist(pfzz.flatten(), alpha=0.5)\n",
    "            plt.title(\"pfz\")\n",
    "            plt.show()\n",
    "        print(numpy.mean(pfzz), numpy.std(pfzz))\n",
    "        print(toolittle, goldi, toomuch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tup = (100,1,5)\n",
    "key = (tup,(10000,70))\n",
    "if tup in gen_m_cache:\n",
    "    gen_m = gen_m_cache[tup]\n",
    "else:\n",
    "    raise Exception(\"Trying to use generator that does not exist\")\n",
    "\n",
    "factors_master = [tup]\n",
    "w_list = [200]\n",
    "factors_list = [ m+(w,) for m in factors_master for w in w_list ]\n",
    "\n",
    "bal=0.500\n",
    "explore_mode = False\n",
    "\n",
    "distro_params = []\n",
    "\n",
    "\n",
    "balance_lookup = {}\n",
    "\n",
    "\n",
    "# for (n_factors, min_active, max_active, _) in factors_list:\n",
    "bal_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "for bal in bal_list:\n",
    "    i=0\n",
    "    model_list=[]\n",
    "    rasch=True\n",
    "\n",
    "    questions=None\n",
    "\n",
    "    qws_list = []\n",
    "    sws_list = []\n",
    "    tr_list = []\n",
    "    params_list = []\n",
    "    # questions=None\n",
    "    real_stu_list=[]\n",
    "    real_que_list=[]\n",
    "    perseverance_list=[]\n",
    "    test_datasets=[]\n",
    "\n",
    "    pred_list = []\n",
    "\n",
    "    tensorflow.random.set_seed(666)\n",
    "    seed(666)\n",
    "    found = False\n",
    "    \n",
    "    probs_list = []\n",
    "    pfz_list = []\n",
    "    while i < 100:\n",
    "        print(\"BAL target:\", bal)\n",
    "        tw = random.uniform(0.5, 3.5)\n",
    "#                         a1 = random.uniform(1, 3.5)\n",
    "        a1 = random.uniform(1, 3.5)\n",
    "        a0 = gen_m.predict(numpy.array([[tw,a1, bal]]).reshape(1,-1))\n",
    "#         a0 = random.uniform(-1,1.6)\n",
    "#         a0 = random.uniform(-1,3.5)\n",
    " \n",
    "        print(\"gening data\")\n",
    "        _, probs, students_temp, qz_temp  = gen_rasch_run(n_factors, a0, a1, min_active, max_active, test_w = tw, n_students=100, n_questions=100)\n",
    "\n",
    "        print(\"run created\")\n",
    "        students2 = students_temp\n",
    "\n",
    "    #     if questions is None:\n",
    "        questions = qz_temp\n",
    "\n",
    "#                         qn_av = numpy.mean(questions, axis=0)\n",
    "#                         qn_std = numpy.std(questions, axis=0)\n",
    "\n",
    "        if explore_mode:\n",
    "            plot_items([], questions, None)\n",
    "\n",
    "            bin_spread = lambda x: max(1,int(abs(2*(numpy.max(x)-numpy.min(x)))))\n",
    "\n",
    "            plt.hist(students2.flatten(), alpha=0.5, bins=bin_spread(students2))\n",
    "            plt.hist(questions.flatten(), alpha=0.5, bins=bin_spread(questions))\n",
    "            plt.show()\n",
    "\n",
    "        obs=numpy.zeros((len(students2), len(questions)))\n",
    "\n",
    "        all_pairs = []\n",
    "        tr_pairs = []\n",
    "        v_pairs = []\n",
    "        tt_pairs = []\n",
    "        perseverance = []\n",
    "        slist = list(range(len(students2)))\n",
    "        random.seed(666)\n",
    "        shuffle(slist)\n",
    "        for vi in slist:\n",
    "#                             c=0\n",
    "            p_cont = None #(n_students * n_questions)//5\n",
    "            v_size = p_cont\n",
    "            qlist= list(range(len(questions)))\n",
    "            shuffle(qlist)\n",
    "            first = True\n",
    "            for mi in qlist:\n",
    "                if first:\n",
    "                    tt_pairs.append((vi,mi))\n",
    "                    first = False\n",
    "                else:\n",
    "                    tr_pairs.append((vi,mi))\n",
    "\n",
    "        print(\"splitting\")\n",
    "        tr_pairs, _ = train_test_split(tr_pairs, test_size=0.5, shuffle=False)\n",
    "        tr_pairs, v_pairs = train_test_split(tr_pairs, test_size=0.1, shuffle=False)\n",
    "        print(\"splut\")\n",
    "\n",
    "#                         print(\"scanning\")\n",
    "#                         for pa in tr_pairs:\n",
    "# #                             print(pa)\n",
    "#                             if pa in tt_pairs:\n",
    "#                                 print(\"TR IN TT\")\n",
    "#                                 raise Exception\n",
    "#                             if pa in v_pairs:\n",
    "#                                 print(\"TR IN V\")\n",
    "#                                 raise Exception\n",
    "#                         print(\"scun\")\n",
    "\n",
    "        pfz, sz, qz = stitch_n_split(tr_pairs, probs)\n",
    "        vpfz, vsz, vqz = stitch_n_split(v_pairs, probs)\n",
    "\n",
    "        print(\"lens of pfz and vpfz, tt_pairs\", len(pfz), len(vpfz), len(tt_pairs))\n",
    "\n",
    "#                         print(probs)\n",
    "\n",
    "        hard = (probs >= 0.5)\n",
    "        agt = 0 #numpy.zeros_like(probs)\n",
    "        n_agt_runs = 10\n",
    "        for _ in range(n_agt_runs):\n",
    "            this_obs  = (numpy.random.random(probs.shape) < probs).astype(int)\n",
    "            this_agt = numpy.mean((hard==this_obs).astype(int).flatten())\n",
    "            agt += this_agt / n_agt_runs\n",
    "        print(\"*** AGT:\", agt)                            \n",
    "\n",
    "        if explore_mode:\n",
    "            plt.hist(probs.flatten(), alpha=0.5)\n",
    "            plt.title(\"Histogram of $p_{pass}$\")\n",
    "            plt.xlabel(\"$p_{pass}\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            plt.hist(numpy.array(pfz).flatten(), alpha=0.5)\n",
    "            plt.title(\"pfz\")\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "        print(tw, a1, a0)\n",
    "        mn = numpy.mean(pfz)\n",
    "        vmn = numpy.mean(vpfz)\n",
    "        print(mn, vmn)\n",
    "\n",
    "        distro_params.append((vmn,agt))\n",
    "        pfz_list.append(pfz)\n",
    "        probs_list.append(probs.flatten())\n",
    "        \n",
    "        i+=1\n",
    "    balance_lookup[bal] = (probs_list, pfz_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(numpy.array(probs_list).flatten())\n",
    "# plt.title(\"Counts of pass probabilties (Raw params, 100 trials)\")\n",
    "# plt.xlabel(\"$p_{pass}$\")\n",
    "# plt.ylabel(\"Count\")\n",
    "\n",
    "bal_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "for bal in bal_list:\n",
    "    (probs_list, pfz_list) = balance_lookup[bal]\n",
    "    mnz = [numpy.mean(pfz) for pfz in pfz_list]\n",
    "    print(bal, numpy.mean(mnz))\n",
    "#     print(mnz)\n",
    "    \n",
    "    #print(numpy.mean(pfz_list))\n",
    "    print (numpy.sqrt(numpy.mean(numpy.power(numpy.subtract(mnz,bal), 2))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnz2, agtz2 = zip(*distro_params)\n",
    "print(mnz2)\n",
    "mnz2 = numpy.array(mnz2).flatten()\n",
    "print (numpy.mean(numpy.power(mnz2 - 0.5, 2)))\n",
    "plt.hist((mnz2 - 0.5))\n",
    "plt.show()\n",
    "print (numpy.std((mnz2 - 0.5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnz, agtz = zip(*distro_params)\n",
    "plt.xlim(0,1)\n",
    "plt.hist(ub_mnz, label=\"Raw\", alpha=0.3)\n",
    "plt.hist(hd_mnz, label=\"Balanced (Grid search)\", alpha=0.3)\n",
    "plt.hist(mnz, label=\"Balanced (MLP)\", alpha=1)\n",
    "plt.title(\"Raw vs Balanced generated data (100 trials)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"Mean pass-rate of generated data, $\\overline{p_{exp}}$\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(agtz)\n",
    "plt.show()\n",
    "\n",
    "summary_mnz = []\n",
    "for pfz in pfz_list:\n",
    "    mn = numpy.mean(pfz)\n",
    "    summary_mnz.append(mn)\n",
    "    plt.hist(pfz, alpha=.4)\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean of means:\", numpy.mean(summary_mnz))\n",
    "print(\"Std of means:\", numpy.std(summary_mnz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "def report(n_factors, min_active, max_active, emb_w, nn_mode, loss_mode, sws_list, qws_list, model_list, real_stu_list, real_que_list, test_datasets, params_list, spars_list):\n",
    "    \n",
    "    tot_sqerr = 0\n",
    "    mean_err_list = []\n",
    "    mean_std_list = []\n",
    "    mean_hit_list = []\n",
    "    \n",
    "    print(\"*****\")\n",
    "    print(nn_mode, loss_mode)\n",
    "#     print(\"*****\")\n",
    "    print(len(sws_list), len(qws_list), len(model_list), len(real_stu_list), len(real_que_list), len(test_datasets), len(params_list))\n",
    "    \n",
    "    for sw,qw,m,stz,qnz,tt_pairs, params, spars in zip(sws_list, qws_list, model_list, real_stu_list, real_que_list, test_datasets, params_list, spars_list):\n",
    "        tw,a1,a0,trbal,vbal,agt = params\n",
    "        \n",
    "        print(\"params:\", n_factors, min_active, max_active, emb_w, \"/\", tw,a1,a0, \"(\", trbal,vbal,agt,\") [\",spars,\"]\")\n",
    "        \n",
    "        err_list = []\n",
    "        true_err_list = []\n",
    "        hit_list = []\n",
    "    #     for six,qix in numpy.sort(tt_pairs, axis=0):\n",
    "    \n",
    "        true_pz = []\n",
    "        pred_pz = []\n",
    "        for six, qix in tt_pairs:\n",
    "    #         print(six, qix)\n",
    "    #     print(\"\\n------\\n\")\n",
    "    #     continue\n",
    "    #     if False:\n",
    "            tq = qnz[qix,:]\n",
    "            ts = stz[six,:]\n",
    "            qrow = qw[qix, :]\n",
    "            srow = sw[six, :]\n",
    "#             print(qrow)\n",
    "    #         print(\"raw\",tq,ts)\n",
    "    #         print(\"dif\",ts-tq)\n",
    "    #         print(numpy.prod(logistic(ts-tq,1,0)))\n",
    "            if rasch:\n",
    "                true_p = float(calc_probs_from_embs(ts.reshape(1,-1),tq.reshape(1,-1)))\n",
    "#                 dif = ts-tq\n",
    "#                 true_ps = 1.0 / (1.0 + numpy.exp(-dif))\n",
    "#                 true_p = numpy.prod(true_ps)\n",
    "            else:\n",
    "                true_p = numpy.prod((1-tq)+(ts*tq))\n",
    "            pred_p = m.predict([[qix],[six]])\n",
    "            true_pz.append(float(true_p))\n",
    "            pred_pz.append(float(pred_p))\n",
    "    #         pred_p = random.random()\n",
    "    \n",
    "            mae = numpy.abs(true_p - pred_p)\n",
    "#             print(true_p, float(pred_p), \"err:\", float(mae))\n",
    "\n",
    "            err = true_p - pred_p\n",
    "\n",
    "            true_err_list.append(err)\n",
    "            err_list.append(mae)\n",
    "            good_guess = int(numpy.round(true_p))==int(numpy.round(pred_p))\n",
    "            hit_list.append(int(good_guess))\n",
    "    #         sqerr = numpy.power(true_p - pred_p, 2)\n",
    "\n",
    "#             print(six, qix, \":\", srow, qrow)\n",
    "#             print(\"-->\", pred_p, true_p, \" ... \", good_guess)\n",
    "\n",
    "        print(\"R2 = \", r2_score(true_pz, pred_pz))\n",
    "        print(\"MAE = \", mean_absolute_error(true_pz, pred_pz))\n",
    "        numpy.set_printoptions(precision=3)\n",
    "    #     print(\"Mean sq err {}:\".format(qrow.shape), numpy.sqrt(numpy.mean(err_list)))\n",
    "    \n",
    "        plt.hist(true_pz, alpha=0.5)\n",
    "        plt.hist(pred_pz, alpha=0.5)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.hist(numpy.array(true_err_list).flatten(), alpha=0.5)\n",
    "        plt.show()\n",
    "        \n",
    "        mean_err_list.append(numpy.mean(err_list))\n",
    "        mean_std_list.append(numpy.std(err_list))\n",
    "        mean_hit_list.append(numpy.mean(hit_list))\n",
    "    #     print(sum(hit_list), len(hit_list), sum(hit_list)/len(hit_list))\n",
    "\n",
    "    # print(mean_err_list)\n",
    "    # print(mean_std_list)\n",
    "    # print(mean_hit_list)\n",
    "    # print(params_list)\n",
    "    print(len(stz),\"x\",len(qnz))\n",
    "#     for e,s,acc,params in zip(mean_err_list, mean_std_list, mean_hit_list, params_list):\n",
    "#         print(\"acc=\",acc)\n",
    "#         print(\"mae=\",e,\"sig=\",s)\n",
    "#         print(params)\n",
    "#     print(\"aggregated:\")\n",
    "    print(numpy.median(mean_hit_list), numpy.std(mean_hit_list), \"/\", numpy.median(mean_err_list), numpy.median(mean_std_list))\n",
    "    \n",
    "# report(n_factors, min_active, max_active, emb_w, nn_mode, loss_mode, sws_list, qws_list, model_list, real_stu_list, real_que_list, test_datasets, params_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 students, 200 questions\n",
      "using datacache\n",
      "run created\n",
      "pred list shape (0,)\n",
      "real items shape (200, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/auto/homes/rjm49/.venvs/isaac36/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1116: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input=overwrite_input)\n",
      "/auto/homes/rjm49/.venvs/isaac36/lib/python3.6/site-packages/ipykernel_launcher.py:125: RuntimeWarning: invalid value encountered in true_divide\n",
      "/auto/homes/rjm49/.venvs/isaac36/lib/python3.6/site-packages/ipykernel_launcher.py:126: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7c9e29d30>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len Cs 0\n",
      "Mean rand score = nan nan\n",
      "~ ~ ~ ~~ ATTEMPT 0 [[2.079]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/auto/homes/rjm49/.venvs/isaac36/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/auto/homes/rjm49/.venvs/isaac36/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/auto/homes/rjm49/.venvs/isaac36/lib/python3.6/site-packages/numpy/core/_methods.py:217: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/auto/homes/rjm49/.venvs/isaac36/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/auto/homes/rjm49/.venvs/isaac36/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7c85fce10>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting\n",
      "splut\n",
      "[0.676 0.107 0.949 ... 0.981 0.777 0.4  ]\n",
      "[0.676 0.107 0.949 ... 0.981 0.777 0.4  ]\n",
      "[0 0 0 ... 0 0 0]\n",
      "0 correctly labelled out of 1791000 %= 0.0\n",
      "[0.308 0.914 0.153 ... 0.166 0.574 0.871]\n",
      "[0.308 0.914 0.153 ... 0.166 0.574 0.871]\n",
      "[0 0 0 ... 0 0 0]\n",
      "0 correctly labelled out of 199000 %= 0.0\n",
      "Sparsity 1.0 lens of pfz and vpfz, tt_pairs 1791000 199000 10000\n",
      "[[0.454 0.803 0.593 ... 0.094 0.886 0.634]\n",
      " [0.007 0.886 0.083 ... 0.063 0.317 0.709]\n",
      " [0.527 0.804 0.035 ... 0.619 0.835 0.798]\n",
      " ...\n",
      " [0.465 0.534 0.387 ... 0.02  0.373 0.241]\n",
      " [0.26  0.421 0.159 ... 0.157 0.735 0.7  ]\n",
      " [0.891 0.486 0.2   ... 0.724 0.994 0.898]]\n",
      "*** AGT: 0.7911066499999999\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7c8ba7940>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7c92dd320>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5636739213509128 4.2794227824212765 [[2.079]]\n",
      "0.516223827849955 0.5157171235946164\n",
      "xhxwx ELIGIBLE SPREAD for 0.516223827849955 0.5 0.55 0.45\n",
      "FOUND ELIGIBLE SPREAD for 0.516223827849955 0.5 0.55 0.45\n",
      "mean pers is nan\n",
      "binary obs? False\n",
      "callio:\n",
      "1791000 1791000 1791000\n",
      "199000 199000 199000 128\n",
      "nn_mode DEEP\n",
      "kk (200, 128)\n",
      "selector shape (None, 1)\n",
      "flat selector shape (None,)\n",
      "call kk (200, 128)\n",
      "'rows' shape, (None, 128)\n",
      "kk (10000, 128)\n",
      "selector shape (None, 1)\n",
      "flat selector shape (None,)\n",
      "call kk (10000, 128)\n",
      "'rows' shape, (None, 128)\n",
      "Mode is DEEP\n",
      "loss mode is XENT\n",
      "Model: \"model_23\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "psi_select (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "q_select (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "big_table_45 (BigTable)         (None, 128)          1280000     psi_select[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "big_table_46 (BigTable)         (None, 128)          25600       q_select[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "subtract_5 (Subtract)           (None, 128)          0           big_table_45[0][0]               \n",
      "                                                                 big_table_46[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_71 (Dense)                (None, 64)           8256        subtract_5[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_72 (Dense)                (None, 1)            65          dense_71[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,313,921\n",
      "Trainable params: 1,313,921\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "PRE-TR AVG  =  0.22986376326074817\n",
      "PRE-TR VAVG =  0.229964824120603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/auto/homes/rjm49/.venvs/isaac36/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1791000 samples, validate on 199000 samples\n",
      "Epoch 1/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6962 - accuracy: 0.0000e+00 - val_loss: 0.6946 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6947 - accuracy: 0.0000e+00 - val_loss: 0.6926 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6926 - accuracy: 0.0000e+00 - val_loss: 0.6908 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6908 - accuracy: 0.0000e+00 - val_loss: 0.6890 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6890 - accuracy: 0.0000e+00 - val_loss: 0.6872 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6872 - accuracy: 0.0000e+00 - val_loss: 0.6854 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6853 - accuracy: 0.0000e+00 - val_loss: 0.6836 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6835 - accuracy: 0.0000e+00 - val_loss: 0.6818 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6817 - accuracy: 0.0000e+00 - val_loss: 0.6800 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6798 - accuracy: 0.0000e+00 - val_loss: 0.6781 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6779 - accuracy: 0.0000e+00 - val_loss: 0.6761 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6759 - accuracy: 0.0000e+00 - val_loss: 0.6741 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6738 - accuracy: 0.0000e+00 - val_loss: 0.6720 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6716 - accuracy: 0.0000e+00 - val_loss: 0.6698 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6694 - accuracy: 0.0000e+00 - val_loss: 0.6675 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6671 - accuracy: 0.0000e+00 - val_loss: 0.6652 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6647 - accuracy: 0.0000e+00 - val_loss: 0.6627 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6622 - accuracy: 0.0000e+00 - val_loss: 0.6602 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6596 - accuracy: 0.0000e+00 - val_loss: 0.6576 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6570 - accuracy: 0.0000e+00 - val_loss: 0.6550 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6543 - accuracy: 0.0000e+00 - val_loss: 0.6523 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6516 - accuracy: 0.0000e+00 - val_loss: 0.6497 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6489 - accuracy: 0.0000e+00 - val_loss: 0.6470 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6462 - accuracy: 0.0000e+00 - val_loss: 0.6444 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6435 - accuracy: 0.0000e+00 - val_loss: 0.6419 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6409 - accuracy: 0.0000e+00 - val_loss: 0.6394 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6384 - accuracy: 0.0000e+00 - val_loss: 0.6371 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6360 - accuracy: 0.0000e+00 - val_loss: 0.6349 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6338 - accuracy: 0.0000e+00 - val_loss: 0.6329 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6317 - accuracy: 0.0000e+00 - val_loss: 0.6311 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6298 - accuracy: 0.0000e+00 - val_loss: 0.6295 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6281 - accuracy: 0.0000e+00 - val_loss: 0.6281 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6267 - accuracy: 0.0000e+00 - val_loss: 0.6269 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6254 - accuracy: 0.0000e+00 - val_loss: 0.6259 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6243 - accuracy: 0.0000e+00 - val_loss: 0.6250 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6234 - accuracy: 0.0000e+00 - val_loss: 0.6244 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6226 - accuracy: 0.0000e+00 - val_loss: 0.6238 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6220 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6215 - accuracy: 0.0000e+00 - val_loss: 0.6231 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6211 - accuracy: 0.0000e+00 - val_loss: 0.6228 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6208 - accuracy: 0.0000e+00 - val_loss: 0.6226 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6205 - accuracy: 0.0000e+00 - val_loss: 0.6224 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6202 - accuracy: 0.0000e+00 - val_loss: 0.6222 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6199 - accuracy: 0.0000e+00 - val_loss: 0.6220 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6197 - accuracy: 0.0000e+00 - val_loss: 0.6217 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6194 - accuracy: 0.0000e+00 - val_loss: 0.6215 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6192 - accuracy: 0.0000e+00 - val_loss: 0.6213 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6189 - accuracy: 0.0000e+00 - val_loss: 0.6210 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6187 - accuracy: 0.0000e+00 - val_loss: 0.6208 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6184 - accuracy: 0.0000e+00 - val_loss: 0.6206 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6182 - accuracy: 0.0000e+00 - val_loss: 0.6204 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6180 - accuracy: 0.0000e+00 - val_loss: 0.6202 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6178 - accuracy: 0.0000e+00 - val_loss: 0.6200 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6177 - accuracy: 0.0000e+00 - val_loss: 0.6199 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6175 - accuracy: 0.0000e+00 - val_loss: 0.6198 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6174 - accuracy: 0.0000e+00 - val_loss: 0.6197 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6173 - accuracy: 0.0000e+00 - val_loss: 0.6196 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6172 - accuracy: 0.0000e+00 - val_loss: 0.6196 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6172 - accuracy: 0.0000e+00 - val_loss: 0.6195 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6171 - accuracy: 0.0000e+00 - val_loss: 0.6195 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6170 - accuracy: 0.0000e+00 - val_loss: 0.6195 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6170 - accuracy: 0.0000e+00 - val_loss: 0.6194 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6169 - accuracy: 0.0000e+00 - val_loss: 0.6194 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6169 - accuracy: 0.0000e+00 - val_loss: 0.6194 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6168 - accuracy: 0.0000e+00 - val_loss: 0.6194 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6167 - accuracy: 0.0000e+00 - val_loss: 0.6194 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6167 - accuracy: 0.0000e+00 - val_loss: 0.6193 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6166 - accuracy: 0.0000e+00 - val_loss: 0.6193 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6166 - accuracy: 0.0000e+00 - val_loss: 0.6193 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6165 - accuracy: 0.0000e+00 - val_loss: 0.6193 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6164 - accuracy: 0.0000e+00 - val_loss: 0.6193 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6164 - accuracy: 0.0000e+00 - val_loss: 0.6193 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6163 - accuracy: 0.0000e+00 - val_loss: 0.6193 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6163 - accuracy: 0.0000e+00 - val_loss: 0.6192 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6162 - accuracy: 0.0000e+00 - val_loss: 0.6192 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6162 - accuracy: 0.0000e+00 - val_loss: 0.6192 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6161 - accuracy: 0.0000e+00 - val_loss: 0.6192 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6161 - accuracy: 0.0000e+00 - val_loss: 0.6192 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6160 - accuracy: 0.0000e+00 - val_loss: 0.6192 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6160 - accuracy: 0.0000e+00 - val_loss: 0.6192 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6159 - accuracy: 0.0000e+00 - val_loss: 0.6192 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6159 - accuracy: 0.0000e+00 - val_loss: 0.6192 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6158 - accuracy: 0.0000e+00 - val_loss: 0.6192 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6158 - accuracy: 0.0000e+00 - val_loss: 0.6192 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6157 - accuracy: 0.0000e+00 - val_loss: 0.6192 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6157 - accuracy: 0.0000e+00 - val_loss: 0.6192 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6156 - accuracy: 0.0000e+00 - val_loss: 0.6192 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6156 - accuracy: 0.0000e+00 - val_loss: 0.6191 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6155 - accuracy: 0.0000e+00 - val_loss: 0.6191 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6155 - accuracy: 0.0000e+00 - val_loss: 0.6191 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6154 - accuracy: 0.0000e+00 - val_loss: 0.6191 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6154 - accuracy: 0.0000e+00 - val_loss: 0.6191 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6154 - accuracy: 0.0000e+00 - val_loss: 0.6191 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6153 - accuracy: 0.0000e+00 - val_loss: 0.6191 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6153 - accuracy: 0.0000e+00 - val_loss: 0.6191 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6152 - accuracy: 0.0000e+00 - val_loss: 0.6191 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6152 - accuracy: 0.0000e+00 - val_loss: 0.6191 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6151 - accuracy: 0.0000e+00 - val_loss: 0.6191 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6151 - accuracy: 0.0000e+00 - val_loss: 0.6191 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6150 - accuracy: 0.0000e+00 - val_loss: 0.6190 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6150 - accuracy: 0.0000e+00 - val_loss: 0.6190 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6149 - accuracy: 0.0000e+00 - val_loss: 0.6190 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6149 - accuracy: 0.0000e+00 - val_loss: 0.6190 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6149 - accuracy: 0.0000e+00 - val_loss: 0.6190 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6148 - accuracy: 0.0000e+00 - val_loss: 0.6190 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6148 - accuracy: 0.0000e+00 - val_loss: 0.6190 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6147 - accuracy: 0.0000e+00 - val_loss: 0.6190 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6147 - accuracy: 0.0000e+00 - val_loss: 0.6190 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6146 - accuracy: 0.0000e+00 - val_loss: 0.6190 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6146 - accuracy: 0.0000e+00 - val_loss: 0.6190 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6145 - accuracy: 0.0000e+00 - val_loss: 0.6190 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6145 - accuracy: 0.0000e+00 - val_loss: 0.6190 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6144 - accuracy: 0.0000e+00 - val_loss: 0.6190 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6144 - accuracy: 0.0000e+00 - val_loss: 0.6189 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6143 - accuracy: 0.0000e+00 - val_loss: 0.6189 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6143 - accuracy: 0.0000e+00 - val_loss: 0.6189 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6142 - accuracy: 0.0000e+00 - val_loss: 0.6189 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6142 - accuracy: 0.0000e+00 - val_loss: 0.6189 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6141 - accuracy: 0.0000e+00 - val_loss: 0.6189 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6141 - accuracy: 0.0000e+00 - val_loss: 0.6189 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6140 - accuracy: 0.0000e+00 - val_loss: 0.6189 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6140 - accuracy: 0.0000e+00 - val_loss: 0.6188 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6139 - accuracy: 0.0000e+00 - val_loss: 0.6188 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6139 - accuracy: 0.0000e+00 - val_loss: 0.6188 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6138 - accuracy: 0.0000e+00 - val_loss: 0.6188 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6138 - accuracy: 0.0000e+00 - val_loss: 0.6188 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6137 - accuracy: 0.0000e+00 - val_loss: 0.6188 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6136 - accuracy: 0.0000e+00 - val_loss: 0.6187 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6136 - accuracy: 0.0000e+00 - val_loss: 0.6187 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6135 - accuracy: 0.0000e+00 - val_loss: 0.6187 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6135 - accuracy: 0.0000e+00 - val_loss: 0.6187 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6134 - accuracy: 0.0000e+00 - val_loss: 0.6187 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6134 - accuracy: 0.0000e+00 - val_loss: 0.6186 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6133 - accuracy: 0.0000e+00 - val_loss: 0.6186 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6132 - accuracy: 0.0000e+00 - val_loss: 0.6186 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6132 - accuracy: 0.0000e+00 - val_loss: 0.6186 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6131 - accuracy: 0.0000e+00 - val_loss: 0.6186 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6130 - accuracy: 0.0000e+00 - val_loss: 0.6185 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6130 - accuracy: 0.0000e+00 - val_loss: 0.6185 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6129 - accuracy: 0.0000e+00 - val_loss: 0.6185 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6129 - accuracy: 0.0000e+00 - val_loss: 0.6185 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6128 - accuracy: 0.0000e+00 - val_loss: 0.6184 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6127 - accuracy: 0.0000e+00 - val_loss: 0.6184 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6127 - accuracy: 0.0000e+00 - val_loss: 0.6184 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6126 - accuracy: 0.0000e+00 - val_loss: 0.6184 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6125 - accuracy: 0.0000e+00 - val_loss: 0.6183 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6125 - accuracy: 0.0000e+00 - val_loss: 0.6183 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6124 - accuracy: 0.0000e+00 - val_loss: 0.6183 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6123 - accuracy: 0.0000e+00 - val_loss: 0.6182 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6122 - accuracy: 0.0000e+00 - val_loss: 0.6182 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6122 - accuracy: 0.0000e+00 - val_loss: 0.6182 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6121 - accuracy: 0.0000e+00 - val_loss: 0.6181 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6120 - accuracy: 0.0000e+00 - val_loss: 0.6181 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6119 - accuracy: 0.0000e+00 - val_loss: 0.6181 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6119 - accuracy: 0.0000e+00 - val_loss: 0.6180 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6118 - accuracy: 0.0000e+00 - val_loss: 0.6180 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6117 - accuracy: 0.0000e+00 - val_loss: 0.6180 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6116 - accuracy: 0.0000e+00 - val_loss: 0.6179 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6115 - accuracy: 0.0000e+00 - val_loss: 0.6179 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6114 - accuracy: 0.0000e+00 - val_loss: 0.6179 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6114 - accuracy: 0.0000e+00 - val_loss: 0.6178 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6113 - accuracy: 0.0000e+00 - val_loss: 0.6178 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6112 - accuracy: 0.0000e+00 - val_loss: 0.6177 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6111 - accuracy: 0.0000e+00 - val_loss: 0.6177 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6110 - accuracy: 0.0000e+00 - val_loss: 0.6177 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6109 - accuracy: 0.0000e+00 - val_loss: 0.6176 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6108 - accuracy: 0.0000e+00 - val_loss: 0.6176 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6107 - accuracy: 0.0000e+00 - val_loss: 0.6175 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6106 - accuracy: 0.0000e+00 - val_loss: 0.6175 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6105 - accuracy: 0.0000e+00 - val_loss: 0.6174 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6104 - accuracy: 0.0000e+00 - val_loss: 0.6174 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6103 - accuracy: 0.0000e+00 - val_loss: 0.6173 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6102 - accuracy: 0.0000e+00 - val_loss: 0.6173 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6101 - accuracy: 0.0000e+00 - val_loss: 0.6172 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6100 - accuracy: 0.0000e+00 - val_loss: 0.6172 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6099 - accuracy: 0.0000e+00 - val_loss: 0.6171 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6098 - accuracy: 0.0000e+00 - val_loss: 0.6171 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6097 - accuracy: 0.0000e+00 - val_loss: 0.6170 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6096 - accuracy: 0.0000e+00 - val_loss: 0.6170 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6095 - accuracy: 0.0000e+00 - val_loss: 0.6169 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6093 - accuracy: 0.0000e+00 - val_loss: 0.6169 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6092 - accuracy: 0.0000e+00 - val_loss: 0.6168 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6091 - accuracy: 0.0000e+00 - val_loss: 0.6168 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6090 - accuracy: 0.0000e+00 - val_loss: 0.6167 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6089 - accuracy: 0.0000e+00 - val_loss: 0.6166 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6087 - accuracy: 0.0000e+00 - val_loss: 0.6166 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6086 - accuracy: 0.0000e+00 - val_loss: 0.6165 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6085 - accuracy: 0.0000e+00 - val_loss: 0.6164 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6084 - accuracy: 0.0000e+00 - val_loss: 0.6164 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6082 - accuracy: 0.0000e+00 - val_loss: 0.6163 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6081 - accuracy: 0.0000e+00 - val_loss: 0.6163 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6080 - accuracy: 0.0000e+00 - val_loss: 0.6162 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6078 - accuracy: 0.0000e+00 - val_loss: 0.6161 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6077 - accuracy: 0.0000e+00 - val_loss: 0.6160 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6076 - accuracy: 0.0000e+00 - val_loss: 0.6160 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6074 - accuracy: 0.0000e+00 - val_loss: 0.6159 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6073 - accuracy: 0.0000e+00 - val_loss: 0.6158 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6071 - accuracy: 0.0000e+00 - val_loss: 0.6158 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6070 - accuracy: 0.0000e+00 - val_loss: 0.6157 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6069 - accuracy: 0.0000e+00 - val_loss: 0.6156 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6067 - accuracy: 0.0000e+00 - val_loss: 0.6155 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6066 - accuracy: 0.0000e+00 - val_loss: 0.6155 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6064 - accuracy: 0.0000e+00 - val_loss: 0.6154 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6063 - accuracy: 0.0000e+00 - val_loss: 0.6153 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 205/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6061 - accuracy: 0.0000e+00 - val_loss: 0.6152 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6059 - accuracy: 0.0000e+00 - val_loss: 0.6152 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6058 - accuracy: 0.0000e+00 - val_loss: 0.6151 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6056 - accuracy: 0.0000e+00 - val_loss: 0.6150 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6055 - accuracy: 0.0000e+00 - val_loss: 0.6149 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6053 - accuracy: 0.0000e+00 - val_loss: 0.6148 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6051 - accuracy: 0.0000e+00 - val_loss: 0.6148 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6050 - accuracy: 0.0000e+00 - val_loss: 0.6147 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6048 - accuracy: 0.0000e+00 - val_loss: 0.6146 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6046 - accuracy: 0.0000e+00 - val_loss: 0.6145 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6044 - accuracy: 0.0000e+00 - val_loss: 0.6144 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6043 - accuracy: 0.0000e+00 - val_loss: 0.6143 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6041 - accuracy: 0.0000e+00 - val_loss: 0.6143 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6039 - accuracy: 0.0000e+00 - val_loss: 0.6142 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6037 - accuracy: 0.0000e+00 - val_loss: 0.6141 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6036 - accuracy: 0.0000e+00 - val_loss: 0.6140 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6034 - accuracy: 0.0000e+00 - val_loss: 0.6139 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6032 - accuracy: 0.0000e+00 - val_loss: 0.6138 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6030 - accuracy: 0.0000e+00 - val_loss: 0.6137 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6028 - accuracy: 0.0000e+00 - val_loss: 0.6136 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6026 - accuracy: 0.0000e+00 - val_loss: 0.6135 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6024 - accuracy: 0.0000e+00 - val_loss: 0.6134 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6022 - accuracy: 0.0000e+00 - val_loss: 0.6134 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6020 - accuracy: 0.0000e+00 - val_loss: 0.6133 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6019 - accuracy: 0.0000e+00 - val_loss: 0.6132 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6017 - accuracy: 0.0000e+00 - val_loss: 0.6131 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6015 - accuracy: 0.0000e+00 - val_loss: 0.6130 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6013 - accuracy: 0.0000e+00 - val_loss: 0.6129 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6011 - accuracy: 0.0000e+00 - val_loss: 0.6128 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6008 - accuracy: 0.0000e+00 - val_loss: 0.6127 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6006 - accuracy: 0.0000e+00 - val_loss: 0.6126 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6004 - accuracy: 0.0000e+00 - val_loss: 0.6125 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6002 - accuracy: 0.0000e+00 - val_loss: 0.6124 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.6000 - accuracy: 0.0000e+00 - val_loss: 0.6123 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5998 - accuracy: 0.0000e+00 - val_loss: 0.6122 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5996 - accuracy: 0.0000e+00 - val_loss: 0.6121 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5994 - accuracy: 0.0000e+00 - val_loss: 0.6120 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5992 - accuracy: 0.0000e+00 - val_loss: 0.6119 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5989 - accuracy: 0.0000e+00 - val_loss: 0.6118 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5987 - accuracy: 0.0000e+00 - val_loss: 0.6117 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5985 - accuracy: 0.0000e+00 - val_loss: 0.6116 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5983 - accuracy: 0.0000e+00 - val_loss: 0.6115 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5981 - accuracy: 0.0000e+00 - val_loss: 0.6114 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5978 - accuracy: 0.0000e+00 - val_loss: 0.6113 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5976 - accuracy: 0.0000e+00 - val_loss: 0.6112 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5974 - accuracy: 0.0000e+00 - val_loss: 0.6111 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5972 - accuracy: 0.0000e+00 - val_loss: 0.6110 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5969 - accuracy: 0.0000e+00 - val_loss: 0.6109 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5967 - accuracy: 0.0000e+00 - val_loss: 0.6108 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5965 - accuracy: 0.0000e+00 - val_loss: 0.6107 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5962 - accuracy: 0.0000e+00 - val_loss: 0.6106 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5960 - accuracy: 0.0000e+00 - val_loss: 0.6105 - val_accuracy: 0.0000e+00\n",
      "Epoch 257/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5958 - accuracy: 0.0000e+00 - val_loss: 0.6104 - val_accuracy: 0.0000e+00\n",
      "Epoch 258/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5955 - accuracy: 0.0000e+00 - val_loss: 0.6103 - val_accuracy: 0.0000e+00\n",
      "Epoch 259/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5953 - accuracy: 0.0000e+00 - val_loss: 0.6102 - val_accuracy: 0.0000e+00\n",
      "Epoch 260/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5950 - accuracy: 0.0000e+00 - val_loss: 0.6101 - val_accuracy: 0.0000e+00\n",
      "Epoch 261/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5948 - accuracy: 0.0000e+00 - val_loss: 0.6100 - val_accuracy: 0.0000e+00\n",
      "Epoch 262/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5945 - accuracy: 0.0000e+00 - val_loss: 0.6099 - val_accuracy: 0.0000e+00\n",
      "Epoch 263/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5943 - accuracy: 0.0000e+00 - val_loss: 0.6097 - val_accuracy: 0.0000e+00\n",
      "Epoch 264/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5941 - accuracy: 0.0000e+00 - val_loss: 0.6096 - val_accuracy: 0.0000e+00\n",
      "Epoch 265/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5938 - accuracy: 0.0000e+00 - val_loss: 0.6095 - val_accuracy: 0.0000e+00\n",
      "Epoch 266/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5936 - accuracy: 0.0000e+00 - val_loss: 0.6094 - val_accuracy: 0.0000e+00\n",
      "Epoch 267/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5933 - accuracy: 0.0000e+00 - val_loss: 0.6093 - val_accuracy: 0.0000e+00\n",
      "Epoch 268/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5931 - accuracy: 0.0000e+00 - val_loss: 0.6092 - val_accuracy: 0.0000e+00\n",
      "Epoch 269/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5928 - accuracy: 0.0000e+00 - val_loss: 0.6091 - val_accuracy: 0.0000e+00\n",
      "Epoch 270/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5925 - accuracy: 0.0000e+00 - val_loss: 0.6090 - val_accuracy: 0.0000e+00\n",
      "Epoch 271/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5923 - accuracy: 0.0000e+00 - val_loss: 0.6089 - val_accuracy: 0.0000e+00\n",
      "Epoch 272/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5920 - accuracy: 0.0000e+00 - val_loss: 0.6088 - val_accuracy: 0.0000e+00\n",
      "Epoch 273/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5918 - accuracy: 0.0000e+00 - val_loss: 0.6086 - val_accuracy: 0.0000e+00\n",
      "Epoch 274/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5915 - accuracy: 0.0000e+00 - val_loss: 0.6085 - val_accuracy: 0.0000e+00\n",
      "Epoch 275/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5912 - accuracy: 0.0000e+00 - val_loss: 0.6084 - val_accuracy: 0.0000e+00\n",
      "Epoch 276/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5910 - accuracy: 0.0000e+00 - val_loss: 0.6083 - val_accuracy: 0.0000e+00\n",
      "Epoch 277/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5907 - accuracy: 0.0000e+00 - val_loss: 0.6082 - val_accuracy: 0.0000e+00\n",
      "Epoch 278/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5904 - accuracy: 0.0000e+00 - val_loss: 0.6081 - val_accuracy: 0.0000e+00\n",
      "Epoch 279/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5902 - accuracy: 0.0000e+00 - val_loss: 0.6080 - val_accuracy: 0.0000e+00\n",
      "Epoch 280/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5899 - accuracy: 0.0000e+00 - val_loss: 0.6079 - val_accuracy: 0.0000e+00\n",
      "Epoch 281/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5896 - accuracy: 0.0000e+00 - val_loss: 0.6077 - val_accuracy: 0.0000e+00\n",
      "Epoch 282/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5894 - accuracy: 0.0000e+00 - val_loss: 0.6076 - val_accuracy: 0.0000e+00\n",
      "Epoch 283/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5891 - accuracy: 0.0000e+00 - val_loss: 0.6075 - val_accuracy: 0.0000e+00\n",
      "Epoch 284/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5888 - accuracy: 0.0000e+00 - val_loss: 0.6074 - val_accuracy: 0.0000e+00\n",
      "Epoch 285/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5885 - accuracy: 0.0000e+00 - val_loss: 0.6073 - val_accuracy: 0.0000e+00\n",
      "Epoch 286/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5883 - accuracy: 0.0000e+00 - val_loss: 0.6072 - val_accuracy: 0.0000e+00\n",
      "Epoch 287/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5880 - accuracy: 0.0000e+00 - val_loss: 0.6070 - val_accuracy: 0.0000e+00\n",
      "Epoch 288/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5877 - accuracy: 0.0000e+00 - val_loss: 0.6069 - val_accuracy: 0.0000e+00\n",
      "Epoch 289/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5874 - accuracy: 0.0000e+00 - val_loss: 0.6068 - val_accuracy: 0.0000e+00\n",
      "Epoch 290/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5872 - accuracy: 0.0000e+00 - val_loss: 0.6067 - val_accuracy: 0.0000e+00\n",
      "Epoch 291/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5869 - accuracy: 0.0000e+00 - val_loss: 0.6066 - val_accuracy: 0.0000e+00\n",
      "Epoch 292/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5866 - accuracy: 0.0000e+00 - val_loss: 0.6064 - val_accuracy: 0.0000e+00\n",
      "Epoch 293/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5863 - accuracy: 0.0000e+00 - val_loss: 0.6063 - val_accuracy: 0.0000e+00\n",
      "Epoch 294/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5860 - accuracy: 0.0000e+00 - val_loss: 0.6062 - val_accuracy: 0.0000e+00\n",
      "Epoch 295/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5857 - accuracy: 0.0000e+00 - val_loss: 0.6061 - val_accuracy: 0.0000e+00\n",
      "Epoch 296/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5854 - accuracy: 0.0000e+00 - val_loss: 0.6060 - val_accuracy: 0.0000e+00\n",
      "Epoch 297/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5852 - accuracy: 0.0000e+00 - val_loss: 0.6058 - val_accuracy: 0.0000e+00\n",
      "Epoch 298/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5849 - accuracy: 0.0000e+00 - val_loss: 0.6057 - val_accuracy: 0.0000e+00\n",
      "Epoch 299/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5846 - accuracy: 0.0000e+00 - val_loss: 0.6056 - val_accuracy: 0.0000e+00\n",
      "Epoch 300/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5843 - accuracy: 0.0000e+00 - val_loss: 0.6055 - val_accuracy: 0.0000e+00\n",
      "Epoch 301/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5840 - accuracy: 0.0000e+00 - val_loss: 0.6054 - val_accuracy: 0.0000e+00\n",
      "Epoch 302/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5837 - accuracy: 0.0000e+00 - val_loss: 0.6052 - val_accuracy: 0.0000e+00\n",
      "Epoch 303/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5834 - accuracy: 0.0000e+00 - val_loss: 0.6051 - val_accuracy: 0.0000e+00\n",
      "Epoch 304/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5831 - accuracy: 0.0000e+00 - val_loss: 0.6050 - val_accuracy: 0.0000e+00\n",
      "Epoch 305/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5828 - accuracy: 0.0000e+00 - val_loss: 0.6049 - val_accuracy: 0.0000e+00\n",
      "Epoch 306/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5825 - accuracy: 0.0000e+00 - val_loss: 0.6048 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 307/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5822 - accuracy: 0.0000e+00 - val_loss: 0.6046 - val_accuracy: 0.0000e+00\n",
      "Epoch 308/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5819 - accuracy: 0.0000e+00 - val_loss: 0.6045 - val_accuracy: 0.0000e+00\n",
      "Epoch 309/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5816 - accuracy: 0.0000e+00 - val_loss: 0.6044 - val_accuracy: 0.0000e+00\n",
      "Epoch 310/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5813 - accuracy: 0.0000e+00 - val_loss: 0.6043 - val_accuracy: 0.0000e+00\n",
      "Epoch 311/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5810 - accuracy: 0.0000e+00 - val_loss: 0.6042 - val_accuracy: 0.0000e+00\n",
      "Epoch 312/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5807 - accuracy: 0.0000e+00 - val_loss: 0.6040 - val_accuracy: 0.0000e+00\n",
      "Epoch 313/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5804 - accuracy: 0.0000e+00 - val_loss: 0.6039 - val_accuracy: 0.0000e+00\n",
      "Epoch 314/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5801 - accuracy: 0.0000e+00 - val_loss: 0.6038 - val_accuracy: 0.0000e+00\n",
      "Epoch 315/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5798 - accuracy: 0.0000e+00 - val_loss: 0.6037 - val_accuracy: 0.0000e+00\n",
      "Epoch 316/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5795 - accuracy: 0.0000e+00 - val_loss: 0.6035 - val_accuracy: 0.0000e+00\n",
      "Epoch 317/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5792 - accuracy: 0.0000e+00 - val_loss: 0.6034 - val_accuracy: 0.0000e+00\n",
      "Epoch 318/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5789 - accuracy: 0.0000e+00 - val_loss: 0.6033 - val_accuracy: 0.0000e+00\n",
      "Epoch 319/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5786 - accuracy: 0.0000e+00 - val_loss: 0.6032 - val_accuracy: 0.0000e+00\n",
      "Epoch 320/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5783 - accuracy: 0.0000e+00 - val_loss: 0.6031 - val_accuracy: 0.0000e+00\n",
      "Epoch 321/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5780 - accuracy: 0.0000e+00 - val_loss: 0.6029 - val_accuracy: 0.0000e+00\n",
      "Epoch 322/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5777 - accuracy: 0.0000e+00 - val_loss: 0.6028 - val_accuracy: 0.0000e+00\n",
      "Epoch 323/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5774 - accuracy: 0.0000e+00 - val_loss: 0.6027 - val_accuracy: 0.0000e+00\n",
      "Epoch 324/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5771 - accuracy: 0.0000e+00 - val_loss: 0.6026 - val_accuracy: 0.0000e+00\n",
      "Epoch 325/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5768 - accuracy: 0.0000e+00 - val_loss: 0.6025 - val_accuracy: 0.0000e+00\n",
      "Epoch 326/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5765 - accuracy: 0.0000e+00 - val_loss: 0.6024 - val_accuracy: 0.0000e+00\n",
      "Epoch 327/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5762 - accuracy: 0.0000e+00 - val_loss: 0.6022 - val_accuracy: 0.0000e+00\n",
      "Epoch 328/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5759 - accuracy: 0.0000e+00 - val_loss: 0.6021 - val_accuracy: 0.0000e+00\n",
      "Epoch 329/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5756 - accuracy: 0.0000e+00 - val_loss: 0.6020 - val_accuracy: 0.0000e+00\n",
      "Epoch 330/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5753 - accuracy: 0.0000e+00 - val_loss: 0.6019 - val_accuracy: 0.0000e+00\n",
      "Epoch 331/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5750 - accuracy: 0.0000e+00 - val_loss: 0.6018 - val_accuracy: 0.0000e+00\n",
      "Epoch 332/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5747 - accuracy: 0.0000e+00 - val_loss: 0.6016 - val_accuracy: 0.0000e+00\n",
      "Epoch 333/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5744 - accuracy: 0.0000e+00 - val_loss: 0.6015 - val_accuracy: 0.0000e+00\n",
      "Epoch 334/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5741 - accuracy: 0.0000e+00 - val_loss: 0.6014 - val_accuracy: 0.0000e+00\n",
      "Epoch 335/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5738 - accuracy: 0.0000e+00 - val_loss: 0.6013 - val_accuracy: 0.0000e+00\n",
      "Epoch 336/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5735 - accuracy: 0.0000e+00 - val_loss: 0.6012 - val_accuracy: 0.0000e+00\n",
      "Epoch 337/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5732 - accuracy: 0.0000e+00 - val_loss: 0.6011 - val_accuracy: 0.0000e+00\n",
      "Epoch 338/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5729 - accuracy: 0.0000e+00 - val_loss: 0.6009 - val_accuracy: 0.0000e+00\n",
      "Epoch 339/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5726 - accuracy: 0.0000e+00 - val_loss: 0.6008 - val_accuracy: 0.0000e+00\n",
      "Epoch 340/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5723 - accuracy: 0.0000e+00 - val_loss: 0.6007 - val_accuracy: 0.0000e+00\n",
      "Epoch 341/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5720 - accuracy: 0.0000e+00 - val_loss: 0.6006 - val_accuracy: 0.0000e+00\n",
      "Epoch 342/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5717 - accuracy: 0.0000e+00 - val_loss: 0.6005 - val_accuracy: 0.0000e+00\n",
      "Epoch 343/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5714 - accuracy: 0.0000e+00 - val_loss: 0.6004 - val_accuracy: 0.0000e+00\n",
      "Epoch 344/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5711 - accuracy: 0.0000e+00 - val_loss: 0.6002 - val_accuracy: 0.0000e+00\n",
      "Epoch 345/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5708 - accuracy: 0.0000e+00 - val_loss: 0.6001 - val_accuracy: 0.0000e+00\n",
      "Epoch 346/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5705 - accuracy: 0.0000e+00 - val_loss: 0.6000 - val_accuracy: 0.0000e+00\n",
      "Epoch 347/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5702 - accuracy: 0.0000e+00 - val_loss: 0.5999 - val_accuracy: 0.0000e+00\n",
      "Epoch 348/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5699 - accuracy: 0.0000e+00 - val_loss: 0.5998 - val_accuracy: 0.0000e+00\n",
      "Epoch 349/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5696 - accuracy: 0.0000e+00 - val_loss: 0.5997 - val_accuracy: 0.0000e+00\n",
      "Epoch 350/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5693 - accuracy: 0.0000e+00 - val_loss: 0.5995 - val_accuracy: 0.0000e+00\n",
      "Epoch 351/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5690 - accuracy: 0.0000e+00 - val_loss: 0.5994 - val_accuracy: 0.0000e+00\n",
      "Epoch 352/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5687 - accuracy: 0.0000e+00 - val_loss: 0.5993 - val_accuracy: 0.0000e+00\n",
      "Epoch 353/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5684 - accuracy: 0.0000e+00 - val_loss: 0.5992 - val_accuracy: 0.0000e+00\n",
      "Epoch 354/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5682 - accuracy: 0.0000e+00 - val_loss: 0.5991 - val_accuracy: 0.0000e+00\n",
      "Epoch 355/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5679 - accuracy: 0.0000e+00 - val_loss: 0.5990 - val_accuracy: 0.0000e+00\n",
      "Epoch 356/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5676 - accuracy: 0.0000e+00 - val_loss: 0.5989 - val_accuracy: 0.0000e+00\n",
      "Epoch 357/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5673 - accuracy: 0.0000e+00 - val_loss: 0.5987 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 358/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5670 - accuracy: 0.0000e+00 - val_loss: 0.5986 - val_accuracy: 0.0000e+00\n",
      "Epoch 359/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5667 - accuracy: 0.0000e+00 - val_loss: 0.5985 - val_accuracy: 0.0000e+00\n",
      "Epoch 360/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5664 - accuracy: 0.0000e+00 - val_loss: 0.5984 - val_accuracy: 0.0000e+00\n",
      "Epoch 361/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5661 - accuracy: 0.0000e+00 - val_loss: 0.5983 - val_accuracy: 0.0000e+00\n",
      "Epoch 362/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5658 - accuracy: 0.0000e+00 - val_loss: 0.5982 - val_accuracy: 0.0000e+00\n",
      "Epoch 363/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5655 - accuracy: 0.0000e+00 - val_loss: 0.5981 - val_accuracy: 0.0000e+00\n",
      "Epoch 364/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5652 - accuracy: 0.0000e+00 - val_loss: 0.5980 - val_accuracy: 0.0000e+00\n",
      "Epoch 365/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5649 - accuracy: 0.0000e+00 - val_loss: 0.5978 - val_accuracy: 0.0000e+00\n",
      "Epoch 366/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5646 - accuracy: 0.0000e+00 - val_loss: 0.5977 - val_accuracy: 0.0000e+00\n",
      "Epoch 367/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5643 - accuracy: 0.0000e+00 - val_loss: 0.5976 - val_accuracy: 0.0000e+00\n",
      "Epoch 368/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5641 - accuracy: 0.0000e+00 - val_loss: 0.5975 - val_accuracy: 0.0000e+00\n",
      "Epoch 369/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5638 - accuracy: 0.0000e+00 - val_loss: 0.5974 - val_accuracy: 0.0000e+00\n",
      "Epoch 370/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5635 - accuracy: 0.0000e+00 - val_loss: 0.5973 - val_accuracy: 0.0000e+00\n",
      "Epoch 371/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5632 - accuracy: 0.0000e+00 - val_loss: 0.5972 - val_accuracy: 0.0000e+00\n",
      "Epoch 372/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5629 - accuracy: 0.0000e+00 - val_loss: 0.5971 - val_accuracy: 0.0000e+00\n",
      "Epoch 373/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5626 - accuracy: 0.0000e+00 - val_loss: 0.5970 - val_accuracy: 0.0000e+00\n",
      "Epoch 374/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5623 - accuracy: 0.0000e+00 - val_loss: 0.5969 - val_accuracy: 0.0000e+00\n",
      "Epoch 375/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5621 - accuracy: 0.0000e+00 - val_loss: 0.5967 - val_accuracy: 0.0000e+00\n",
      "Epoch 376/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5618 - accuracy: 0.0000e+00 - val_loss: 0.5966 - val_accuracy: 0.0000e+00\n",
      "Epoch 377/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5615 - accuracy: 0.0000e+00 - val_loss: 0.5965 - val_accuracy: 0.0000e+00\n",
      "Epoch 378/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5612 - accuracy: 0.0000e+00 - val_loss: 0.5964 - val_accuracy: 0.0000e+00\n",
      "Epoch 379/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5609 - accuracy: 0.0000e+00 - val_loss: 0.5963 - val_accuracy: 0.0000e+00\n",
      "Epoch 380/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5606 - accuracy: 0.0000e+00 - val_loss: 0.5962 - val_accuracy: 0.0000e+00\n",
      "Epoch 381/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5604 - accuracy: 0.0000e+00 - val_loss: 0.5961 - val_accuracy: 0.0000e+00\n",
      "Epoch 382/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5601 - accuracy: 0.0000e+00 - val_loss: 0.5960 - val_accuracy: 0.0000e+00\n",
      "Epoch 383/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5598 - accuracy: 0.0000e+00 - val_loss: 0.5959 - val_accuracy: 0.0000e+00\n",
      "Epoch 384/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5595 - accuracy: 0.0000e+00 - val_loss: 0.5957 - val_accuracy: 0.0000e+00\n",
      "Epoch 385/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5592 - accuracy: 0.0000e+00 - val_loss: 0.5956 - val_accuracy: 0.0000e+00\n",
      "Epoch 386/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5590 - accuracy: 0.0000e+00 - val_loss: 0.5955 - val_accuracy: 0.0000e+00\n",
      "Epoch 387/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5587 - accuracy: 0.0000e+00 - val_loss: 0.5954 - val_accuracy: 0.0000e+00\n",
      "Epoch 388/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5584 - accuracy: 0.0000e+00 - val_loss: 0.5953 - val_accuracy: 0.0000e+00\n",
      "Epoch 389/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5581 - accuracy: 0.0000e+00 - val_loss: 0.5952 - val_accuracy: 0.0000e+00\n",
      "Epoch 390/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5578 - accuracy: 0.0000e+00 - val_loss: 0.5951 - val_accuracy: 0.0000e+00\n",
      "Epoch 391/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5576 - accuracy: 0.0000e+00 - val_loss: 0.5950 - val_accuracy: 0.0000e+00\n",
      "Epoch 392/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5573 - accuracy: 0.0000e+00 - val_loss: 0.5949 - val_accuracy: 0.0000e+00\n",
      "Epoch 393/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5570 - accuracy: 0.0000e+00 - val_loss: 0.5948 - val_accuracy: 0.0000e+00\n",
      "Epoch 394/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5567 - accuracy: 0.0000e+00 - val_loss: 0.5946 - val_accuracy: 0.0000e+00\n",
      "Epoch 395/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5565 - accuracy: 0.0000e+00 - val_loss: 0.5945 - val_accuracy: 0.0000e+00\n",
      "Epoch 396/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5562 - accuracy: 0.0000e+00 - val_loss: 0.5944 - val_accuracy: 0.0000e+00\n",
      "Epoch 397/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5559 - accuracy: 0.0000e+00 - val_loss: 0.5943 - val_accuracy: 0.0000e+00\n",
      "Epoch 398/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5556 - accuracy: 0.0000e+00 - val_loss: 0.5942 - val_accuracy: 0.0000e+00\n",
      "Epoch 399/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5554 - accuracy: 0.0000e+00 - val_loss: 0.5941 - val_accuracy: 0.0000e+00\n",
      "Epoch 400/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5551 - accuracy: 0.0000e+00 - val_loss: 0.5940 - val_accuracy: 0.0000e+00\n",
      "Epoch 401/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5548 - accuracy: 0.0000e+00 - val_loss: 0.5939 - val_accuracy: 0.0000e+00\n",
      "Epoch 402/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5546 - accuracy: 0.0000e+00 - val_loss: 0.5938 - val_accuracy: 0.0000e+00\n",
      "Epoch 403/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5543 - accuracy: 0.0000e+00 - val_loss: 0.5937 - val_accuracy: 0.0000e+00\n",
      "Epoch 404/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5540 - accuracy: 0.0000e+00 - val_loss: 0.5935 - val_accuracy: 0.0000e+00\n",
      "Epoch 405/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5537 - accuracy: 0.0000e+00 - val_loss: 0.5934 - val_accuracy: 0.0000e+00\n",
      "Epoch 406/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5535 - accuracy: 0.0000e+00 - val_loss: 0.5933 - val_accuracy: 0.0000e+00\n",
      "Epoch 407/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5532 - accuracy: 0.0000e+00 - val_loss: 0.5932 - val_accuracy: 0.0000e+00\n",
      "Epoch 408/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5529 - accuracy: 0.0000e+00 - val_loss: 0.5931 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 409/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5527 - accuracy: 0.0000e+00 - val_loss: 0.5930 - val_accuracy: 0.0000e+00\n",
      "Epoch 410/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5524 - accuracy: 0.0000e+00 - val_loss: 0.5929 - val_accuracy: 0.0000e+00\n",
      "Epoch 411/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5521 - accuracy: 0.0000e+00 - val_loss: 0.5928 - val_accuracy: 0.0000e+00\n",
      "Epoch 412/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5519 - accuracy: 0.0000e+00 - val_loss: 0.5927 - val_accuracy: 0.0000e+00\n",
      "Epoch 413/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5516 - accuracy: 0.0000e+00 - val_loss: 0.5926 - val_accuracy: 0.0000e+00\n",
      "Epoch 414/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5513 - accuracy: 0.0000e+00 - val_loss: 0.5925 - val_accuracy: 0.0000e+00\n",
      "Epoch 415/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5511 - accuracy: 0.0000e+00 - val_loss: 0.5924 - val_accuracy: 0.0000e+00\n",
      "Epoch 416/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5508 - accuracy: 0.0000e+00 - val_loss: 0.5923 - val_accuracy: 0.0000e+00\n",
      "Epoch 417/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5505 - accuracy: 0.0000e+00 - val_loss: 0.5922 - val_accuracy: 0.0000e+00\n",
      "Epoch 418/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5503 - accuracy: 0.0000e+00 - val_loss: 0.5921 - val_accuracy: 0.0000e+00\n",
      "Epoch 419/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5500 - accuracy: 0.0000e+00 - val_loss: 0.5919 - val_accuracy: 0.0000e+00\n",
      "Epoch 420/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5498 - accuracy: 0.0000e+00 - val_loss: 0.5918 - val_accuracy: 0.0000e+00\n",
      "Epoch 421/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5495 - accuracy: 0.0000e+00 - val_loss: 0.5917 - val_accuracy: 0.0000e+00\n",
      "Epoch 422/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5492 - accuracy: 0.0000e+00 - val_loss: 0.5916 - val_accuracy: 0.0000e+00\n",
      "Epoch 423/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5490 - accuracy: 0.0000e+00 - val_loss: 0.5915 - val_accuracy: 0.0000e+00\n",
      "Epoch 424/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5487 - accuracy: 0.0000e+00 - val_loss: 0.5914 - val_accuracy: 0.0000e+00\n",
      "Epoch 425/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5485 - accuracy: 0.0000e+00 - val_loss: 0.5913 - val_accuracy: 0.0000e+00\n",
      "Epoch 426/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5482 - accuracy: 0.0000e+00 - val_loss: 0.5913 - val_accuracy: 0.0000e+00\n",
      "Epoch 427/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5479 - accuracy: 0.0000e+00 - val_loss: 0.5912 - val_accuracy: 0.0000e+00\n",
      "Epoch 428/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5477 - accuracy: 0.0000e+00 - val_loss: 0.5911 - val_accuracy: 0.0000e+00\n",
      "Epoch 429/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5474 - accuracy: 0.0000e+00 - val_loss: 0.5910 - val_accuracy: 0.0000e+00\n",
      "Epoch 430/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5472 - accuracy: 0.0000e+00 - val_loss: 0.5909 - val_accuracy: 0.0000e+00\n",
      "Epoch 431/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5469 - accuracy: 0.0000e+00 - val_loss: 0.5908 - val_accuracy: 0.0000e+00\n",
      "Epoch 432/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5466 - accuracy: 0.0000e+00 - val_loss: 0.5907 - val_accuracy: 0.0000e+00\n",
      "Epoch 433/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5464 - accuracy: 0.0000e+00 - val_loss: 0.5906 - val_accuracy: 0.0000e+00\n",
      "Epoch 434/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5461 - accuracy: 0.0000e+00 - val_loss: 0.5905 - val_accuracy: 0.0000e+00\n",
      "Epoch 435/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5459 - accuracy: 0.0000e+00 - val_loss: 0.5904 - val_accuracy: 0.0000e+00\n",
      "Epoch 436/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5456 - accuracy: 0.0000e+00 - val_loss: 0.5903 - val_accuracy: 0.0000e+00\n",
      "Epoch 437/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5454 - accuracy: 0.0000e+00 - val_loss: 0.5902 - val_accuracy: 0.0000e+00\n",
      "Epoch 438/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5451 - accuracy: 0.0000e+00 - val_loss: 0.5901 - val_accuracy: 0.0000e+00\n",
      "Epoch 439/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5449 - accuracy: 0.0000e+00 - val_loss: 0.5900 - val_accuracy: 0.0000e+00\n",
      "Epoch 440/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5446 - accuracy: 0.0000e+00 - val_loss: 0.5899 - val_accuracy: 0.0000e+00\n",
      "Epoch 441/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5444 - accuracy: 0.0000e+00 - val_loss: 0.5898 - val_accuracy: 0.0000e+00\n",
      "Epoch 442/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5441 - accuracy: 0.0000e+00 - val_loss: 0.5897 - val_accuracy: 0.0000e+00\n",
      "Epoch 443/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5439 - accuracy: 0.0000e+00 - val_loss: 0.5896 - val_accuracy: 0.0000e+00\n",
      "Epoch 444/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5436 - accuracy: 0.0000e+00 - val_loss: 0.5895 - val_accuracy: 0.0000e+00\n",
      "Epoch 445/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5434 - accuracy: 0.0000e+00 - val_loss: 0.5894 - val_accuracy: 0.0000e+00\n",
      "Epoch 446/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5431 - accuracy: 0.0000e+00 - val_loss: 0.5893 - val_accuracy: 0.0000e+00\n",
      "Epoch 447/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5429 - accuracy: 0.0000e+00 - val_loss: 0.5892 - val_accuracy: 0.0000e+00\n",
      "Epoch 448/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5426 - accuracy: 0.0000e+00 - val_loss: 0.5891 - val_accuracy: 0.0000e+00\n",
      "Epoch 449/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5424 - accuracy: 0.0000e+00 - val_loss: 0.5890 - val_accuracy: 0.0000e+00\n",
      "Epoch 450/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5421 - accuracy: 0.0000e+00 - val_loss: 0.5889 - val_accuracy: 0.0000e+00\n",
      "Epoch 451/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5419 - accuracy: 0.0000e+00 - val_loss: 0.5888 - val_accuracy: 0.0000e+00\n",
      "Epoch 452/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5416 - accuracy: 0.0000e+00 - val_loss: 0.5888 - val_accuracy: 0.0000e+00\n",
      "Epoch 453/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5414 - accuracy: 0.0000e+00 - val_loss: 0.5887 - val_accuracy: 0.0000e+00\n",
      "Epoch 454/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5411 - accuracy: 0.0000e+00 - val_loss: 0.5886 - val_accuracy: 0.0000e+00\n",
      "Epoch 455/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5409 - accuracy: 0.0000e+00 - val_loss: 0.5885 - val_accuracy: 0.0000e+00\n",
      "Epoch 456/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5406 - accuracy: 0.0000e+00 - val_loss: 0.5884 - val_accuracy: 0.0000e+00\n",
      "Epoch 457/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5404 - accuracy: 0.0000e+00 - val_loss: 0.5883 - val_accuracy: 0.0000e+00\n",
      "Epoch 458/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5402 - accuracy: 0.0000e+00 - val_loss: 0.5882 - val_accuracy: 0.0000e+00\n",
      "Epoch 459/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5399 - accuracy: 0.0000e+00 - val_loss: 0.5881 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 460/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5397 - accuracy: 0.0000e+00 - val_loss: 0.5880 - val_accuracy: 0.0000e+00\n",
      "Epoch 461/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5394 - accuracy: 0.0000e+00 - val_loss: 0.5879 - val_accuracy: 0.0000e+00\n",
      "Epoch 462/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5392 - accuracy: 0.0000e+00 - val_loss: 0.5878 - val_accuracy: 0.0000e+00\n",
      "Epoch 463/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5390 - accuracy: 0.0000e+00 - val_loss: 0.5877 - val_accuracy: 0.0000e+00\n",
      "Epoch 464/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5387 - accuracy: 0.0000e+00 - val_loss: 0.5876 - val_accuracy: 0.0000e+00\n",
      "Epoch 465/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5385 - accuracy: 0.0000e+00 - val_loss: 0.5875 - val_accuracy: 0.0000e+00\n",
      "Epoch 466/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5382 - accuracy: 0.0000e+00 - val_loss: 0.5874 - val_accuracy: 0.0000e+00\n",
      "Epoch 467/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5380 - accuracy: 0.0000e+00 - val_loss: 0.5873 - val_accuracy: 0.0000e+00\n",
      "Epoch 468/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5378 - accuracy: 0.0000e+00 - val_loss: 0.5872 - val_accuracy: 0.0000e+00\n",
      "Epoch 469/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5375 - accuracy: 0.0000e+00 - val_loss: 0.5871 - val_accuracy: 0.0000e+00\n",
      "Epoch 470/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5373 - accuracy: 0.0000e+00 - val_loss: 0.5870 - val_accuracy: 0.0000e+00\n",
      "Epoch 471/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5370 - accuracy: 0.0000e+00 - val_loss: 0.5869 - val_accuracy: 0.0000e+00\n",
      "Epoch 472/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5368 - accuracy: 0.0000e+00 - val_loss: 0.5868 - val_accuracy: 0.0000e+00\n",
      "Epoch 473/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5366 - accuracy: 0.0000e+00 - val_loss: 0.5867 - val_accuracy: 0.0000e+00\n",
      "Epoch 474/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5363 - accuracy: 0.0000e+00 - val_loss: 0.5866 - val_accuracy: 0.0000e+00\n",
      "Epoch 475/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5361 - accuracy: 0.0000e+00 - val_loss: 0.5866 - val_accuracy: 0.0000e+00\n",
      "Epoch 476/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5359 - accuracy: 0.0000e+00 - val_loss: 0.5865 - val_accuracy: 0.0000e+00\n",
      "Epoch 477/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5356 - accuracy: 0.0000e+00 - val_loss: 0.5864 - val_accuracy: 0.0000e+00\n",
      "Epoch 478/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5354 - accuracy: 0.0000e+00 - val_loss: 0.5863 - val_accuracy: 0.0000e+00\n",
      "Epoch 479/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5352 - accuracy: 0.0000e+00 - val_loss: 0.5862 - val_accuracy: 0.0000e+00\n",
      "Epoch 480/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5350 - accuracy: 0.0000e+00 - val_loss: 0.5861 - val_accuracy: 0.0000e+00\n",
      "Epoch 481/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5347 - accuracy: 0.0000e+00 - val_loss: 0.5860 - val_accuracy: 0.0000e+00\n",
      "Epoch 482/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5345 - accuracy: 0.0000e+00 - val_loss: 0.5859 - val_accuracy: 0.0000e+00\n",
      "Epoch 483/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5343 - accuracy: 0.0000e+00 - val_loss: 0.5858 - val_accuracy: 0.0000e+00\n",
      "Epoch 484/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5340 - accuracy: 0.0000e+00 - val_loss: 0.5857 - val_accuracy: 0.0000e+00\n",
      "Epoch 485/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5338 - accuracy: 0.0000e+00 - val_loss: 0.5856 - val_accuracy: 0.0000e+00\n",
      "Epoch 486/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5336 - accuracy: 0.0000e+00 - val_loss: 0.5855 - val_accuracy: 0.0000e+00\n",
      "Epoch 487/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5334 - accuracy: 0.0000e+00 - val_loss: 0.5854 - val_accuracy: 0.0000e+00\n",
      "Epoch 488/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5331 - accuracy: 0.0000e+00 - val_loss: 0.5854 - val_accuracy: 0.0000e+00\n",
      "Epoch 489/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5329 - accuracy: 0.0000e+00 - val_loss: 0.5853 - val_accuracy: 0.0000e+00\n",
      "Epoch 490/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5327 - accuracy: 0.0000e+00 - val_loss: 0.5852 - val_accuracy: 0.0000e+00\n",
      "Epoch 491/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5325 - accuracy: 0.0000e+00 - val_loss: 0.5851 - val_accuracy: 0.0000e+00\n",
      "Epoch 492/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5322 - accuracy: 0.0000e+00 - val_loss: 0.5850 - val_accuracy: 0.0000e+00\n",
      "Epoch 493/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5320 - accuracy: 0.0000e+00 - val_loss: 0.5849 - val_accuracy: 0.0000e+00\n",
      "Epoch 494/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5318 - accuracy: 0.0000e+00 - val_loss: 0.5848 - val_accuracy: 0.0000e+00\n",
      "Epoch 495/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5316 - accuracy: 0.0000e+00 - val_loss: 0.5847 - val_accuracy: 0.0000e+00\n",
      "Epoch 496/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5314 - accuracy: 0.0000e+00 - val_loss: 0.5846 - val_accuracy: 0.0000e+00\n",
      "Epoch 497/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5311 - accuracy: 0.0000e+00 - val_loss: 0.5846 - val_accuracy: 0.0000e+00\n",
      "Epoch 498/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5309 - accuracy: 0.0000e+00 - val_loss: 0.5845 - val_accuracy: 0.0000e+00\n",
      "Epoch 499/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5307 - accuracy: 0.0000e+00 - val_loss: 0.5844 - val_accuracy: 0.0000e+00\n",
      "Epoch 500/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5305 - accuracy: 0.0000e+00 - val_loss: 0.5843 - val_accuracy: 0.0000e+00\n",
      "Epoch 501/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5303 - accuracy: 0.0000e+00 - val_loss: 0.5842 - val_accuracy: 0.0000e+00\n",
      "Epoch 502/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5300 - accuracy: 0.0000e+00 - val_loss: 0.5841 - val_accuracy: 0.0000e+00\n",
      "Epoch 503/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5298 - accuracy: 0.0000e+00 - val_loss: 0.5840 - val_accuracy: 0.0000e+00\n",
      "Epoch 504/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5296 - accuracy: 0.0000e+00 - val_loss: 0.5840 - val_accuracy: 0.0000e+00\n",
      "Epoch 505/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5294 - accuracy: 0.0000e+00 - val_loss: 0.5839 - val_accuracy: 0.0000e+00\n",
      "Epoch 506/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5292 - accuracy: 0.0000e+00 - val_loss: 0.5838 - val_accuracy: 0.0000e+00\n",
      "Epoch 507/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5290 - accuracy: 0.0000e+00 - val_loss: 0.5837 - val_accuracy: 0.0000e+00\n",
      "Epoch 508/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5288 - accuracy: 0.0000e+00 - val_loss: 0.5836 - val_accuracy: 0.0000e+00\n",
      "Epoch 509/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5285 - accuracy: 0.0000e+00 - val_loss: 0.5835 - val_accuracy: 0.0000e+00\n",
      "Epoch 510/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5283 - accuracy: 0.0000e+00 - val_loss: 0.5835 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 511/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5281 - accuracy: 0.0000e+00 - val_loss: 0.5834 - val_accuracy: 0.0000e+00\n",
      "Epoch 512/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5279 - accuracy: 0.0000e+00 - val_loss: 0.5833 - val_accuracy: 0.0000e+00\n",
      "Epoch 513/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5277 - accuracy: 0.0000e+00 - val_loss: 0.5832 - val_accuracy: 0.0000e+00\n",
      "Epoch 514/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5275 - accuracy: 0.0000e+00 - val_loss: 0.5831 - val_accuracy: 0.0000e+00\n",
      "Epoch 515/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5273 - accuracy: 0.0000e+00 - val_loss: 0.5831 - val_accuracy: 0.0000e+00\n",
      "Epoch 516/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5271 - accuracy: 0.0000e+00 - val_loss: 0.5830 - val_accuracy: 0.0000e+00\n",
      "Epoch 517/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5269 - accuracy: 0.0000e+00 - val_loss: 0.5829 - val_accuracy: 0.0000e+00\n",
      "Epoch 518/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5267 - accuracy: 0.0000e+00 - val_loss: 0.5828 - val_accuracy: 0.0000e+00\n",
      "Epoch 519/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5265 - accuracy: 0.0000e+00 - val_loss: 0.5827 - val_accuracy: 0.0000e+00\n",
      "Epoch 520/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5263 - accuracy: 0.0000e+00 - val_loss: 0.5827 - val_accuracy: 0.0000e+00\n",
      "Epoch 521/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5261 - accuracy: 0.0000e+00 - val_loss: 0.5826 - val_accuracy: 0.0000e+00\n",
      "Epoch 522/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.5825 - val_accuracy: 0.0000e+00\n",
      "Epoch 523/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.5824 - val_accuracy: 0.0000e+00\n",
      "Epoch 524/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5254 - accuracy: 0.0000e+00 - val_loss: 0.5823 - val_accuracy: 0.0000e+00\n",
      "Epoch 525/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5252 - accuracy: 0.0000e+00 - val_loss: 0.5823 - val_accuracy: 0.0000e+00\n",
      "Epoch 526/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5250 - accuracy: 0.0000e+00 - val_loss: 0.5822 - val_accuracy: 0.0000e+00\n",
      "Epoch 527/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5248 - accuracy: 0.0000e+00 - val_loss: 0.5821 - val_accuracy: 0.0000e+00\n",
      "Epoch 528/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5246 - accuracy: 0.0000e+00 - val_loss: 0.5820 - val_accuracy: 0.0000e+00\n",
      "Epoch 529/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5244 - accuracy: 0.0000e+00 - val_loss: 0.5819 - val_accuracy: 0.0000e+00\n",
      "Epoch 530/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5242 - accuracy: 0.0000e+00 - val_loss: 0.5819 - val_accuracy: 0.0000e+00\n",
      "Epoch 531/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5240 - accuracy: 0.0000e+00 - val_loss: 0.5818 - val_accuracy: 0.0000e+00\n",
      "Epoch 532/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5239 - accuracy: 0.0000e+00 - val_loss: 0.5817 - val_accuracy: 0.0000e+00\n",
      "Epoch 533/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5237 - accuracy: 0.0000e+00 - val_loss: 0.5817 - val_accuracy: 0.0000e+00\n",
      "Epoch 534/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5235 - accuracy: 0.0000e+00 - val_loss: 0.5816 - val_accuracy: 0.0000e+00\n",
      "Epoch 535/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5233 - accuracy: 0.0000e+00 - val_loss: 0.5815 - val_accuracy: 0.0000e+00\n",
      "Epoch 536/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5231 - accuracy: 0.0000e+00 - val_loss: 0.5814 - val_accuracy: 0.0000e+00\n",
      "Epoch 537/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5229 - accuracy: 0.0000e+00 - val_loss: 0.5814 - val_accuracy: 0.0000e+00\n",
      "Epoch 538/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5227 - accuracy: 0.0000e+00 - val_loss: 0.5813 - val_accuracy: 0.0000e+00\n",
      "Epoch 539/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5225 - accuracy: 0.0000e+00 - val_loss: 0.5812 - val_accuracy: 0.0000e+00\n",
      "Epoch 540/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5223 - accuracy: 0.0000e+00 - val_loss: 0.5811 - val_accuracy: 0.0000e+00\n",
      "Epoch 541/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5221 - accuracy: 0.0000e+00 - val_loss: 0.5811 - val_accuracy: 0.0000e+00\n",
      "Epoch 542/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5219 - accuracy: 0.0000e+00 - val_loss: 0.5810 - val_accuracy: 0.0000e+00\n",
      "Epoch 543/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5217 - accuracy: 0.0000e+00 - val_loss: 0.5809 - val_accuracy: 0.0000e+00\n",
      "Epoch 544/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5215 - accuracy: 0.0000e+00 - val_loss: 0.5809 - val_accuracy: 0.0000e+00\n",
      "Epoch 545/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5213 - accuracy: 0.0000e+00 - val_loss: 0.5808 - val_accuracy: 0.0000e+00\n",
      "Epoch 546/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5212 - accuracy: 0.0000e+00 - val_loss: 0.5807 - val_accuracy: 0.0000e+00\n",
      "Epoch 547/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5210 - accuracy: 0.0000e+00 - val_loss: 0.5806 - val_accuracy: 0.0000e+00\n",
      "Epoch 548/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5208 - accuracy: 0.0000e+00 - val_loss: 0.5806 - val_accuracy: 0.0000e+00\n",
      "Epoch 549/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5206 - accuracy: 0.0000e+00 - val_loss: 0.5805 - val_accuracy: 0.0000e+00\n",
      "Epoch 550/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5204 - accuracy: 0.0000e+00 - val_loss: 0.5804 - val_accuracy: 0.0000e+00\n",
      "Epoch 551/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5202 - accuracy: 0.0000e+00 - val_loss: 0.5804 - val_accuracy: 0.0000e+00\n",
      "Epoch 552/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5200 - accuracy: 0.0000e+00 - val_loss: 0.5803 - val_accuracy: 0.0000e+00\n",
      "Epoch 553/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5198 - accuracy: 0.0000e+00 - val_loss: 0.5802 - val_accuracy: 0.0000e+00\n",
      "Epoch 554/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5197 - accuracy: 0.0000e+00 - val_loss: 0.5801 - val_accuracy: 0.0000e+00\n",
      "Epoch 555/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5195 - accuracy: 0.0000e+00 - val_loss: 0.5801 - val_accuracy: 0.0000e+00\n",
      "Epoch 556/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5193 - accuracy: 0.0000e+00 - val_loss: 0.5800 - val_accuracy: 0.0000e+00\n",
      "Epoch 557/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5191 - accuracy: 0.0000e+00 - val_loss: 0.5799 - val_accuracy: 0.0000e+00\n",
      "Epoch 558/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5189 - accuracy: 0.0000e+00 - val_loss: 0.5798 - val_accuracy: 0.0000e+00\n",
      "Epoch 559/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5188 - accuracy: 0.0000e+00 - val_loss: 0.5798 - val_accuracy: 0.0000e+00\n",
      "Epoch 560/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5186 - accuracy: 0.0000e+00 - val_loss: 0.5797 - val_accuracy: 0.0000e+00\n",
      "Epoch 561/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5184 - accuracy: 0.0000e+00 - val_loss: 0.5796 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 562/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5182 - accuracy: 0.0000e+00 - val_loss: 0.5796 - val_accuracy: 0.0000e+00\n",
      "Epoch 563/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5180 - accuracy: 0.0000e+00 - val_loss: 0.5795 - val_accuracy: 0.0000e+00\n",
      "Epoch 564/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5179 - accuracy: 0.0000e+00 - val_loss: 0.5794 - val_accuracy: 0.0000e+00\n",
      "Epoch 565/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5177 - accuracy: 0.0000e+00 - val_loss: 0.5793 - val_accuracy: 0.0000e+00\n",
      "Epoch 566/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5175 - accuracy: 0.0000e+00 - val_loss: 0.5793 - val_accuracy: 0.0000e+00\n",
      "Epoch 567/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5173 - accuracy: 0.0000e+00 - val_loss: 0.5792 - val_accuracy: 0.0000e+00\n",
      "Epoch 568/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5172 - accuracy: 0.0000e+00 - val_loss: 0.5791 - val_accuracy: 0.0000e+00\n",
      "Epoch 569/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5170 - accuracy: 0.0000e+00 - val_loss: 0.5791 - val_accuracy: 0.0000e+00\n",
      "Epoch 570/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5168 - accuracy: 0.0000e+00 - val_loss: 0.5790 - val_accuracy: 0.0000e+00\n",
      "Epoch 571/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5166 - accuracy: 0.0000e+00 - val_loss: 0.5789 - val_accuracy: 0.0000e+00\n",
      "Epoch 572/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5165 - accuracy: 0.0000e+00 - val_loss: 0.5789 - val_accuracy: 0.0000e+00\n",
      "Epoch 573/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5163 - accuracy: 0.0000e+00 - val_loss: 0.5788 - val_accuracy: 0.0000e+00\n",
      "Epoch 574/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5161 - accuracy: 0.0000e+00 - val_loss: 0.5787 - val_accuracy: 0.0000e+00\n",
      "Epoch 575/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5159 - accuracy: 0.0000e+00 - val_loss: 0.5787 - val_accuracy: 0.0000e+00\n",
      "Epoch 576/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5158 - accuracy: 0.0000e+00 - val_loss: 0.5786 - val_accuracy: 0.0000e+00\n",
      "Epoch 577/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5156 - accuracy: 0.0000e+00 - val_loss: 0.5785 - val_accuracy: 0.0000e+00\n",
      "Epoch 578/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5154 - accuracy: 0.0000e+00 - val_loss: 0.5785 - val_accuracy: 0.0000e+00\n",
      "Epoch 579/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5153 - accuracy: 0.0000e+00 - val_loss: 0.5784 - val_accuracy: 0.0000e+00\n",
      "Epoch 580/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5151 - accuracy: 0.0000e+00 - val_loss: 0.5783 - val_accuracy: 0.0000e+00\n",
      "Epoch 581/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5149 - accuracy: 0.0000e+00 - val_loss: 0.5782 - val_accuracy: 0.0000e+00\n",
      "Epoch 582/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5148 - accuracy: 0.0000e+00 - val_loss: 0.5782 - val_accuracy: 0.0000e+00\n",
      "Epoch 583/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5146 - accuracy: 0.0000e+00 - val_loss: 0.5781 - val_accuracy: 0.0000e+00\n",
      "Epoch 584/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5144 - accuracy: 0.0000e+00 - val_loss: 0.5780 - val_accuracy: 0.0000e+00\n",
      "Epoch 585/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5143 - accuracy: 0.0000e+00 - val_loss: 0.5780 - val_accuracy: 0.0000e+00\n",
      "Epoch 586/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5141 - accuracy: 0.0000e+00 - val_loss: 0.5779 - val_accuracy: 0.0000e+00\n",
      "Epoch 587/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5139 - accuracy: 0.0000e+00 - val_loss: 0.5778 - val_accuracy: 0.0000e+00\n",
      "Epoch 588/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5138 - accuracy: 0.0000e+00 - val_loss: 0.5778 - val_accuracy: 0.0000e+00\n",
      "Epoch 589/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5136 - accuracy: 0.0000e+00 - val_loss: 0.5777 - val_accuracy: 0.0000e+00\n",
      "Epoch 590/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5134 - accuracy: 0.0000e+00 - val_loss: 0.5776 - val_accuracy: 0.0000e+00\n",
      "Epoch 591/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5133 - accuracy: 0.0000e+00 - val_loss: 0.5776 - val_accuracy: 0.0000e+00\n",
      "Epoch 592/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5131 - accuracy: 0.0000e+00 - val_loss: 0.5775 - val_accuracy: 0.0000e+00\n",
      "Epoch 593/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5130 - accuracy: 0.0000e+00 - val_loss: 0.5774 - val_accuracy: 0.0000e+00\n",
      "Epoch 594/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5128 - accuracy: 0.0000e+00 - val_loss: 0.5774 - val_accuracy: 0.0000e+00\n",
      "Epoch 595/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5126 - accuracy: 0.0000e+00 - val_loss: 0.5773 - val_accuracy: 0.0000e+00\n",
      "Epoch 596/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5125 - accuracy: 0.0000e+00 - val_loss: 0.5773 - val_accuracy: 0.0000e+00\n",
      "Epoch 597/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5123 - accuracy: 0.0000e+00 - val_loss: 0.5772 - val_accuracy: 0.0000e+00\n",
      "Epoch 598/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5122 - accuracy: 0.0000e+00 - val_loss: 0.5771 - val_accuracy: 0.0000e+00\n",
      "Epoch 599/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5120 - accuracy: 0.0000e+00 - val_loss: 0.5771 - val_accuracy: 0.0000e+00\n",
      "Epoch 600/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5118 - accuracy: 0.0000e+00 - val_loss: 0.5770 - val_accuracy: 0.0000e+00\n",
      "Epoch 601/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5117 - accuracy: 0.0000e+00 - val_loss: 0.5769 - val_accuracy: 0.0000e+00\n",
      "Epoch 602/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5115 - accuracy: 0.0000e+00 - val_loss: 0.5769 - val_accuracy: 0.0000e+00\n",
      "Epoch 603/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5114 - accuracy: 0.0000e+00 - val_loss: 0.5768 - val_accuracy: 0.0000e+00\n",
      "Epoch 604/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5112 - accuracy: 0.0000e+00 - val_loss: 0.5768 - val_accuracy: 0.0000e+00\n",
      "Epoch 605/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5111 - accuracy: 0.0000e+00 - val_loss: 0.5767 - val_accuracy: 0.0000e+00\n",
      "Epoch 606/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5109 - accuracy: 0.0000e+00 - val_loss: 0.5766 - val_accuracy: 0.0000e+00\n",
      "Epoch 607/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5108 - accuracy: 0.0000e+00 - val_loss: 0.5766 - val_accuracy: 0.0000e+00\n",
      "Epoch 608/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5106 - accuracy: 0.0000e+00 - val_loss: 0.5765 - val_accuracy: 0.0000e+00\n",
      "Epoch 609/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5105 - accuracy: 0.0000e+00 - val_loss: 0.5765 - val_accuracy: 0.0000e+00\n",
      "Epoch 610/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5103 - accuracy: 0.0000e+00 - val_loss: 0.5764 - val_accuracy: 0.0000e+00\n",
      "Epoch 611/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5102 - accuracy: 0.0000e+00 - val_loss: 0.5764 - val_accuracy: 0.0000e+00\n",
      "Epoch 612/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5100 - accuracy: 0.0000e+00 - val_loss: 0.5763 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 613/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5099 - accuracy: 0.0000e+00 - val_loss: 0.5762 - val_accuracy: 0.0000e+00\n",
      "Epoch 614/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5097 - accuracy: 0.0000e+00 - val_loss: 0.5762 - val_accuracy: 0.0000e+00\n",
      "Epoch 615/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5096 - accuracy: 0.0000e+00 - val_loss: 0.5761 - val_accuracy: 0.0000e+00\n",
      "Epoch 616/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5094 - accuracy: 0.0000e+00 - val_loss: 0.5761 - val_accuracy: 0.0000e+00\n",
      "Epoch 617/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5093 - accuracy: 0.0000e+00 - val_loss: 0.5760 - val_accuracy: 0.0000e+00\n",
      "Epoch 618/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5091 - accuracy: 0.0000e+00 - val_loss: 0.5760 - val_accuracy: 0.0000e+00\n",
      "Epoch 619/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5090 - accuracy: 0.0000e+00 - val_loss: 0.5759 - val_accuracy: 0.0000e+00\n",
      "Epoch 620/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5088 - accuracy: 0.0000e+00 - val_loss: 0.5759 - val_accuracy: 0.0000e+00\n",
      "Epoch 621/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5087 - accuracy: 0.0000e+00 - val_loss: 0.5758 - val_accuracy: 0.0000e+00\n",
      "Epoch 622/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5085 - accuracy: 0.0000e+00 - val_loss: 0.5758 - val_accuracy: 0.0000e+00\n",
      "Epoch 623/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5084 - accuracy: 0.0000e+00 - val_loss: 0.5757 - val_accuracy: 0.0000e+00\n",
      "Epoch 624/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5083 - accuracy: 0.0000e+00 - val_loss: 0.5757 - val_accuracy: 0.0000e+00\n",
      "Epoch 625/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5081 - accuracy: 0.0000e+00 - val_loss: 0.5756 - val_accuracy: 0.0000e+00\n",
      "Epoch 626/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5080 - accuracy: 0.0000e+00 - val_loss: 0.5756 - val_accuracy: 0.0000e+00\n",
      "Epoch 627/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5078 - accuracy: 0.0000e+00 - val_loss: 0.5755 - val_accuracy: 0.0000e+00\n",
      "Epoch 628/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5077 - accuracy: 0.0000e+00 - val_loss: 0.5755 - val_accuracy: 0.0000e+00\n",
      "Epoch 629/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5076 - accuracy: 0.0000e+00 - val_loss: 0.5754 - val_accuracy: 0.0000e+00\n",
      "Epoch 630/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5074 - accuracy: 0.0000e+00 - val_loss: 0.5754 - val_accuracy: 0.0000e+00\n",
      "Epoch 631/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5073 - accuracy: 0.0000e+00 - val_loss: 0.5753 - val_accuracy: 0.0000e+00\n",
      "Epoch 632/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5071 - accuracy: 0.0000e+00 - val_loss: 0.5753 - val_accuracy: 0.0000e+00\n",
      "Epoch 633/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5070 - accuracy: 0.0000e+00 - val_loss: 0.5752 - val_accuracy: 0.0000e+00\n",
      "Epoch 634/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5069 - accuracy: 0.0000e+00 - val_loss: 0.5752 - val_accuracy: 0.0000e+00\n",
      "Epoch 635/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5067 - accuracy: 0.0000e+00 - val_loss: 0.5751 - val_accuracy: 0.0000e+00\n",
      "Epoch 636/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5066 - accuracy: 0.0000e+00 - val_loss: 0.5751 - val_accuracy: 0.0000e+00\n",
      "Epoch 637/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5065 - accuracy: 0.0000e+00 - val_loss: 0.5750 - val_accuracy: 0.0000e+00\n",
      "Epoch 638/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5063 - accuracy: 0.0000e+00 - val_loss: 0.5750 - val_accuracy: 0.0000e+00\n",
      "Epoch 639/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5062 - accuracy: 0.0000e+00 - val_loss: 0.5749 - val_accuracy: 0.0000e+00\n",
      "Epoch 640/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5061 - accuracy: 0.0000e+00 - val_loss: 0.5749 - val_accuracy: 0.0000e+00\n",
      "Epoch 641/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5059 - accuracy: 0.0000e+00 - val_loss: 0.5749 - val_accuracy: 0.0000e+00\n",
      "Epoch 642/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5058 - accuracy: 0.0000e+00 - val_loss: 0.5748 - val_accuracy: 0.0000e+00\n",
      "Epoch 643/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5057 - accuracy: 0.0000e+00 - val_loss: 0.5748 - val_accuracy: 0.0000e+00\n",
      "Epoch 644/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5055 - accuracy: 0.0000e+00 - val_loss: 0.5747 - val_accuracy: 0.0000e+00\n",
      "Epoch 645/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5054 - accuracy: 0.0000e+00 - val_loss: 0.5747 - val_accuracy: 0.0000e+00\n",
      "Epoch 646/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5053 - accuracy: 0.0000e+00 - val_loss: 0.5746 - val_accuracy: 0.0000e+00\n",
      "Epoch 647/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5051 - accuracy: 0.0000e+00 - val_loss: 0.5746 - val_accuracy: 0.0000e+00\n",
      "Epoch 648/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5050 - accuracy: 0.0000e+00 - val_loss: 0.5746 - val_accuracy: 0.0000e+00\n",
      "Epoch 649/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5049 - accuracy: 0.0000e+00 - val_loss: 0.5745 - val_accuracy: 0.0000e+00\n",
      "Epoch 650/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5048 - accuracy: 0.0000e+00 - val_loss: 0.5745 - val_accuracy: 0.0000e+00\n",
      "Epoch 651/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5046 - accuracy: 0.0000e+00 - val_loss: 0.5744 - val_accuracy: 0.0000e+00\n",
      "Epoch 652/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5045 - accuracy: 0.0000e+00 - val_loss: 0.5744 - val_accuracy: 0.0000e+00\n",
      "Epoch 653/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5044 - accuracy: 0.0000e+00 - val_loss: 0.5744 - val_accuracy: 0.0000e+00\n",
      "Epoch 654/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5043 - accuracy: 0.0000e+00 - val_loss: 0.5743 - val_accuracy: 0.0000e+00\n",
      "Epoch 655/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5041 - accuracy: 0.0000e+00 - val_loss: 0.5743 - val_accuracy: 0.0000e+00\n",
      "Epoch 656/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5040 - accuracy: 0.0000e+00 - val_loss: 0.5742 - val_accuracy: 0.0000e+00\n",
      "Epoch 657/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5039 - accuracy: 0.0000e+00 - val_loss: 0.5742 - val_accuracy: 0.0000e+00\n",
      "Epoch 658/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5038 - accuracy: 0.0000e+00 - val_loss: 0.5742 - val_accuracy: 0.0000e+00\n",
      "Epoch 659/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5036 - accuracy: 0.0000e+00 - val_loss: 0.5741 - val_accuracy: 0.0000e+00\n",
      "Epoch 660/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5035 - accuracy: 0.0000e+00 - val_loss: 0.5741 - val_accuracy: 0.0000e+00\n",
      "Epoch 661/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5034 - accuracy: 0.0000e+00 - val_loss: 0.5740 - val_accuracy: 0.0000e+00\n",
      "Epoch 662/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5033 - accuracy: 0.0000e+00 - val_loss: 0.5740 - val_accuracy: 0.0000e+00\n",
      "Epoch 663/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5032 - accuracy: 0.0000e+00 - val_loss: 0.5740 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 664/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5030 - accuracy: 0.0000e+00 - val_loss: 0.5739 - val_accuracy: 0.0000e+00\n",
      "Epoch 665/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5029 - accuracy: 0.0000e+00 - val_loss: 0.5739 - val_accuracy: 0.0000e+00\n",
      "Epoch 666/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5028 - accuracy: 0.0000e+00 - val_loss: 0.5739 - val_accuracy: 0.0000e+00\n",
      "Epoch 667/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5027 - accuracy: 0.0000e+00 - val_loss: 0.5738 - val_accuracy: 0.0000e+00\n",
      "Epoch 668/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5026 - accuracy: 0.0000e+00 - val_loss: 0.5738 - val_accuracy: 0.0000e+00\n",
      "Epoch 669/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5024 - accuracy: 0.0000e+00 - val_loss: 0.5737 - val_accuracy: 0.0000e+00\n",
      "Epoch 670/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5023 - accuracy: 0.0000e+00 - val_loss: 0.5737 - val_accuracy: 0.0000e+00\n",
      "Epoch 671/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5022 - accuracy: 0.0000e+00 - val_loss: 0.5737 - val_accuracy: 0.0000e+00\n",
      "Epoch 672/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5021 - accuracy: 0.0000e+00 - val_loss: 0.5736 - val_accuracy: 0.0000e+00\n",
      "Epoch 673/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5020 - accuracy: 0.0000e+00 - val_loss: 0.5736 - val_accuracy: 0.0000e+00\n",
      "Epoch 674/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5019 - accuracy: 0.0000e+00 - val_loss: 0.5736 - val_accuracy: 0.0000e+00\n",
      "Epoch 675/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5018 - accuracy: 0.0000e+00 - val_loss: 0.5735 - val_accuracy: 0.0000e+00\n",
      "Epoch 676/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5016 - accuracy: 0.0000e+00 - val_loss: 0.5735 - val_accuracy: 0.0000e+00\n",
      "Epoch 677/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5015 - accuracy: 0.0000e+00 - val_loss: 0.5735 - val_accuracy: 0.0000e+00\n",
      "Epoch 678/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5014 - accuracy: 0.0000e+00 - val_loss: 0.5734 - val_accuracy: 0.0000e+00\n",
      "Epoch 679/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5013 - accuracy: 0.0000e+00 - val_loss: 0.5734 - val_accuracy: 0.0000e+00\n",
      "Epoch 680/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5012 - accuracy: 0.0000e+00 - val_loss: 0.5734 - val_accuracy: 0.0000e+00\n",
      "Epoch 681/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5011 - accuracy: 0.0000e+00 - val_loss: 0.5733 - val_accuracy: 0.0000e+00\n",
      "Epoch 682/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5010 - accuracy: 0.0000e+00 - val_loss: 0.5733 - val_accuracy: 0.0000e+00\n",
      "Epoch 683/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5009 - accuracy: 0.0000e+00 - val_loss: 0.5733 - val_accuracy: 0.0000e+00\n",
      "Epoch 684/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5007 - accuracy: 0.0000e+00 - val_loss: 0.5732 - val_accuracy: 0.0000e+00\n",
      "Epoch 685/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5006 - accuracy: 0.0000e+00 - val_loss: 0.5732 - val_accuracy: 0.0000e+00\n",
      "Epoch 686/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5005 - accuracy: 0.0000e+00 - val_loss: 0.5732 - val_accuracy: 0.0000e+00\n",
      "Epoch 687/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5004 - accuracy: 0.0000e+00 - val_loss: 0.5731 - val_accuracy: 0.0000e+00\n",
      "Epoch 688/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5003 - accuracy: 0.0000e+00 - val_loss: 0.5731 - val_accuracy: 0.0000e+00\n",
      "Epoch 689/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5002 - accuracy: 0.0000e+00 - val_loss: 0.5731 - val_accuracy: 0.0000e+00\n",
      "Epoch 690/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5001 - accuracy: 0.0000e+00 - val_loss: 0.5730 - val_accuracy: 0.0000e+00\n",
      "Epoch 691/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.5000 - accuracy: 0.0000e+00 - val_loss: 0.5730 - val_accuracy: 0.0000e+00\n",
      "Epoch 692/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4999 - accuracy: 0.0000e+00 - val_loss: 0.5730 - val_accuracy: 0.0000e+00\n",
      "Epoch 693/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4998 - accuracy: 0.0000e+00 - val_loss: 0.5729 - val_accuracy: 0.0000e+00\n",
      "Epoch 694/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4997 - accuracy: 0.0000e+00 - val_loss: 0.5729 - val_accuracy: 0.0000e+00\n",
      "Epoch 695/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4996 - accuracy: 0.0000e+00 - val_loss: 0.5729 - val_accuracy: 0.0000e+00\n",
      "Epoch 696/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4995 - accuracy: 0.0000e+00 - val_loss: 0.5728 - val_accuracy: 0.0000e+00\n",
      "Epoch 697/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4994 - accuracy: 0.0000e+00 - val_loss: 0.5728 - val_accuracy: 0.0000e+00\n",
      "Epoch 698/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4993 - accuracy: 0.0000e+00 - val_loss: 0.5728 - val_accuracy: 0.0000e+00\n",
      "Epoch 699/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4992 - accuracy: 0.0000e+00 - val_loss: 0.5727 - val_accuracy: 0.0000e+00\n",
      "Epoch 700/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4991 - accuracy: 0.0000e+00 - val_loss: 0.5727 - val_accuracy: 0.0000e+00\n",
      "Epoch 701/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4990 - accuracy: 0.0000e+00 - val_loss: 0.5727 - val_accuracy: 0.0000e+00\n",
      "Epoch 702/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4989 - accuracy: 0.0000e+00 - val_loss: 0.5726 - val_accuracy: 0.0000e+00\n",
      "Epoch 703/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4988 - accuracy: 0.0000e+00 - val_loss: 0.5726 - val_accuracy: 0.0000e+00\n",
      "Epoch 704/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4987 - accuracy: 0.0000e+00 - val_loss: 0.5726 - val_accuracy: 0.0000e+00\n",
      "Epoch 705/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4986 - accuracy: 0.0000e+00 - val_loss: 0.5726 - val_accuracy: 0.0000e+00\n",
      "Epoch 706/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4985 - accuracy: 0.0000e+00 - val_loss: 0.5725 - val_accuracy: 0.0000e+00\n",
      "Epoch 707/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4984 - accuracy: 0.0000e+00 - val_loss: 0.5725 - val_accuracy: 0.0000e+00\n",
      "Epoch 708/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4983 - accuracy: 0.0000e+00 - val_loss: 0.5725 - val_accuracy: 0.0000e+00\n",
      "Epoch 709/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4982 - accuracy: 0.0000e+00 - val_loss: 0.5724 - val_accuracy: 0.0000e+00\n",
      "Epoch 710/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4981 - accuracy: 0.0000e+00 - val_loss: 0.5724 - val_accuracy: 0.0000e+00\n",
      "Epoch 711/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4980 - accuracy: 0.0000e+00 - val_loss: 0.5724 - val_accuracy: 0.0000e+00\n",
      "Epoch 712/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4979 - accuracy: 0.0000e+00 - val_loss: 0.5724 - val_accuracy: 0.0000e+00\n",
      "Epoch 713/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4978 - accuracy: 0.0000e+00 - val_loss: 0.5723 - val_accuracy: 0.0000e+00\n",
      "Epoch 714/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4977 - accuracy: 0.0000e+00 - val_loss: 0.5723 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 715/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4976 - accuracy: 0.0000e+00 - val_loss: 0.5723 - val_accuracy: 0.0000e+00\n",
      "Epoch 716/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4975 - accuracy: 0.0000e+00 - val_loss: 0.5723 - val_accuracy: 0.0000e+00\n",
      "Epoch 717/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4974 - accuracy: 0.0000e+00 - val_loss: 0.5722 - val_accuracy: 0.0000e+00\n",
      "Epoch 718/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4973 - accuracy: 0.0000e+00 - val_loss: 0.5722 - val_accuracy: 0.0000e+00\n",
      "Epoch 719/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4972 - accuracy: 0.0000e+00 - val_loss: 0.5722 - val_accuracy: 0.0000e+00\n",
      "Epoch 720/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4971 - accuracy: 0.0000e+00 - val_loss: 0.5722 - val_accuracy: 0.0000e+00\n",
      "Epoch 721/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4970 - accuracy: 0.0000e+00 - val_loss: 0.5721 - val_accuracy: 0.0000e+00\n",
      "Epoch 722/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4970 - accuracy: 0.0000e+00 - val_loss: 0.5721 - val_accuracy: 0.0000e+00\n",
      "Epoch 723/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4969 - accuracy: 0.0000e+00 - val_loss: 0.5721 - val_accuracy: 0.0000e+00\n",
      "Epoch 724/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4968 - accuracy: 0.0000e+00 - val_loss: 0.5721 - val_accuracy: 0.0000e+00\n",
      "Epoch 725/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4967 - accuracy: 0.0000e+00 - val_loss: 0.5720 - val_accuracy: 0.0000e+00\n",
      "Epoch 726/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4966 - accuracy: 0.0000e+00 - val_loss: 0.5720 - val_accuracy: 0.0000e+00\n",
      "Epoch 727/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4965 - accuracy: 0.0000e+00 - val_loss: 0.5720 - val_accuracy: 0.0000e+00\n",
      "Epoch 728/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4964 - accuracy: 0.0000e+00 - val_loss: 0.5719 - val_accuracy: 0.0000e+00\n",
      "Epoch 729/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4963 - accuracy: 0.0000e+00 - val_loss: 0.5719 - val_accuracy: 0.0000e+00\n",
      "Epoch 730/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4962 - accuracy: 0.0000e+00 - val_loss: 0.5719 - val_accuracy: 0.0000e+00\n",
      "Epoch 731/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4961 - accuracy: 0.0000e+00 - val_loss: 0.5719 - val_accuracy: 0.0000e+00\n",
      "Epoch 732/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4961 - accuracy: 0.0000e+00 - val_loss: 0.5718 - val_accuracy: 0.0000e+00\n",
      "Epoch 733/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4960 - accuracy: 0.0000e+00 - val_loss: 0.5718 - val_accuracy: 0.0000e+00\n",
      "Epoch 734/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4959 - accuracy: 0.0000e+00 - val_loss: 0.5718 - val_accuracy: 0.0000e+00\n",
      "Epoch 735/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4958 - accuracy: 0.0000e+00 - val_loss: 0.5718 - val_accuracy: 0.0000e+00\n",
      "Epoch 736/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4957 - accuracy: 0.0000e+00 - val_loss: 0.5718 - val_accuracy: 0.0000e+00\n",
      "Epoch 737/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4956 - accuracy: 0.0000e+00 - val_loss: 0.5717 - val_accuracy: 0.0000e+00\n",
      "Epoch 738/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4955 - accuracy: 0.0000e+00 - val_loss: 0.5717 - val_accuracy: 0.0000e+00\n",
      "Epoch 739/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4955 - accuracy: 0.0000e+00 - val_loss: 0.5717 - val_accuracy: 0.0000e+00\n",
      "Epoch 740/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4954 - accuracy: 0.0000e+00 - val_loss: 0.5716 - val_accuracy: 0.0000e+00\n",
      "Epoch 741/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4953 - accuracy: 0.0000e+00 - val_loss: 0.5716 - val_accuracy: 0.0000e+00\n",
      "Epoch 742/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4952 - accuracy: 0.0000e+00 - val_loss: 0.5716 - val_accuracy: 0.0000e+00\n",
      "Epoch 743/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4951 - accuracy: 0.0000e+00 - val_loss: 0.5716 - val_accuracy: 0.0000e+00\n",
      "Epoch 744/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4951 - accuracy: 0.0000e+00 - val_loss: 0.5715 - val_accuracy: 0.0000e+00\n",
      "Epoch 745/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4950 - accuracy: 0.0000e+00 - val_loss: 0.5715 - val_accuracy: 0.0000e+00\n",
      "Epoch 746/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4949 - accuracy: 0.0000e+00 - val_loss: 0.5715 - val_accuracy: 0.0000e+00\n",
      "Epoch 747/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4948 - accuracy: 0.0000e+00 - val_loss: 0.5715 - val_accuracy: 0.0000e+00\n",
      "Epoch 748/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4947 - accuracy: 0.0000e+00 - val_loss: 0.5715 - val_accuracy: 0.0000e+00\n",
      "Epoch 749/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4947 - accuracy: 0.0000e+00 - val_loss: 0.5714 - val_accuracy: 0.0000e+00\n",
      "Epoch 750/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4946 - accuracy: 0.0000e+00 - val_loss: 0.5714 - val_accuracy: 0.0000e+00\n",
      "Epoch 751/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4945 - accuracy: 0.0000e+00 - val_loss: 0.5714 - val_accuracy: 0.0000e+00\n",
      "Epoch 752/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4944 - accuracy: 0.0000e+00 - val_loss: 0.5714 - val_accuracy: 0.0000e+00\n",
      "Epoch 753/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4943 - accuracy: 0.0000e+00 - val_loss: 0.5713 - val_accuracy: 0.0000e+00\n",
      "Epoch 754/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4943 - accuracy: 0.0000e+00 - val_loss: 0.5713 - val_accuracy: 0.0000e+00\n",
      "Epoch 755/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4942 - accuracy: 0.0000e+00 - val_loss: 0.5713 - val_accuracy: 0.0000e+00\n",
      "Epoch 756/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4941 - accuracy: 0.0000e+00 - val_loss: 0.5713 - val_accuracy: 0.0000e+00\n",
      "Epoch 757/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4940 - accuracy: 0.0000e+00 - val_loss: 0.5713 - val_accuracy: 0.0000e+00\n",
      "Epoch 758/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4940 - accuracy: 0.0000e+00 - val_loss: 0.5712 - val_accuracy: 0.0000e+00\n",
      "Epoch 759/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4939 - accuracy: 0.0000e+00 - val_loss: 0.5712 - val_accuracy: 0.0000e+00\n",
      "Epoch 760/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4938 - accuracy: 0.0000e+00 - val_loss: 0.5712 - val_accuracy: 0.0000e+00\n",
      "Epoch 761/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4937 - accuracy: 0.0000e+00 - val_loss: 0.5712 - val_accuracy: 0.0000e+00\n",
      "Epoch 762/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4936 - accuracy: 0.0000e+00 - val_loss: 0.5712 - val_accuracy: 0.0000e+00\n",
      "Epoch 763/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4936 - accuracy: 0.0000e+00 - val_loss: 0.5711 - val_accuracy: 0.0000e+00\n",
      "Epoch 764/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4935 - accuracy: 0.0000e+00 - val_loss: 0.5711 - val_accuracy: 0.0000e+00\n",
      "Epoch 765/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4934 - accuracy: 0.0000e+00 - val_loss: 0.5711 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 766/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4934 - accuracy: 0.0000e+00 - val_loss: 0.5711 - val_accuracy: 0.0000e+00\n",
      "Epoch 767/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4933 - accuracy: 0.0000e+00 - val_loss: 0.5711 - val_accuracy: 0.0000e+00\n",
      "Epoch 768/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4932 - accuracy: 0.0000e+00 - val_loss: 0.5710 - val_accuracy: 0.0000e+00\n",
      "Epoch 769/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4931 - accuracy: 0.0000e+00 - val_loss: 0.5710 - val_accuracy: 0.0000e+00\n",
      "Epoch 770/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4931 - accuracy: 0.0000e+00 - val_loss: 0.5710 - val_accuracy: 0.0000e+00\n",
      "Epoch 771/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4930 - accuracy: 0.0000e+00 - val_loss: 0.5710 - val_accuracy: 0.0000e+00\n",
      "Epoch 772/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4929 - accuracy: 0.0000e+00 - val_loss: 0.5710 - val_accuracy: 0.0000e+00\n",
      "Epoch 773/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4928 - accuracy: 0.0000e+00 - val_loss: 0.5709 - val_accuracy: 0.0000e+00\n",
      "Epoch 774/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4928 - accuracy: 0.0000e+00 - val_loss: 0.5709 - val_accuracy: 0.0000e+00\n",
      "Epoch 775/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4927 - accuracy: 0.0000e+00 - val_loss: 0.5709 - val_accuracy: 0.0000e+00\n",
      "Epoch 776/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4926 - accuracy: 0.0000e+00 - val_loss: 0.5709 - val_accuracy: 0.0000e+00\n",
      "Epoch 777/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4926 - accuracy: 0.0000e+00 - val_loss: 0.5709 - val_accuracy: 0.0000e+00\n",
      "Epoch 778/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4925 - accuracy: 0.0000e+00 - val_loss: 0.5709 - val_accuracy: 0.0000e+00\n",
      "Epoch 779/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4924 - accuracy: 0.0000e+00 - val_loss: 0.5708 - val_accuracy: 0.0000e+00\n",
      "Epoch 780/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4924 - accuracy: 0.0000e+00 - val_loss: 0.5708 - val_accuracy: 0.0000e+00\n",
      "Epoch 781/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4923 - accuracy: 0.0000e+00 - val_loss: 0.5708 - val_accuracy: 0.0000e+00\n",
      "Epoch 782/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4922 - accuracy: 0.0000e+00 - val_loss: 0.5708 - val_accuracy: 0.0000e+00\n",
      "Epoch 783/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4922 - accuracy: 0.0000e+00 - val_loss: 0.5708 - val_accuracy: 0.0000e+00\n",
      "Epoch 784/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4921 - accuracy: 0.0000e+00 - val_loss: 0.5708 - val_accuracy: 0.0000e+00\n",
      "Epoch 785/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4920 - accuracy: 0.0000e+00 - val_loss: 0.5707 - val_accuracy: 0.0000e+00\n",
      "Epoch 786/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4920 - accuracy: 0.0000e+00 - val_loss: 0.5707 - val_accuracy: 0.0000e+00\n",
      "Epoch 787/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4919 - accuracy: 0.0000e+00 - val_loss: 0.5707 - val_accuracy: 0.0000e+00\n",
      "Epoch 788/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4918 - accuracy: 0.0000e+00 - val_loss: 0.5707 - val_accuracy: 0.0000e+00\n",
      "Epoch 789/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4918 - accuracy: 0.0000e+00 - val_loss: 0.5707 - val_accuracy: 0.0000e+00\n",
      "Epoch 790/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4917 - accuracy: 0.0000e+00 - val_loss: 0.5707 - val_accuracy: 0.0000e+00\n",
      "Epoch 791/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4916 - accuracy: 0.0000e+00 - val_loss: 0.5706 - val_accuracy: 0.0000e+00\n",
      "Epoch 792/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4916 - accuracy: 0.0000e+00 - val_loss: 0.5706 - val_accuracy: 0.0000e+00\n",
      "Epoch 793/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4915 - accuracy: 0.0000e+00 - val_loss: 0.5706 - val_accuracy: 0.0000e+00\n",
      "Epoch 794/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4914 - accuracy: 0.0000e+00 - val_loss: 0.5706 - val_accuracy: 0.0000e+00\n",
      "Epoch 795/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4914 - accuracy: 0.0000e+00 - val_loss: 0.5706 - val_accuracy: 0.0000e+00\n",
      "Epoch 796/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4913 - accuracy: 0.0000e+00 - val_loss: 0.5706 - val_accuracy: 0.0000e+00\n",
      "Epoch 797/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4912 - accuracy: 0.0000e+00 - val_loss: 0.5706 - val_accuracy: 0.0000e+00\n",
      "Epoch 798/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4912 - accuracy: 0.0000e+00 - val_loss: 0.5705 - val_accuracy: 0.0000e+00\n",
      "Epoch 799/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4911 - accuracy: 0.0000e+00 - val_loss: 0.5705 - val_accuracy: 0.0000e+00\n",
      "Epoch 800/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4911 - accuracy: 0.0000e+00 - val_loss: 0.5705 - val_accuracy: 0.0000e+00\n",
      "Epoch 801/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4910 - accuracy: 0.0000e+00 - val_loss: 0.5705 - val_accuracy: 0.0000e+00\n",
      "Epoch 802/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4909 - accuracy: 0.0000e+00 - val_loss: 0.5705 - val_accuracy: 0.0000e+00\n",
      "Epoch 803/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4909 - accuracy: 0.0000e+00 - val_loss: 0.5705 - val_accuracy: 0.0000e+00\n",
      "Epoch 804/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4908 - accuracy: 0.0000e+00 - val_loss: 0.5704 - val_accuracy: 0.0000e+00\n",
      "Epoch 805/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4908 - accuracy: 0.0000e+00 - val_loss: 0.5704 - val_accuracy: 0.0000e+00\n",
      "Epoch 806/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4907 - accuracy: 0.0000e+00 - val_loss: 0.5704 - val_accuracy: 0.0000e+00\n",
      "Epoch 807/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4906 - accuracy: 0.0000e+00 - val_loss: 0.5704 - val_accuracy: 0.0000e+00\n",
      "Epoch 808/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4906 - accuracy: 0.0000e+00 - val_loss: 0.5704 - val_accuracy: 0.0000e+00\n",
      "Epoch 809/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4905 - accuracy: 0.0000e+00 - val_loss: 0.5704 - val_accuracy: 0.0000e+00\n",
      "Epoch 810/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4905 - accuracy: 0.0000e+00 - val_loss: 0.5704 - val_accuracy: 0.0000e+00\n",
      "Epoch 811/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4904 - accuracy: 0.0000e+00 - val_loss: 0.5703 - val_accuracy: 0.0000e+00\n",
      "Epoch 812/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4903 - accuracy: 0.0000e+00 - val_loss: 0.5703 - val_accuracy: 0.0000e+00\n",
      "Epoch 813/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4903 - accuracy: 0.0000e+00 - val_loss: 0.5703 - val_accuracy: 0.0000e+00\n",
      "Epoch 814/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4902 - accuracy: 0.0000e+00 - val_loss: 0.5703 - val_accuracy: 0.0000e+00\n",
      "Epoch 815/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4902 - accuracy: 0.0000e+00 - val_loss: 0.5703 - val_accuracy: 0.0000e+00\n",
      "Epoch 816/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4901 - accuracy: 0.0000e+00 - val_loss: 0.5703 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 817/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4901 - accuracy: 0.0000e+00 - val_loss: 0.5703 - val_accuracy: 0.0000e+00\n",
      "Epoch 818/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4900 - accuracy: 0.0000e+00 - val_loss: 0.5703 - val_accuracy: 0.0000e+00\n",
      "Epoch 819/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4899 - accuracy: 0.0000e+00 - val_loss: 0.5702 - val_accuracy: 0.0000e+00\n",
      "Epoch 820/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4899 - accuracy: 0.0000e+00 - val_loss: 0.5702 - val_accuracy: 0.0000e+00\n",
      "Epoch 821/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4898 - accuracy: 0.0000e+00 - val_loss: 0.5702 - val_accuracy: 0.0000e+00\n",
      "Epoch 822/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4898 - accuracy: 0.0000e+00 - val_loss: 0.5702 - val_accuracy: 0.0000e+00\n",
      "Epoch 823/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4897 - accuracy: 0.0000e+00 - val_loss: 0.5702 - val_accuracy: 0.0000e+00\n",
      "Epoch 824/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4896 - accuracy: 0.0000e+00 - val_loss: 0.5702 - val_accuracy: 0.0000e+00\n",
      "Epoch 825/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4896 - accuracy: 0.0000e+00 - val_loss: 0.5702 - val_accuracy: 0.0000e+00\n",
      "Epoch 826/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4895 - accuracy: 0.0000e+00 - val_loss: 0.5701 - val_accuracy: 0.0000e+00\n",
      "Epoch 827/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4895 - accuracy: 0.0000e+00 - val_loss: 0.5701 - val_accuracy: 0.0000e+00\n",
      "Epoch 828/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4894 - accuracy: 0.0000e+00 - val_loss: 0.5701 - val_accuracy: 0.0000e+00\n",
      "Epoch 829/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4894 - accuracy: 0.0000e+00 - val_loss: 0.5701 - val_accuracy: 0.0000e+00\n",
      "Epoch 830/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4893 - accuracy: 0.0000e+00 - val_loss: 0.5701 - val_accuracy: 0.0000e+00\n",
      "Epoch 831/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4893 - accuracy: 0.0000e+00 - val_loss: 0.5701 - val_accuracy: 0.0000e+00\n",
      "Epoch 832/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4892 - accuracy: 0.0000e+00 - val_loss: 0.5701 - val_accuracy: 0.0000e+00\n",
      "Epoch 833/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4892 - accuracy: 0.0000e+00 - val_loss: 0.5701 - val_accuracy: 0.0000e+00\n",
      "Epoch 834/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4891 - accuracy: 0.0000e+00 - val_loss: 0.5701 - val_accuracy: 0.0000e+00\n",
      "Epoch 835/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4891 - accuracy: 0.0000e+00 - val_loss: 0.5701 - val_accuracy: 0.0000e+00\n",
      "Epoch 836/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4890 - accuracy: 0.0000e+00 - val_loss: 0.5700 - val_accuracy: 0.0000e+00\n",
      "Epoch 837/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4890 - accuracy: 0.0000e+00 - val_loss: 0.5700 - val_accuracy: 0.0000e+00\n",
      "Epoch 838/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4889 - accuracy: 0.0000e+00 - val_loss: 0.5700 - val_accuracy: 0.0000e+00\n",
      "Epoch 839/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4889 - accuracy: 0.0000e+00 - val_loss: 0.5700 - val_accuracy: 0.0000e+00\n",
      "Epoch 840/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4888 - accuracy: 0.0000e+00 - val_loss: 0.5700 - val_accuracy: 0.0000e+00\n",
      "Epoch 841/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4887 - accuracy: 0.0000e+00 - val_loss: 0.5700 - val_accuracy: 0.0000e+00\n",
      "Epoch 842/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4887 - accuracy: 0.0000e+00 - val_loss: 0.5700 - val_accuracy: 0.0000e+00\n",
      "Epoch 843/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4886 - accuracy: 0.0000e+00 - val_loss: 0.5700 - val_accuracy: 0.0000e+00\n",
      "Epoch 844/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4886 - accuracy: 0.0000e+00 - val_loss: 0.5700 - val_accuracy: 0.0000e+00\n",
      "Epoch 845/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4885 - accuracy: 0.0000e+00 - val_loss: 0.5700 - val_accuracy: 0.0000e+00\n",
      "Epoch 846/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4885 - accuracy: 0.0000e+00 - val_loss: 0.5700 - val_accuracy: 0.0000e+00\n",
      "Epoch 847/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4884 - accuracy: 0.0000e+00 - val_loss: 0.5699 - val_accuracy: 0.0000e+00\n",
      "Epoch 848/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4884 - accuracy: 0.0000e+00 - val_loss: 0.5699 - val_accuracy: 0.0000e+00\n",
      "Epoch 849/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4883 - accuracy: 0.0000e+00 - val_loss: 0.5699 - val_accuracy: 0.0000e+00\n",
      "Epoch 850/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4883 - accuracy: 0.0000e+00 - val_loss: 0.5699 - val_accuracy: 0.0000e+00\n",
      "Epoch 851/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4882 - accuracy: 0.0000e+00 - val_loss: 0.5699 - val_accuracy: 0.0000e+00\n",
      "Epoch 852/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4882 - accuracy: 0.0000e+00 - val_loss: 0.5699 - val_accuracy: 0.0000e+00\n",
      "Epoch 853/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4881 - accuracy: 0.0000e+00 - val_loss: 0.5699 - val_accuracy: 0.0000e+00\n",
      "Epoch 854/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4881 - accuracy: 0.0000e+00 - val_loss: 0.5699 - val_accuracy: 0.0000e+00\n",
      "Epoch 855/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4880 - accuracy: 0.0000e+00 - val_loss: 0.5699 - val_accuracy: 0.0000e+00\n",
      "Epoch 856/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4880 - accuracy: 0.0000e+00 - val_loss: 0.5699 - val_accuracy: 0.0000e+00\n",
      "Epoch 857/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4880 - accuracy: 0.0000e+00 - val_loss: 0.5699 - val_accuracy: 0.0000e+00\n",
      "Epoch 858/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4879 - accuracy: 0.0000e+00 - val_loss: 0.5699 - val_accuracy: 0.0000e+00\n",
      "Epoch 859/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4879 - accuracy: 0.0000e+00 - val_loss: 0.5699 - val_accuracy: 0.0000e+00\n",
      "Epoch 860/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4878 - accuracy: 0.0000e+00 - val_loss: 0.5699 - val_accuracy: 0.0000e+00\n",
      "Epoch 861/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4878 - accuracy: 0.0000e+00 - val_loss: 0.5699 - val_accuracy: 0.0000e+00\n",
      "Epoch 862/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4877 - accuracy: 0.0000e+00 - val_loss: 0.5699 - val_accuracy: 0.0000e+00\n",
      "Epoch 863/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4877 - accuracy: 0.0000e+00 - val_loss: 0.5699 - val_accuracy: 0.0000e+00\n",
      "Epoch 864/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4876 - accuracy: 0.0000e+00 - val_loss: 0.5699 - val_accuracy: 0.0000e+00\n",
      "Epoch 865/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4876 - accuracy: 0.0000e+00 - val_loss: 0.5699 - val_accuracy: 0.0000e+00\n",
      "Epoch 866/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4875 - accuracy: 0.0000e+00 - val_loss: 0.5699 - val_accuracy: 0.0000e+00\n",
      "Epoch 867/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4875 - accuracy: 0.0000e+00 - val_loss: 0.5699 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 868/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4875 - accuracy: 0.0000e+00 - val_loss: 0.5699 - val_accuracy: 0.0000e+00\n",
      "Epoch 869/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4874 - accuracy: 0.0000e+00 - val_loss: 0.5698 - val_accuracy: 0.0000e+00\n",
      "Epoch 870/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4874 - accuracy: 0.0000e+00 - val_loss: 0.5699 - val_accuracy: 0.0000e+00\n",
      "Epoch 871/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4873 - accuracy: 0.0000e+00 - val_loss: 0.5698 - val_accuracy: 0.0000e+00\n",
      "Epoch 872/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4873 - accuracy: 0.0000e+00 - val_loss: 0.5699 - val_accuracy: 0.0000e+00\n",
      "Epoch 873/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4872 - accuracy: 0.0000e+00 - val_loss: 0.5698 - val_accuracy: 0.0000e+00\n",
      "Epoch 874/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4872 - accuracy: 0.0000e+00 - val_loss: 0.5698 - val_accuracy: 0.0000e+00\n",
      "Epoch 875/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4871 - accuracy: 0.0000e+00 - val_loss: 0.5698 - val_accuracy: 0.0000e+00\n",
      "Epoch 876/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4871 - accuracy: 0.0000e+00 - val_loss: 0.5698 - val_accuracy: 0.0000e+00\n",
      "Epoch 877/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4871 - accuracy: 0.0000e+00 - val_loss: 0.5698 - val_accuracy: 0.0000e+00\n",
      "Epoch 878/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4870 - accuracy: 0.0000e+00 - val_loss: 0.5698 - val_accuracy: 0.0000e+00\n",
      "Epoch 879/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4870 - accuracy: 0.0000e+00 - val_loss: 0.5698 - val_accuracy: 0.0000e+00\n",
      "Epoch 880/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4869 - accuracy: 0.0000e+00 - val_loss: 0.5698 - val_accuracy: 0.0000e+00\n",
      "Epoch 881/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4869 - accuracy: 0.0000e+00 - val_loss: 0.5698 - val_accuracy: 0.0000e+00\n",
      "Epoch 882/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4868 - accuracy: 0.0000e+00 - val_loss: 0.5698 - val_accuracy: 0.0000e+00\n",
      "Epoch 883/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4868 - accuracy: 0.0000e+00 - val_loss: 0.5698 - val_accuracy: 0.0000e+00\n",
      "Epoch 884/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4868 - accuracy: 0.0000e+00 - val_loss: 0.5698 - val_accuracy: 0.0000e+00\n",
      "Epoch 885/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4867 - accuracy: 0.0000e+00 - val_loss: 0.5698 - val_accuracy: 0.0000e+00\n",
      "Epoch 886/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4867 - accuracy: 0.0000e+00 - val_loss: 0.5698 - val_accuracy: 0.0000e+00\n",
      "Epoch 887/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4866 - accuracy: 0.0000e+00 - val_loss: 0.5698 - val_accuracy: 0.0000e+00\n",
      "Epoch 888/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4866 - accuracy: 0.0000e+00 - val_loss: 0.5698 - val_accuracy: 0.0000e+00\n",
      "Epoch 889/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4865 - accuracy: 0.0000e+00 - val_loss: 0.5697 - val_accuracy: 0.0000e+00\n",
      "Epoch 890/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4865 - accuracy: 0.0000e+00 - val_loss: 0.5697 - val_accuracy: 0.0000e+00\n",
      "Epoch 891/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4865 - accuracy: 0.0000e+00 - val_loss: 0.5697 - val_accuracy: 0.0000e+00\n",
      "Epoch 892/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4864 - accuracy: 0.0000e+00 - val_loss: 0.5697 - val_accuracy: 0.0000e+00\n",
      "Epoch 893/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4864 - accuracy: 0.0000e+00 - val_loss: 0.5697 - val_accuracy: 0.0000e+00\n",
      "Epoch 894/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4863 - accuracy: 0.0000e+00 - val_loss: 0.5697 - val_accuracy: 0.0000e+00\n",
      "Epoch 895/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4863 - accuracy: 0.0000e+00 - val_loss: 0.5697 - val_accuracy: 0.0000e+00\n",
      "Epoch 896/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4863 - accuracy: 0.0000e+00 - val_loss: 0.5697 - val_accuracy: 0.0000e+00\n",
      "Epoch 897/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4862 - accuracy: 0.0000e+00 - val_loss: 0.5697 - val_accuracy: 0.0000e+00\n",
      "Epoch 898/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4862 - accuracy: 0.0000e+00 - val_loss: 0.5697 - val_accuracy: 0.0000e+00\n",
      "Epoch 899/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4862 - accuracy: 0.0000e+00 - val_loss: 0.5697 - val_accuracy: 0.0000e+00\n",
      "Epoch 900/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4861 - accuracy: 0.0000e+00 - val_loss: 0.5697 - val_accuracy: 0.0000e+00\n",
      "Epoch 901/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4861 - accuracy: 0.0000e+00 - val_loss: 0.5697 - val_accuracy: 0.0000e+00\n",
      "Epoch 902/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4861 - accuracy: 0.0000e+00 - val_loss: 0.5697 - val_accuracy: 0.0000e+00\n",
      "Epoch 903/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4860 - accuracy: 0.0000e+00 - val_loss: 0.5697 - val_accuracy: 0.0000e+00\n",
      "Epoch 904/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4860 - accuracy: 0.0000e+00 - val_loss: 0.5697 - val_accuracy: 0.0000e+00\n",
      "Epoch 905/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4859 - accuracy: 0.0000e+00 - val_loss: 0.5697 - val_accuracy: 0.0000e+00\n",
      "Epoch 906/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4859 - accuracy: 0.0000e+00 - val_loss: 0.5697 - val_accuracy: 0.0000e+00\n",
      "Epoch 907/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4858 - accuracy: 0.0000e+00 - val_loss: 0.5697 - val_accuracy: 0.0000e+00\n",
      "Epoch 908/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4858 - accuracy: 0.0000e+00 - val_loss: 0.5696 - val_accuracy: 0.0000e+00\n",
      "Epoch 909/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4858 - accuracy: 0.0000e+00 - val_loss: 0.5696 - val_accuracy: 0.0000e+00\n",
      "Epoch 910/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4857 - accuracy: 0.0000e+00 - val_loss: 0.5697 - val_accuracy: 0.0000e+00\n",
      "Epoch 911/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4857 - accuracy: 0.0000e+00 - val_loss: 0.5696 - val_accuracy: 0.0000e+00\n",
      "Epoch 912/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4857 - accuracy: 0.0000e+00 - val_loss: 0.5696 - val_accuracy: 0.0000e+00\n",
      "Epoch 913/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4856 - accuracy: 0.0000e+00 - val_loss: 0.5696 - val_accuracy: 0.0000e+00\n",
      "Epoch 914/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4856 - accuracy: 0.0000e+00 - val_loss: 0.5696 - val_accuracy: 0.0000e+00\n",
      "Epoch 915/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4855 - accuracy: 0.0000e+00 - val_loss: 0.5696 - val_accuracy: 0.0000e+00\n",
      "Epoch 916/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4855 - accuracy: 0.0000e+00 - val_loss: 0.5696 - val_accuracy: 0.0000e+00\n",
      "Epoch 917/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4855 - accuracy: 0.0000e+00 - val_loss: 0.5696 - val_accuracy: 0.0000e+00\n",
      "Epoch 918/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4854 - accuracy: 0.0000e+00 - val_loss: 0.5696 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 919/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4854 - accuracy: 0.0000e+00 - val_loss: 0.5696 - val_accuracy: 0.0000e+00\n",
      "Epoch 920/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4854 - accuracy: 0.0000e+00 - val_loss: 0.5696 - val_accuracy: 0.0000e+00\n",
      "Epoch 921/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4853 - accuracy: 0.0000e+00 - val_loss: 0.5696 - val_accuracy: 0.0000e+00\n",
      "Epoch 922/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4853 - accuracy: 0.0000e+00 - val_loss: 0.5696 - val_accuracy: 0.0000e+00\n",
      "Epoch 923/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4852 - accuracy: 0.0000e+00 - val_loss: 0.5696 - val_accuracy: 0.0000e+00\n",
      "Epoch 924/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4852 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 925/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4852 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 926/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4851 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 927/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4851 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 928/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4851 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 929/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4850 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 930/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4850 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 931/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4850 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 932/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4849 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 933/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4849 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 934/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4849 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 935/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4848 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 936/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4848 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 937/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4848 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 938/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4847 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 939/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4847 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 940/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4847 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 941/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4846 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 942/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4846 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 943/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4846 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 944/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4845 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 945/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4845 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 946/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4845 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 947/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4844 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 948/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4844 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 949/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4844 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 950/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4843 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 951/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4843 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 952/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4843 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 953/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4842 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 954/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4842 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 955/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4842 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 956/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4841 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 957/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4841 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 958/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4841 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 959/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4841 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 960/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4840 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 961/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4840 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 962/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4840 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 963/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4839 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 964/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4839 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 965/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4839 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 966/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4838 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 967/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4838 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 968/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4838 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 969/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4837 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 970/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4837 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 971/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4837 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 972/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4837 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 973/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4836 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 974/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4836 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 975/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4836 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 976/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4835 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 977/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4835 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 978/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4835 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 979/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4835 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 980/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4834 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 981/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4834 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 982/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4834 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 983/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4834 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 984/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4833 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 985/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4833 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 986/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4833 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 987/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4832 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 988/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4832 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 989/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4832 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 990/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4831 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 991/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4831 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 992/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4831 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 993/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4831 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 994/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4830 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 995/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4830 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 996/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4830 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 997/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4830 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 998/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4829 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 999/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4829 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 1000/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4829 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 1001/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4828 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 1002/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4828 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 1003/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4828 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 1004/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4828 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 1005/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4827 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 1006/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4827 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 1007/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4827 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 1008/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4827 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 1009/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4826 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 1010/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4826 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 1011/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4826 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 1012/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4826 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 1013/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4825 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 1014/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4825 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 1015/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4825 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 1016/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4825 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 1017/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4824 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 1018/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4824 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 1019/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4824 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 1020/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4824 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1021/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4823 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 1022/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4823 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 1023/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4823 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 1024/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4823 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 1025/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4822 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 1026/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4822 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 1027/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4822 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 1028/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4822 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 1029/10000\n",
      "1791000/1791000 [==============================] - 2s 1us/step - loss: 0.4821 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "TR AVG =  0.5165994\n",
      "TR R2  =  0.854992681619856\n",
      "TR MAE =  0.09345584426849315\n",
      "TR ACC =  0.9009664991624791\n",
      "TR AGT =  0.6815226130653267\n",
      "VA AVG =  0.51518303\n",
      "VA R2  =  0.5411088578836238\n",
      "VA MAE =  0.16346617700320754\n",
      "VA ACC =  0.7906231155778894\n",
      "VA AGT =  0.6486733668341709\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.90      0.90      0.90    863409\n",
      "       True       0.91      0.90      0.90    927591\n",
      "\n",
      "avg / total       0.90      0.90      0.90   1791000\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.78      0.79      0.78     96007\n",
      "       True       0.80      0.79      0.80    102993\n",
      "\n",
      "avg / total       0.79      0.79      0.79    199000\n",
      "\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "#tw should be ~U[0.5, 3.5]\n",
    "#sw should be ~N[0, sd] with sd ~U[1, 3.5]\n",
    "#a0 should be ~U[-0.5, 1]\n",
    "#missing proportion should be ~U[0, 0.3]\n",
    "\n",
    "# from tensorflow.random import set_seed\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "explore_mode = True\n",
    "\n",
    "reportz=[]\n",
    "\n",
    "# factors_master = [(10,1,5)]\n",
    "factors_master = [(100,1,5)]\n",
    "w_list = [128]\n",
    "factors_list = [ m+(w,) for m in factors_master for w in w_list ]\n",
    "\n",
    "# nn_modes = [\"MLTM\",\"COND\",\"MXFN\"]\n",
    "nn_modes = [\"DEEP\"]\n",
    "loss_modes = [\"XENT\"]\n",
    "# sq_nums = [(int(1000*(1.4)**3), int(150*(1.4)**3))]\n",
    "sq_nums = [(10000, 200)]\n",
    "# sq_nums = [(10000, 150)]\n",
    "# student_staminas = [0.01, 0.1, 0.5, 0.75, 1.0]\n",
    "\n",
    "spars_list = [1.0] # [0.01, 0.05, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "n_runs = 1\n",
    "bal = .5\n",
    "\n",
    "for (n_students, n_questions) in sq_nums:\n",
    "    print(\"{} students, {} questions\".format(n_students, n_questions))\n",
    "    for nn_mode in nn_modes:\n",
    "        for loss_mode in loss_modes:\n",
    "            for (n_factors, min_active, max_active, emb_w) in factors_list:\n",
    "                for spars in spars_list:\n",
    "\n",
    "                    model_list=[]\n",
    "                    rasch=True\n",
    "\n",
    "                    questions=None\n",
    "\n",
    "                    tup = ((n_factors, min_active, max_active), (10000, 70))\n",
    "                    if tup in gen_m_cache:\n",
    "                        (gen_m, history, best_dims, best_mse) = gen_m_cache[tup]\n",
    "                    else:\n",
    "                        print(gen_m_cache.keys())\n",
    "                        raise Exception(\"Genny not found for\",tup)\n",
    "                    \n",
    "                    qws_list = []\n",
    "                    sws_list = []\n",
    "                    tr_list = []\n",
    "                    params_list = []\n",
    "                    # questions=None\n",
    "                    real_stu_list=[]\n",
    "                    real_que_list=[]\n",
    "                    perseverance_list=[]\n",
    "                    test_datasets=[]\n",
    "                    sparss = []\n",
    "    #                 qn_av = None\n",
    "    #                 qn_std = None\n",
    "\n",
    "                    pred_list = []\n",
    "\n",
    "#                     set_seed(666)\n",
    "                    numpy.random.seed(666)\n",
    "                    for a in range(n_runs):\n",
    "\n",
    "                        found = False\n",
    "                        while not found:\n",
    "                            tw = random.uniform(0.5, 3.5)\n",
    "    #                         a1 = random.uniform(1, 3.5)\n",
    "                            a1 = random.uniform(1, 5)\n",
    "                            a0 = gen_m.predict(numpy.array([[tw,a1, bal]]).reshape(1,-1))\n",
    "\n",
    "                            gen_new_data = False\n",
    "                            if rasch:\n",
    "                                if tup not in data_cache:\n",
    "                                    print(\"Not data in cache for desired shape\")\n",
    "                                    gen_new_data = True\n",
    "                                else:\n",
    "                                    print(\"using datacache\")\n",
    "                                    (probs, students_temp, qz_temp) = data_cache[tup]\n",
    "                                    if students_temp.shape != (n_students, n_factors) or qz_temp.shape!=(n_questions, n_factors):\n",
    "                                        print(\"data shape has changed, need to make new data\")\n",
    "                                        gen_new_data = True\n",
    "                                \n",
    "                                if gen_new_data:\n",
    "                                    print(\"gening data\")\n",
    "                                    _, probs, students_temp, qz_temp  = gen_rasch_run(n_factors, a0, a1, min_active, max_active, test_w = tw, n_students=n_students, n_questions=n_questions)\n",
    "                                \n",
    "\n",
    "                                \n",
    "                            print(\"run created\")\n",
    "                            students2 = students_temp\n",
    "                            questions = qz_temp\n",
    "\n",
    "                            if explore_mode:\n",
    "                                plot_items([], questions, None)\n",
    "\n",
    "                                print(\"~ ~ ~ ~~ ATTEMPT\",a, a0)\n",
    "                                bin_spread = lambda x: max(1,int(abs(2*(numpy.max(x)-numpy.min(x)))))\n",
    "\n",
    "                                plt.hist(students2.flatten(), alpha=0.5, bins=bin_spread(students2))\n",
    "                                plt.hist(questions.flatten(), alpha=0.5, bins=bin_spread(questions))\n",
    "                                plt.show()\n",
    "\n",
    "                        #     (sz2,qz2,pfz2), (vsz2,vqz2,vpfz2), (tsz2,tqz2,tpfz2), obs2, probs2 = tvt_split(students2, questions, split_mode=1)\n",
    "                        #     tr_list.append(((sz2,qz2,pfz2), (vsz2,vqz2,vpfz2), (tsz2,tqz2,tpfz2)))\n",
    "\n",
    "\n",
    "    #                         probs=numpy.zeros((len(students2), len(questions)))\n",
    "                            obs=numpy.zeros((len(students2), len(questions)))\n",
    "\n",
    "                            all_pairs = []\n",
    "                            tr_pairs = []\n",
    "                            v_pairs = []\n",
    "                            tt_pairs = []\n",
    "                            perseverance = []\n",
    "                            slist = list(range(len(students2)))\n",
    "                            random.seed(666)\n",
    "                            shuffle(slist)\n",
    "                            for vi in slist:\n",
    "    #                             c=0\n",
    "    #                             p_cont = (n_students * n_questions)//20\n",
    "    #                             v_size = p_cont\n",
    "                                qlist= list(range(len(questions)))\n",
    "                                shuffle(qlist)\n",
    "                                first = True\n",
    "                                for mi in qlist:\n",
    "                                    if first:\n",
    "                                        tt_pairs.append((vi,mi))\n",
    "                                        first = False\n",
    "                                    else:\n",
    "                                        tr_pairs.append((vi,mi))\n",
    "\n",
    "                            print(\"splitting\")\n",
    "                            if spars < 1:\n",
    "                                tr_pairs, _ = train_test_split(tr_pairs, train_size=spars)\n",
    "                            tr_pairs, v_pairs = train_test_split(tr_pairs, test_size=0.1)\n",
    "                            print(\"splut\")\n",
    "\n",
    "#                             print(\"scanning\")\n",
    "#                             for pa in tr_pairs:\n",
    "#     #                             print(pa)\n",
    "#                                 if pa in tt_pairs:\n",
    "#                                     print(\"TR IN TT\")\n",
    "#                                     raise Exception\n",
    "#                                 if pa in v_pairs:\n",
    "#                                     print(\"TR IN V\")\n",
    "#                                     raise Exception\n",
    "#                             print(\"scun\")\n",
    "\n",
    "    #                         print(\"tr_pairs\", tr_pairs)\n",
    "                            pfz, sz, qz = stitch_n_split(tr_pairs, probs)\n",
    "                            vpfz, vsz, vqz = stitch_n_split(v_pairs, probs)\n",
    "#                             vpfz, vsz, vqz = [],[],[]\n",
    "\n",
    "                            print(\"Sparsity\",spars,\"lens of pfz and vpfz, tt_pairs\", len(pfz), len(vpfz), len(tt_pairs))\n",
    "\n",
    "                            print(probs)\n",
    "\n",
    "                            hard =numpy.round(probs)\n",
    "                            agt = 0 #numpy.zeros_like(probs)\n",
    "                            n_agt_runs = 10\n",
    "                            for _ in range(n_agt_runs):\n",
    "                                this_obs  = (numpy.random.random(probs.shape) < probs).astype(int)\n",
    "                                this_agt = numpy.mean((hard==this_obs).astype(int).flatten())\n",
    "                                agt += this_agt / n_agt_runs\n",
    "                            print(\"*** AGT:\", agt)                            \n",
    "\n",
    "                            if explore_mode:\n",
    "                                plt.hist(probs.flatten(), alpha=0.5)\n",
    "                                plt.title(\"Histogram of $p_{pass}$\")\n",
    "                                plt.xlabel(\"$p_{pass}\")\n",
    "                                plt.ylabel(\"Frequency\")\n",
    "                                plt.legend()\n",
    "                                plt.show()\n",
    "\n",
    "                                plt.hist(numpy.array(pfz).flatten(), alpha=0.5)\n",
    "                                plt.title(\"pfz\")\n",
    "                                plt.show()\n",
    "\n",
    "\n",
    "                            print(tw, a1, a0)\n",
    "                            mn = numpy.mean(pfz)\n",
    "                            print(mn, numpy.mean(vpfz))\n",
    "                            uppa = bal+0.05\n",
    "                            lowa = bal-0.05\n",
    "                            print(\"xhxwx ELIGIBLE SPREAD for\",mn,bal, uppa, lowa)\n",
    "                            if (mn >= lowa) and (mn <= uppa):\n",
    "                                print(\"FOUND ELIGIBLE SPREAD for\",mn,bal, uppa, lowa)\n",
    "                                data_cache[tup] = (probs, students_temp, qz_temp)\n",
    "                                found=True\n",
    "\n",
    "                        hard=None\n",
    "                        probs=None\n",
    "\n",
    "                        print(\"mean pers is\", numpy.mean(perseverance))\n",
    "                        perseverance_list.append(perseverance)\n",
    "                        real_stu_list.append(students2)\n",
    "                        real_que_list.append(questions)\n",
    "                        test_datasets.append(tt_pairs)\n",
    "                        params_list.append((tw,a1,a0,numpy.mean(pfz), numpy.mean(vpfz), agt))\n",
    "                    #     if numpy.mean(pfz) <0.4 or numpy.mean(pfz)>0.6:\n",
    "                    #         continue\n",
    "\n",
    "                    # for runix in range(n_runs):\n",
    "                    #     (sz2,qz2,pfz2), (vsz2,vqz2,vpfz2), (tsz2,tqz2,tpfz2) = tr_list[runix]\n",
    "                        obs_are_binary = numpy.array_equal(numpy.array(pfz).flatten(), numpy.array(pfz).flatten().astype(bool))\n",
    "                        print(\"binary obs?\", obs_are_binary)\n",
    "\n",
    "                        print(\"callio:\")\n",
    "                        print(len(qz),len(sz),len(pfz))\n",
    "                        print(len(vqz),len(vsz),len(vpfz), emb_w)\n",
    "    #                     nn_mode = \"MLTM\"\n",
    "    #                     loss_mode = \"XENT\"\n",
    "                        print(\"nn_mode\", nn_mode)\n",
    "                        s_table2, qn_table2, m2, h2 = generate_and_train(n_students, n_questions, qz,sz,pfz, vqz,vsz,vpfz, emb_w, n_factors, min_active, max_active, nn_mode=nn_mode, loss_mode=loss_mode)\n",
    "    #                     qws2= copy.copy(qn_table2.get_weights()[0])\n",
    "    #                     sws2= copy.copy(s_table2.get_weights()[0])\n",
    "                        qws2= qn_table2.get_weights()[0]\n",
    "                        sws2= s_table2.get_weights()[0]\n",
    "\n",
    "                        pred_probs = m2.predict([qz, sz])\n",
    "    #                     print(pred_probs)\n",
    "                        pred_list.append(pred_probs)\n",
    "                        model_list.append(m2)\n",
    "\n",
    "                    #     qg = q_gates.get_weights()[0]\n",
    "                    #     qg_list.append(qg)\n",
    "                    #     if qn_av is None:\n",
    "                    #         qn_av = numpy.mean(qws2)\n",
    "\n",
    "                        sparss.append(spars)\n",
    "                        sws_list.append(sws2)\n",
    "                        qws_list.append(qws2)\n",
    "                    tup = (n_factors, min_active, max_active, emb_w, nn_mode, loss_mode, sws_list, qws_list, model_list, real_stu_list, real_que_list, test_datasets, params_list, sparss)\n",
    "    #                 reportz.append(zlib.compress(pickle.dumps(tup)))\n",
    "    #                 print(perseverance_list)\n",
    "\n",
    "                    reportz.append(tup)\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****\n",
      "DEEP XENT\n",
      "1 1 1 1 1 1 1\n",
      "params: 100 1 5 128 / 0.5636739213509128 4.2794227824212765 [[2.079]] ( 0.516223827849955 0.5157171235946164 0.7911066499999999 ) [ 1.0 ]\n",
      "R2 =  0.424600231408099\n",
      "MAE =  0.18511217531097632\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7c8c53978>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7ca812940>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 x 200\n",
      "0.7566 0.0 / 0.18511218 0.1640905\n"
     ]
    }
   ],
   "source": [
    "for tup in reportz:\n",
    "#     tup = pickle.loads(zlib.decompress(tup_cmp))\n",
    "    report(*tup)\n",
    "    \n",
    "# qws = reportz[0][7][0]\n",
    "# real_qws = reportz[0][10][0]\n",
    "# print(qws)\n",
    "# numpy.set_printoptions(threshold=10000)\n",
    "# print(real_qws)\n",
    "\n",
    "# *****\n",
    "# MLTM XENT\n",
    "# 1 1 1 1 1 1 1\n",
    "# params: 10 1 5 1 / 2.5806077740610034 2.347493116108512 [[ 0.821]] 0.459641891892 0.495\n",
    "# R2 =  0.447788537896\n",
    "# MAE =  0.204465762654\n",
    "# 1000 x 150\n",
    "# 0.749 0.0 / 0.204466 0.144075\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sws2= s_table2.get_weights()[0]\n",
    "# qws2= qn_table2.get_weights()[0]\n",
    "\n",
    "# offset = numpy.mean(students2) - numpy.mean(sws2) \n",
    "# qws2 = qws2# + offset\n",
    "# sws2 = sws2# + offset\n",
    "\n",
    "bin_spread = lambda x: max(1,int(abs(2*(numpy.max(x)-numpy.min(x)))))\n",
    "\n",
    "for ss in real_stu_list:\n",
    "    plt.hist(ss.flatten(), alpha=0.2, bins=bin_spread(ss), label=\"s true\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "print(\"There are {} els in sws_list and {} els in qws_list\".format(len(sws_list), len(qws_list)))\n",
    "\n",
    "sw_av_list = []\n",
    "for sw in sws_list:\n",
    "#     (sz,qz,pfz), (vsz,vqz,vpfz), (tsz,tqz,tpfz) = tups\n",
    "    plt.hist(sw.flatten(), alpha=0.2, bins=bin_spread(sw), label=\"s pred\")\n",
    "    sw_av_list.append(numpy.median(sw.flatten()))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for qnz in real_que_list:\n",
    "    plt.hist(qnz.flatten(), alpha=0.2, label=\"q true\", bins=bin_spread(questions))\n",
    "\n",
    "# qws_list_2 = []\n",
    "real_max_q = numpy.max(questions.flatten())\n",
    "print(\"len qws_list\", len(qws_list))\n",
    "\n",
    "for qw in qws_list:#, sw_av_list):\n",
    "    qw = copy.copy(qw)\n",
    "    print(\"med\",numpy.median(qw), \"for shape\", qw.shape)\n",
    "#     (sz,qz,pfz), (vsz,vqz,vpfz), (tsz,tqz,tpfz) = tups\n",
    "#     qg2 = (qg>0.5).astype(int)\n",
    "#     masqd = qw*qg2\n",
    "#     qws_list_2.append(masqd)\n",
    "#     print(qw)\n",
    "#     print(qg)\n",
    "#     plt.hist(qw[list(set(qz))].flatten(), alpha=0.4, bins=bin_spread(qw)) \n",
    "\n",
    "#     max_q = numpy.max(qw)\n",
    "#     os = real_max_q - max_q\n",
    "#     qw += os\n",
    "\n",
    "#     thresh = 0\n",
    "#     qw[qw < thresh] = 0\n",
    "    plt.hist(qw.flatten(), alpha=0.2, label=\"q pred\", bins=bin_spread(qw))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# qws_list = qws_list2\n",
    "\n",
    "print(len(pred_list))\n",
    "plt.hist(pfz, alpha=0.2, label=\"true obs\")\n",
    "plt.hist(probs.flatten(), alpha=0.3, label=\"true probs\")\n",
    "for ix,predz in enumerate(pred_list):\n",
    "    plt.hist(predz, alpha=0.1, label=str(ix))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_items(qws_list[1:], qws_list[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qw in qws_list:#, sw_av_list):\n",
    "    print(qw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_ensemble(ref, pred_list):\n",
    "    summage = numpy.zeros_like(ref)\n",
    "    for items in pred_list:\n",
    "        items_aligned, min_total_err, total_q_err, total_s_err, mean_ll, best_cos_dis = calc_arr_arr_err(0, ref, items, max_iter=10)        \n",
    "        print(\"err\", numpy.mean(numpy.abs(items_aligned - ref)))\n",
    "        summage += items_aligned\n",
    "    summage /= len(pred_list)\n",
    "    print(\"ensem err\", numpy.mean(numpy.abs(summage - ref)))\n",
    "    return summage\n",
    "#         for item, real_item in zip(items, real_items):\n",
    "#             print(numpy.sort(real_item))\n",
    "#             print(numpy.sort(item))\n",
    "#             print()\n",
    "            \n",
    "\n",
    "mean_en = mean_ensemble(qws_list[0], qws_list[1:])\n",
    "plot_items([], questions, None)\n",
    "plot_items(qws_list, mean_en, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(m_list)\n",
    "for m,tr in zip(m_list, tr_list):\n",
    "    (sz2,qz2,pfz2), (vsz2,vqz2,vpfz2), (tsz2,tqz2,tpfz2) = tr\n",
    "    print(sz2,qz2,pfz2)\n",
    "    print(vsz2,vqz2,vpfz2)\n",
    "    print(tsz2,tqz2,tpfz2)\n",
    "    preds = m.predict(x=[qz2,sz2])\n",
    "    for sc_true, sc_hat in zip(pfz2,preds):\n",
    "        print(sc_true, sc_hat)\n",
    "\n",
    "    # print(m.evaluate(x=[mz,vz], y=scz))\n",
    "    from sklearn.metrics import mean_absolute_error, mean_absolute_error\n",
    "    print(mean_absolute_error(numpy.around(pfz2), numpy.around(preds)  ))\n",
    "    \n",
    "plt.hist(pfz2)\n",
    "plt.show()\n",
    "plt.hist(preds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "min_loss = math.inf\n",
    "min_v_loss = math.inf\n",
    "init_patience = 5\n",
    "for i in range(100):\n",
    "    print(\"shiteration i=\",i)\n",
    "    h = m.fit(x=[mz,vz], y=scz, batch_size=1000, epochs=2, shuffle=True, validation_split=1000/len(scz), verbose=1)\n",
    "    val_loss = h.history[\"val_loss\"][-1]\n",
    "    loss = h.history[\"loss\"][-1]\n",
    "    if loss < min_loss and val_loss <= min_v_loss:\n",
    "        min_v_loss = val_loss\n",
    "        min_loss = loss\n",
    "        print(\"patience reset\")\n",
    "        patience = init_patience\n",
    "#         sw = s_table.get_weights()\n",
    "#         qw = qn_table.get_weights()\n",
    "    else:\n",
    "        patience -= 1\n",
    "    if patience==0:\n",
    "        print(\"DONE\")\n",
    "        break\n",
    "#     m.fit(x=[mz,vz], y=numpy.array([(0.5+random.uniform(-0.5,0.5)) for _ in scz]).reshape(-1,1), batch_size=1000, shuffle=False, epochs=10, verbose=0)\n",
    "    m.fit(x=[mz,vz], y=numpy.array([0.5 for _ in scz]).reshape(-1,1), batch_size=1000, shuffle=False, epochs=2, verbose=0)\n",
    "    \n",
    "# s_table.set_weights(sw)\n",
    "# qn_table.set_weights(qw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor=\"val_loss\", restore_best_weights=True, patience=10)\n",
    "m.fit(x=[mz,vz], y=scz, batch_size=1000, epochs=100, validation_split=1000/len(scz))#, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = m.predict(x=[tmz,tvz])\n",
    "for p, sc_obsv, sc_hat in zip(t_probz, tscz,preds):\n",
    "    print(p, sc_obsv, sc_hat, (numpy.around(sc_obsv)==numpy.around(sc_hat)))\n",
    "\n",
    "# print(m.evaluate(x=[tmz,tvz], y=tscz))\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error\n",
    "\n",
    "print(\"obvsd acc\", accuracy_score(numpy.around(tscz), numpy.around(preds)))\n",
    "print(\"non-stoch acc\", accuracy_score(numpy.around(t_probz), numpy.around(preds)))\n",
    "print(mean_absolute_error(t_probz, preds))\n",
    "#0.000302638761699 MSE MxMul\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    plt.hist(obs.flatten())\n",
    "    plt.show()\n",
    "    print(numpy.sum(numpy.around(obs)))\n",
    "    print(len(obs.flatten()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_wgts = qn_table.get_weights()[0]\n",
    "real_wgts = movies\n",
    "\n",
    "split = 0\n",
    "\n",
    "items_chosen, min_total_err, total_q_err, total_s_err, mean_ll, best_cos_dis = calc_arr_arr_err(0, real_wgts, pred_wgts, max_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min_total_err, total_q_err, total_s_err)\n",
    "\n",
    "print(items_chosen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fig = plt.gcf()\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "# pca = TSNE(n_components=2)\n",
    "\n",
    "itemz_pred = items_chosen\n",
    "n = len(items_chosen)\n",
    "itemz = movies\n",
    "\n",
    "# s_pred_mean = numpy.mean(s_table.get_weights()[0])\n",
    "base = min( numpy.min(itemz_pred), numpy.min(itemz))\n",
    "# ss1 = StandardScaler()\n",
    "# itemz_pred = ss1.fit_transform(itemz_pred)\n",
    "# itemz = ss1.transform(movies)\n",
    "\n",
    "itemz = itemz - base\n",
    "itemz_pred = itemz_pred - base\n",
    "\n",
    "print(itemz)\n",
    "print(itemz_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itemz_pred = pca.transform(itemz_pred)\n",
    "\n",
    "itemz_2 = numpy.concatenate([itemz, itemz_pred], axis=0)\n",
    "# itemz_2 = itemz\n",
    "itemz_2 = pca.fit_transform(itemz_2)\n",
    "\n",
    "# itemz_2 = MinMaxScaler().fit_transform(itemz_2)\n",
    "\n",
    "# ixes = itemz_pred < baseline\n",
    "# itemz_pred[ixes] = (baseline-1)\n",
    "# itemz_pred = itemz_pred - (baseline-1)\n",
    "# itemz_pred = MinMaxScaler().fit_transform(itemz_pred)\n",
    "# print(itemz_2)\n",
    "\n",
    "# fig,axs = plt.subplots(1,2)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 10)\n",
    "\n",
    "fig.gca().scatter(itemz_2[0:n,0], itemz_2[0:n,1], alpha=0.7)\n",
    "fig.gca().scatter(itemz_2[n:,0], itemz_2[n:,1], alpha=0.7)\n",
    "j=0\n",
    "for j in range(n):\n",
    "    x,xh,y,yh = itemz_2[j,0], itemz_2[j+n,0], itemz_2[j,1], itemz_2[j+n,1]\n",
    "    fig.gca().plot([x,xh],[y,yh],color=\"#aaaaaa\")\n",
    "    fig.gca().annotate(j, (itemz_2[j+n,0], itemz_2[j+n,1]))\n",
    "\n",
    "# fig.gca().scatter(itemz_pred[:,0], itemz_pred[:,1], alpha=0.5)\n",
    "\n",
    "# for i, txt in enumerate(itemz_2):\n",
    "#     fig.gca().annotate(i, (itemz_2[i,0], itemz_2[i,1]))\n",
    "\n",
    "# fig.gca().axvline(x=baseline, linestyle=\"--\")\n",
    "# fig.gca().axhline(y=baseline, linestyle=\"--\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.set_printoptions(precision=2, suppress=True, threshold=100)\n",
    "def create_adj_mx(nodes):\n",
    "    L = numpy.zeros((len(nodes), len(nodes)))\n",
    "    for i in range(len(nodes)):\n",
    "        sum_dist = 0\n",
    "        for j in range(len(nodes)):\n",
    "            dist = numpy.linalg.norm(nodes[j]-nodes[i], ord=2)\n",
    "            L[i,j] = dist\n",
    "    return L\n",
    "\n",
    "def create_laplacian(nodes):\n",
    "    amx = create_adj_mx(nodes)\n",
    "    L = -amx #negate the adj mx\n",
    "    for ix,row in enumerate(L):\n",
    "        deg = -numpy.sum(row)\n",
    "        L[ix,ix] = deg\n",
    "    return L\n",
    "\n",
    "def graph_adj_mx(L, n=3, fn=\"adj_mx\"):\n",
    "    from graphviz import Graph\n",
    "    gg = Graph(strict=True, filename=fn)\n",
    "    if L[0,1]<0:\n",
    "        L = -L\n",
    "    for ix in range(len(L)):\n",
    "        row = L[ix,:]\n",
    "        js = numpy.argsort(row)[1:n+1]\n",
    "        for j in js:\n",
    "            v = row[j]\n",
    "            tup = str(ix),str(j),str(round(v,2))\n",
    "#             print(\"cadd edge\", tup)\n",
    "            sta,end,lab = tup\n",
    "            gg.edge(sta,end, label=str(lab))\n",
    "    gg.view()\n",
    "\n",
    "\n",
    "amx = create_adj_mx(questions)\n",
    "print(amx)\n",
    "graph_adj_mx(amx)\n",
    "\n",
    "    \n",
    "# numpy.set_printoptions(precision=2, suppress=True, threshold=100000)\n",
    "# print(L0.argsort(axis=1))\n",
    "m=3\n",
    "L1 = create_laplacian(questions)\n",
    "print(L1)\n",
    "graph_adj_mx(L1, fn=\"original\", n=m)\n",
    "\n",
    "for j,q in enumerate(qws_list):\n",
    "    L1 = create_laplacian(q)\n",
    "    graph_adj_mx(L1, fn=\"facsimile_{}\".format(j), n=m)\n",
    "\n",
    "# lams = numpy.linalg.eigvalsh(L0)\n",
    "# sum_eig = sum(lams)\n",
    "# sum_upto = 0\n",
    "# ct=0\n",
    "# for lam in lams:\n",
    "#     sum_upto += lam\n",
    "#     if sum_upto > 0.9*sum_eig:\n",
    "#         print(\"broke loop at \\lambda_{}\".format(ct))\n",
    "#         break\n",
    "#     ct+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
